


Paper ID = 6012
Title = Multi-class SVMs: From Tighter Data-Dependent
Generalization Bounds to Novel Algorithms
Yunwen Lei
Department of Mathematics
City University of Hong Kong
yunwelei@cityu.edu.hk
Ürün Dogan
Microsoft Research
Cambridge CB1 2FB, UK
udogan@microsoft.com
Alexander Binder
ISTD Pillar
Singapore University of Technology and Design
Machine Learning Group, TU Berlin
alexander binder@sutd.edu.sg
Marius Kloft
Department of Computer Science
Humboldt University of Berlin
kloft@hu-berlin.de
Abstract
This paper studies the generalization performance of multi-class classification al-
gorithms, for which we obtain—for the first time—a data-dependent generaliza-
tion error bound with a logarithmic dependence on the class size, substantially
improving the state-of-the-art linear dependence in the existing data-dependent
generalization analysis. The theoretical analysis motivates us to introduce a new
multi-class classification machine based on `p-norm regularization, where the pa-
rameter p controls the complexity of the corresponding bounds. We derive an
efficient optimization algorithm based on Fenchel duality theory. Benchmarks on
several real-world datasets show that the proposed algorithm can achieve signifi-
cant accuracy gains over the state of the art.
1 Introduction
Typical multi-class application domains such as natural language processing [1], information re-
trieval [2], image annotation [3] and web advertising [4] involve tens or hundreds of thousands of
classes, and yet these datasets are still growing [5]. To handle such learning tasks, it is essential
to build algorithms that scale favorably with respect to the number of classes. Over the past years,
much progress in this respect has been achieved on the algorithmic side [4–7], including efficient
stochastic gradient optimization strategies [8].
Although also theoretical properties such as consistency [9–11] and finite-sample behavior [1, 12–
15] have been studied, there still is a discrepancy between algorithms and theory in the sense that the
corresponding theoretical bounds do often not scale well with respect to the number of classes. This
discrepancy occurs the most strongly in research on data-dependent generalization bounds, that is,
bounds that can measure generalization performance of prediction models purely from the training
samples, and which thus are very appealing in model selection [16]. A crucial advantage of these
bounds is that they can better capture the properties of the distribution that has generated the data,
which can lead to tighter estimates [17] than conservative data-independent bounds.
To our best knowledge, for multi-class classification, the first data-dependent error bounds were
given by [14]. These bounds exhibit a quadratic dependence on the class size and were used by [12]
and [18] to derive bounds for kernel-based multi-class classification and multiple kernel learning
(MKL) problems, respectively. More recently, [13] improve the quadratic dependence to a linear
dependence by introducing a novel surrogate for the multi-class margin that is independent on the
true realization of the class label.
1
However, a heavy dependence on the class size, such as linear or quadratic, implies a poor gen-
eralization guarantee for large-scale multi-class classification problems with a massive number of
classes. In this paper, we show data-dependent generalization bounds for multi-class classification
problems that—for the first time—exhibit a sublinear dependence on the number of classes. Choos-
ing appropriate regularization, this dependence can be as mild as logarithmic. We achieve these
improved bounds via the use of Gaussian complexities, while previous bounds are based on a well-
known structural result on Rademacher complexities for classes induced by the maximum operator.
The proposed proof technique based on Gaussian complexities exploits potential coupling among
different components of the multi-class classifier, while this fact is ignored by previous analyses.
The result shows that the generalization ability is strongly impacted by the employed regularization.
Which motivates us to propose a new learning machine performing block-norm regularization over
the multi-class components. As a natural choice we investigate here the application of the proven `p
norm [19]. This results in a novel `p-norm multi-class support vector machine (MC-SVM), which
contains the classical model by Crammer & Singer [20] as a special case for p = 2. The bounds
indicate that the parameter p crucially controls the complexity of the resulting prediction models.
We develop an efficient optimization algorithm for the proposed method based on its Fenchel dual
representation. We empirically evaluate its effectiveness on several standard benchmarks for multi-
class classification taken from various domains, where the proposed approach significantly outper-
forms the state-of-the-art method of [20].
The remainder of this paper is structured as follows. Section 2 introduces the problem setting and
presents the main theoretical results. Motivated by which we propose a new multi-class classification
model in Section 3 and give an efficient optimization algorithm based on Fenchel duality theory. In
Section 4 we evaluate the approach for the application of visual image recognition and on several
standard benchmark datasets taken from various application domains. Section 5 concludes.
2 Theory
2.1 Problem Setting and Notations
This paper considers multi-class classification problems with c ≥ 2 classes. Let X denote the input
space and Y = {1, 2, . . . , c} denote the output space. Assume that we are given a sequence of ex-
amples S = {(x1, y1), . . . , (xn, yn)} ∈ (X × Y)n, independently drawn according to a probability
measure P defined on the sample space Z = X ×Y . Based on the training examples S, we wish to
learn a prediction rule h from a space H of hypotheses mapping from Z to R and use the mapping
x → arg maxy∈Y h(x, y) to predict (ties are broken by favoring classes with a lower index, for
which our loss function defined below always counts an error). For any hypothesis h ∈ H , the mar-
gin ρh(x, y) of the function h at a labeled example (x, y) is ρh(x, y) := h(x, y)−maxy′ 6=y h(x, y′).
The prediction rule h makes an error at (x, y) if ρh(x, y) ≤ 0 and thus the expected risk incurred
from using h for prediction is R(h) := E[1ρh(x,y)≤0].
Any function h : X × Y → R can be equivalently represented by the vector-valued function
(h1, . . . , hc) with hj(x) = h(x, j),∀j = 1, . . . , c. We denote by H̃ := {ρh : h ∈ H} the class
of margin functions associated to H . Let k : X × X → R be a Mercer kernel with φ being the
associated feature map, i.e., k(x, x̃) = 〈φ(x), φ(x̃)〉 for all x, x̃ ∈ X . We denote by ‖ · ‖∗ the dual
norm of ‖ · ‖, i.e., ‖w‖∗ := sup‖w̄‖≤1〈w, w̄〉. For a convex function f , we denote by f∗ its Fenchel
conjugate, i.e., f∗(v) := supw[〈w, v〉 − f(w)]. For any w = (w1, . . . ,wc) we define the `2,p-norm
by ‖w‖2,p := [
∑c
j=1 ‖wj‖
p
2]
1/p. For any p ≥ 1, we denote by p∗ the dual exponent of p satisfying
1/p+ 1/p∗ = 1 and p̄ := p(2− p)−1. We require the following definitions.
Definition 1 (Strong Convexity). A function f : X → R is said to be β-strongly convex w.r.t. a
norm ‖ · ‖ iff ∀x, y ∈ X and ∀α ∈ (0, 1), we have
f(αx+ (1− α)y) ≤ αf(x) + (1− α)f(y)− β
2
α(1− α)‖x− y‖2.
Definition 2 (Regular Loss). We call ` a L-regular loss if it satisfies the following properties:
(i) `(t) bounds the 0-1 loss from above: `(t) ≥ 1t≤0;
(ii) ` is L-Lipschitz in the sense |`(t1)− `(t2)| ≤ L|t1 − t2|;
2
(iii) `(t) is decreasing and it has a zero point c`, i.e., `(c`) = 0.
Some examples of L-regular loss functions include the hinge `h(t) = (1− t)+ and the margin loss
`ρ(t) = 1t≤0 + (1− tρ−1)10<t≤ρ, ρ > 0. (1)
2.2 Main results
Our discussion on data-dependent generalization error bounds is based on the established method-
ology of Rademacher and Gaussian complexities [21].
Definition 3 (Rademacher and Gaussian Complexity). Let H be a family of real-valued functions
defined on Z and S = (z1, . . . , zn) a fixed sample of size n with elements in Z . Then, the empirical
Rademacher and Gaussian complexities of H with respect to the sample S are defined by
RS(H) = Eσ
[
sup
h∈H
1
n
n∑
i=1
σih(zi)
]
, GS(H) = Eg
[
sup
h∈H
1
n
n∑
i=1
gih(zi)
]
,
where σ1, . . . , σn are independent random variables with equal probability taking values +1 or−1,
and g1, . . . , gn are independent N(0, 1) random variables.
Note that we have the following comparison inequality relating Rademacher and Gaussian complex-
ities (Cf. Section 4.2 in [22]):
RS(H) ≤
√
π
2
GS(H) ≤ 3
√
π
2
√
log nRS(H). (2)
Existing work on data-dependent generalization bounds for multi-class classifiers [12–14, 18] builds
on the following structural result on Rademacher complexities (e.g., [12], Lemma 8.1):
RS(max{h1, . . . , hc} : hj ∈ Hj , j = 1, . . . , c) ≤
c∑
j=1
RS(Hj), (3)
whereH1, . . . ,Hc are c hypothesis sets. This result is crucial for the standard generalization analysis
of multi-class classification since the margin ρh involves the maximum operator, which is removed
by (3), but at the expense of a linear dependency on the class size. In the following we show that this
linear dependency is suboptimal because (3) does not take into account the coupling among different
classes. For example, a common regularizer used in multi-class learning algorithms is r(h) =∑c
j=1 ‖hj‖22 [20], for which the components h1, . . . , hc are correlated via a ‖ · ‖2,2 regularizer, and
the bound (3) ignoring this correlation would not be effective in this case [12–14, 18].
As a remedy, we here introduce a new structural complexity result on function classes induced
by general classes via the maximum operator, while allowing to preserve the correlations among
different components meanwhile. Instead of considering the Rademacher complexity, Lemma 4
concerns the structural relationship of Gaussian complexities since it is based on a comparison result
among different Gaussian processes.
Lemma 4 (Structural result on Gaussian complexity). Let H be a class of functions defined on
X × Y with Y = {1, . . . , c}. Let g1, . . . , gnc be independent N(0, 1) distributed random variables.
Then, for any sample S = {x1, . . . , xn} of size n, we have
GS
(
{max{h1, . . . , hc} : h = (h1, . . . , hc) ∈ H}
)
≤ 1
n
Eg sup
h=(h1,...,hc)∈H
n∑
i=1
c∑
j=1
g(j−1)n+ihj(xi),
(4)
where Eg denotes the expectation w.r.t. to the Gaussian variables g1, . . . , gnc.
The proof of Lemma 4 is given in Supplementary Material A. Equipped with Lemma 4, we are
now able to present a general data-dependent margin-based generalization bound. The proof of the
following results (Theorem 5, Theorem 7 and Corollary 8) is given in Supplementary Material B.
Theorem 5 (Data-dependent generalization bound for multi-class classification). Let H ⊂ RX×Y
be a hypothesis class with Y = {1, . . . , c}. Let ` be a L-regular loss function and denote B` :=
sup(x,y),h `(ρh(x, y)). Suppose that the examples S = {(x1, y1), . . . , (xn, yn)} are independently
3
drawn from a probability measure defined on X × Y . Then, for any δ > 0, with probability at least
1− δ, the following multi-class classification generalization bound holds for any h ∈ H:
R(h) ≤ 1
n
n∑
i=1
`(ρh(xi, yi)) +
2L
√
2π
n
Eg sup
h=(h1,...,hc)∈H
n∑
i=1
c∑
j=1
g(j−1)n+ihj(xi) + 3B`
√
log 2δ
2n
,
where g1, . . . , gnc are independent N(0, 1) distributed random variables.
Remark 6. Under the same condition of Theorem 5, [12] derive the following data-dependent
generalization bound (Cf. Corollary 8.1 in [12]):
R(h) ≤ 1
n
n∑
i=1
`(ρh(xi, yi)) +
4Lc
n
RS({x→ h(x, y) : y ∈ Y, h ∈ H}) + 3B`
√
log 2δ
2n
.
This linear dependence on c is due to the use of (3). For comparison, Theorem 5 implies that the
dependence on c is governed by the term
∑n
i=1
∑c
j=1 g(j−1)n+ihj(xi), an advantage of which is that
the components h1, . . . , hc are jointly coupled. As we will see, this allows us to derive an improved
result with a favorable dependence on c, when a constraint is imposed on (h1, . . . , hc).
The following Theorem 7 applies the general result in Theorem 5 to kernel-based methods. The
hypothesis space is defined by imposing a constraint with a general strongly convex function.
Theorem 7 (Data-dependent generalization bound for kernel-based multi-class learning algorithms
and MC-SVMs). Suppose that the hypothesis space is defined by
H := Hf,Λ = {hw = (〈w1, φ(x)〉, . . . , 〈wc, φ(x)〉) : f(w) ≤ Λ},
where f is a β-strongly convex function w.r.t. a norm ‖·‖ defined onH satisfying f∗(0) = 0. Let ` be
a L-regular loss function and denote B` := sup(x,y),h `(ρh(x, y)). Let g1, . . . , gnc be independent
N(0, 1) distributed random variables. Then, for any δ > 0, with probability at least 1− δ we have
R(hw) ≤ 1
n
n∑
i=1
`(ρhw(xi, yi)) +
4L
n
√√√√πΛ
β
Eg
n∑
i=1
∥∥∥(g(j−1)n+iφ(xi))j=1,...,c∥∥∥2∗ + 3B`
√
log 2δ
2n
.
We now consider the following specific hypothesis spaces using a ‖ · ‖2,p constraint:
Hp,Λ := {hw = (〈w1, φ(x)〉, . . . , 〈wc, φ(x)〉) : ‖w‖2,p ≤ Λ}, 1 ≤ p ≤ 2. (5)
Corollary 8 (`p-norm MC-SVM generalization bound). Let ` be a L-regular loss function and
denote B` := sup(x,y),h `(ρh(x, y)). Then, with probability at least 1 − δ, for any hw ∈ Hp,Λ the
generalization error R(hw) can be upper bounded by:
1
n
n∑
i=1
`(ρhw(xi, yi))+3B`
√
log 2δ
2n
+
2LΛ
n
√√√√ n∑
i=1
k(xi, xi)×
{√
e(4 log c)1+
1
2 log c , if p∗ ≥ 2 log c,(
2p∗
)1+ 1
p∗ c
1
p∗ , otherwise.
Remark 9. The bounds in Corollary 8 enjoy a mild dependence on the number of classes. The
dependence is polynomial with exponent 1/p∗ for 2 < p∗ < 2 log c and becomes logarithmic if p∗ ≥
2 log c. Even in the theoretically unfavorable case of p = 2 [20], the bounds still exhibit a radical
dependence on the number of classes, which is substantially milder than the quadratic dependence
established in [12, 14, 18] and the linear dependence established in [13]. Our generalization bound
is data-dependent and shows clearly how the margin would affect the generalization performance
(when ` is the margin loss `ρ): a large margin ρ would increase the empirical error while decrease
the model’s complexity, and vice versa.
2.3 Comparison of the Achieved Bounds to the State of the Art
Related work on data-independent bounds. The large body of theoretical work on multi-class
learning considers data-independent bounds. Based on the `∞-norm covering number bound of
linear operators, [15] obtain a generalization bound exhibiting a linear dependence on the class size,
which is improved by [9] to a radical dependence of the formO(n−
1
2 (log
3
2 n)
√
c
ρ ). Under conditions
4
analogous to Corollary 8, [23] derive a class-size independent generalization guarantee. However,
their bound is based on a delicate definition of margin, which is why it is commonly not used in the
mainstream multi-class literature. [1] derive the following generalization bound
E
[1
p
log
(
1 +
∑
ỹ 6=y
ep(ρ−〈ŵy−ŵỹ,φ(x)〉)
)]
≤ inf
w∈H
E
[1
p
log
(
1 +
∑
ỹ 6=y
ep(ρ−〈wy−wỹ,φ(x)〉)
)
+
λn
2(n+ 1)
‖w‖22,2
]
+
2 supx∈X k(x, x)
λn
, (6)
where ρ is a margin condition, p > 0 a scaling factor, and λ a regularization parameter. Eq. (6) is
class-size independent, yet Corollary 8 shows superiority in the following aspects: first, for SVMs
(i.e., margin loss `ρ), our bound consists of an empirical error ( 1n
∑n
i=1 `ρ(ρhw(xi, yi))) and a com-
plexity term divided by the margin value (note that L = 1/ρ in Corollary 8). When the margin
is large (which is often desirable) [14], the last term in the bound given by Corollary 8 becomes
small, while—on the contrary—-the bound (6) is an increasing function of ρ, which is undesirable.
Secondly, Theorem 7 applies to general loss functions, expressed through a strongly convex func-
tion over a general hypothesis space, while the bound (6) only applies to a specific regularization
algorithm. Lastly, all the above mentioned results are conservative data-independent estimates.
Related work on data-dependent bounds. The techniques used in above mentioned papers do not
straightforwardly translate to data-dependent bounds, which is the type of bounds in the focus of
the present work. The investigation of these was initiated, to our best knowledge, by [14]: with the
structural complexity bound (3) for function classes induced via the maximal operator, [14] derive a
margin bound admitting a quadratic dependency on the number of classes. [12] use these results in
[14] to study the generalization performance of MC-SVMs, where the components h1, . . . , hc are
coupled with an ‖ · ‖2,p, p ≥ 1 constraint. Due to the usage of the suboptimal Eq. (3), [12] obtain
a margin bound growing quadratically w.r.t. the number of classes. [18] develop a new multi-class
classification algorithm based on a natural notion called the multi-class margin of a kernel. [18]
also present a novel multi-class Rademacher complexity margin bound based on Eq. (3), and the
bound also depends quadratically on the class size. More recently, [13] give a refined Rademacher
complexity bound with a linear dependence on the class size. The key reason for this improvement
is the introduction of ρθ,h := miny′∈Y [h(x, y)−h(x, y′)+θ1y′=y] bounding margin ρh from below,
and since the maximum operation in ρθ,h is applied to the set Y rather than the subset Y − {yi} for
ρh, one needs not to consider the random realization of yi. We also use this trick in our proof of
Theorem 5. However, [13] fail to improve this linear dependence to a logarithmic dependence, as
we achieved in Corollary 8, due to the use of the suboptimal structural result (3).
3 Algorithms
Motivated by the generalization analysis given in Section 2, we now present a new multi-class
learning algorithm, based on performing empirical risk minimization in the hypothesis space (5).
This corresponds to the following `p-norm MC-SVM (1 ≤ p ≤ 2):
Problem 10 (Primal problem: `p-norm MC-SVM).
min
w
1
2
[ c∑
j=1
‖wj‖p2
] 2
p
+ C
n∑
i=1
`(ti),
s.t. ti = 〈wyi , φ(xi)〉 −max
y 6=yi
〈wy, φ(xi)〉,
(P)
For p = 2 we recover the seminal multi-class algorithm by Crammer & Singer [20] (CS), which is
thus a special case of the proposed formulation. An advantage of the proposed approach over [20]
can be that, as shown in Corollary 8, the dependence of the generalization performance on the class
size becomes milder as p decreases to 1.
3.1 Dual problems
Since the optimization problem (P) is convex, we can derive the associated dual problem for the
construction of efficient optimization algorithms. The derivation of the following dual problem is
deferred to Supplementary Material C. For a matrix α ∈ Rn×c, we denote by αi the i-th row.
Denote by ej the j-th unit vector in Rc and 1 the vector in Rc with all components being zero.
5
Problem 11 (Completely dualized problem for general loss). The Lagrangian dual of (10) is:
sup
α∈Rn×c
− 1
2
[ c∑
j=1
∥∥ n∑
i=1
αijφ(xi)
∥∥ pp−1
2
] 2(p−1)
p − C
n∑
i=1
`∗(−αiyi
C
)
s.t. αij ≤ 0 ∧ αi · 1 = 0, ∀j 6= yi, i = 1, . . . , n.
(D)
Theorem 12 (REPRESENTER THEOREM). For any dual variable α ∈ Rn×c, the associated primal
variable w = (w1, . . . ,wc) minimizing the Lagrangian saddle problem can be represented by:
wj =
[ c∑
j̃=1
‖
n∑
i=1
αij̃φ(xi)‖
p∗
2
] 2
p∗−1
∥∥ n∑
i=1
αijφ(xi)
∥∥p∗−2
2
[ n∑
i=1
αijφ(xi)
]
.
For the hinge loss `h(t) = (1 − t)+, we know its Fenchel-Legendre conjugate is `∗h(t) = t if
−1 ≤ t ≤ 0 and∞ elsewise. Hence `∗h(−
αiyi
C ) = −
αiyi
C if−1 ≤ −
αiyi
C ≤ 0 and∞ elsewise. Now
we have the following dual problem for the hinge loss function:
Problem 13 (Completely dualized problem for the hinge loss (`p-norm MC-SVM)).
sup
α∈Rn×c
− 1
2
[ c∑
j=1
∥∥ n∑
i=1
αijφ(xi)
∥∥ pp−1
2
] 2(p−1)
p
+
n∑
i=1
αiyi
s.t. αi ≤ eyi · C ∧ αi · 1 = 0, ∀i = 1, . . . , n.
(7)
3.2 Optimization Algorithms
The dual problems (D) and (7) are not quadratic programs for p 6= 2, and thus generally not easy to
solve. To circumvent this difficulty, we rewrite Problem 10 as the following equivalent problem:
min
w,β
c∑
j=1
‖wj‖22
2βj
+ C
n∑
i=1
`(ti)
s.t. ti ≤ 〈wyi , φ(xi)〉 − 〈wy, φ(xi)〉, y 6= yi, i = 1, . . . , n,
‖β‖p̄ ≤ 1, p̄ = p(2− p)−1, βj ≥ 0.
(8)
The class weights β1, . . . , βc in Eq. (8) play a similar role as the kernel weights in `p-norm MKL
algorithms [19]. The equivalence between problem (P) and Eq. (8) follows directly from Lemma 26
in [24], which shows that the optimal β = (β1, . . . , βc) in Eq. (8) can be explicitly represented in
closed form. Motivated by the recent work on `p-norm MKL, we propose to solve the problem (8)
via alternately optimizing w and β. As we will show, given temporarily fixed β, the optimization
of w reduces to a standard multi-class classification problem. Furthermore, the update of β, given
fixed w, can be achieved via an analytic formula.
Problem 14 (Partially dualized problem for a general loss). For fixed β, the partial dual problem
for the sub-optimization problem (8) w.r.t. w is
sup
α∈Rn×c
− 1
2
c∑
j=1
βj
∥∥ n∑
i=1
αijφ(xi)
∥∥2
2
− C
n∑
i=1
`∗(−αiyi
C
)
s.t. αij ≤ 0 ∧ αi · 1 = 0, ∀j 6= yi, i = 1, . . . , n.
(9)
The primal variable w minimizing the associated Lagrangian saddle problem is
wj = βj
n∑
i=1
αijφ(xi). (10)
We defer the proof to Supplementary Material C. Analogous to Problem 13, we have the following
partial dual problem for the hinge loss.
Problem 15 (Partially dualized problem for the hinge loss (`p-norm MC-SVM)).
sup
α∈Rn×c
f(α) := −1
2
c∑
j=1
βj
∥∥ n∑
i=1
αijφ(xi)
∥∥2
2
+
n∑
i=1
αiyi
s.t. αi ≤ eyi · C ∧ αi · 1 = 0, ∀i = 1, . . . , n.
(11)
6
The Problems 14 and 15 are quadratic, so we can use the dual coordinate ascent algorithm [25] to
very efficiently solve them for the case of linear kernels. To this end, we need to compute the gradient
and solve the restricted problem of optimizing only one αi,∀i, keeping all other dual variables
fixed [25]. The gradient of f can be exactly represented by w:
∂f
∂αij
= −βj
n∑
ĩ=1
αĩjk(xi, xĩ) + 1yi=j = 1yi=j − 〈wj , φ(xi)〉. (12)
Suppose the additive change to be applied to the current αi is δαi, then from (12) we have
f(α1, . . . , αi−1, αi + δαi, αi+1, . . . , αn) =
c∑
j=1
∂f
∂αij
δαij −
1
2
c∑
j=1
βjk(xi, xi)[δαij ]
2 + const.
Therefore, the sub-problem of optimizing δαi is given by
max
δαi
− 1
2
c∑
j=1
βjk(xi, xi)[δαij ]
2 +
c∑
j=1
∂f
∂αij
δαij
s.t. δαi ≤ eyi · C −αi ∧ δαi · 1 = 0.
(13)
We now consider the subproblem of updating class weights β with temporarily fixed w, for which
we have the following analytic solution. The proof is deferred to the Supplementary Material C.1.
Proposition 16. (Solving the subproblem with respect to the class weights) Given fixed wj , the
minimal βj optimizing the problem (8) is attained at
βj = ‖wj‖2−p2
( c∑
j̃=1
‖wj̃‖
p
2
) p−2
p
. (14)
The update of βj based on Eq. (14) requires calculating ‖wj‖22, which can be easily fulfilled by
recalling the representation established in Eq. (10).
The resulting training algorithm for the proposed `p-norm MC-SVM is given in Algorithm 1. The
algorithm alternates between solving a MC-SVM problem for fixed class weights (Line 3) and up-
dating the class weights in a closed-form manner (Line 5). Recall that Problem 11 establishes a
completely dualized problem, which can be used as a sound stopping criterion for Algorithm 1.
Algorithm 1: Training algorithm for `p-norm MC-SVM.
input: examples {(xi, yi)ni=1} and the kernel k.
initialize βj = p̄
√
1/c,wj = 0 for all j = 1, . . . , c
while Optimality conditions are not satisfied do
optimize the multi-class classification problem (9)
compute ‖wj‖22 for all j = 1, . . . , c, according to Eq. (10)
update βj for all j = 1, . . . , c, according to Eq. (14)
end
4 Empirical Analysis
We implemented the proposed `p-norm MC-SVM algorithm (Algorithm 1) in C++ and solved the
involved MC-SVM problem using dual coordinate ascent [25]. We experiment on six benchmark
datasets: the Sector dataset studied in [26], the News 20 dataset collected by [27], the Rcv1 dataset
collected by [28], the Birds 15, Birds 50 as a part from [29] and the Caltech 256 collected by
griffin2007caltech. We used fc6 features from the BVLC reference caffenet from [30]. Table 1
gives an information on these datasets.
We compare with the classical CS in [20], which constitutes a strong baseline for these datasets
[25]. We employ a 5-fold cross validation on the training set to tune the regularization parameter
C by grid search over the set {2−12, 2−11, . . . , 212} and p from 1.1 to 2 with 10 equidistant points.
We repeat the experiments 10 times, and report in Table 2 on the average accuracy and standard
deviations attained on the test set.
7
Dataset No. of Classes No. of Training Examples No. of Test Examples No. of Attributes
Sector 105 6, 412 3, 207 55, 197
News 20 20 15, 935 3, 993 62, 060
Rcv1 53 15, 564 518, 571 47, 236
Birds 15 200 3, 000 8, 788 4, 096
Birds 50 200 9, 958 1, 830 4, 096
Caltech 256 256 12, 800 16, 980 4, 096
Table 1: Description of datasets used in the experiments.
Method / Dataset Sector News 20 Rcv1 Birds 15 Birds 50 Caltech 256
`p-norm MC-SVM 94.20±0.34 86.19±0.12 85.74±0.71 13.73±1.4 27.86±0.2 56.00±1.2
Crammer & Singer 93.89±0.27 85.12±0.29 85.21±0.32 12.53±1.6 26.28±0.3 54.96±1.1
Table 2: Accuracies achieved by CS and the proposed `p-norm MC-SVM on the benchmark datasets.
We observe that the proposed `p-norm MC-SVM consistently outperforms CS [20] on all considered
datasets. Specifically, our method attains 0.31% accuracy gain on Sector, 1.07% accuracy gain on
News 20, 0.53% accuracy gain on Rcv1, 1.2% accuracy gain on Birds 15, 1.58% accuracy gain on
Birds 50, and 1.04% accuracy gain on Birds 15. We perform a Wilcoxon signed rank test between
the accuracies of CS and our method on the benchmark datasets, and the p-value is 0.03, which
means our method is significantly better than CS at the significance level of 0.05. These promising
results indicate that the proposed `p-norm MC-SVM could further lift the state of the art in multi-
class classification, even in real-world applications beyond the ones studied in this paper.
5 Conclusion
Motivated by the ever growing size of multi-class datasets in real-world applications such as im-
age annotation and web advertising, which involve tens or hundreds of thousands of classes, we
studied the influence of the class size on the generalization behavior of multi-class classifiers. We
focus here on data-dependent generalization bounds enjoying the ability to capture the properties of
the distribution that has generated the data. Of independent interest, for hypothesis classes that are
given as a maximum over base classes, we developed a new structural result on Gaussian complex-
ities that is able to preserve the coupling among different components, while the existing structural
results ignore this coupling and may yield suboptimal generalization bounds. We applied the new
structural result to study learning rates for multi-class classifiers, and derived, for the first time, a
data-dependent bound with a logarithmic dependence on the class size, which substantially outper-
forms the linear dependence in the state-of-the-art data-dependent generalization bounds.
Motivated by the theoretical analysis, we proposed a novel `p-norm MC-SVM, where the parameter
p controls the complexity of the corresponding bounds. This class of algorithms contains the classi-
cal CS [20] as a special case for p = 2. We developed an effective optimization algorithm based on
the Fenchel dual representation. For several standard benchmarks taken from various domains, the
proposed approach surpassed the state-of-the-art method of CS [20] by up to 1.5%.
A future direction will be to derive a data-dependent bound that is completely independent of the
class size (even overcoming the mild logarithmic dependence here). To this end, we will study more
powerful structural results than Lemma 4 for controlling complexities of function classes induced
via the maximum operator. As a good starting point, we will consider `∞-norm covering numbers.
Acknowledgments
We thank Mehryar Mohri for helpful discussions. This work was partly funded by the German
Research Foundation (DFG) award KL 2698/2-1.
References
[1] T. Zhang, “Class-size independent generalization analsysis of some discriminative multi-category classi-
fication,” in Advances in Neural Information Processing Systems, pp. 1625–1632, 2004.
[2] T. Hofmann, L. Cai, and M. Ciaramita, “Learning with taxonomies: Classifying documents and words,”
in NIPS workshop on syntax, semantics, and statistics, 2003.
8
[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical
image database,” in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on,
pp. 248–255, IEEE, 2009.
[4] A. Beygelzimer, J. Langford, Y. Lifshits, G. Sorkin, and A. Strehl, “Conditional probability tree estimation
analysis and algorithms,” in Proceedings of UAI, pp. 51–58, AUAI Press, 2009.
[5] S. Bengio, J. Weston, and D. Grangier, “Label embedding trees for large multi-class tasks,” in Advances
in Neural Information Processing Systems, pp. 163–171, 2010.
[6] P. Jain and A. Kapoor, “Active learning for large multi-class problems,” in Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 762–769, IEEE, 2009.
[7] O. Dekel and O. Shamir, “Multiclass-multilabel classification with more classes than examples,” in Inter-
national Conference on Artificial Intelligence and Statistics, pp. 137–144, 2010.
[8] M. R. Gupta, S. Bengio, and J. Weston, “Training highly multiclass classifiers,” The Journal of Machine
Learning Research, vol. 15, no. 1, pp. 1461–1492, 2014.
[9] T. Zhang, “Statistical analysis of some multi-category large margin classification methods,” The Journal
of Machine Learning Research, vol. 5, pp. 1225–1251, 2004.
[10] A. Tewari and P. L. Bartlett, “On the consistency of multiclass classification methods,” The Journal of
Machine Learning Research, vol. 8, pp. 1007–1025, 2007.
[11] T. Glasmachers, “Universal consistency of multi-class support vector classification,” in Advances in Neu-
ral Information Processing Systems, pp. 739–747, 2010.
[12] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of machine learning. MIT press, 2012.
[13] V. Kuznetsov, M. Mohri, and U. Syed, “Multi-class deep boosting,” in Advances in Neural Information
Processing Systems, pp. 2501–2509, 2014.
[14] V. Koltchinskii and D. Panchenko, “Empirical margin distributions and bounding the generalization error
of combined classifiers,” Annals of Statistics, pp. 1–50, 2002.
[15] Y. Guermeur, “Combining discriminant models with new multi-class svms,” Pattern Analysis & Applica-
tions, vol. 5, no. 2, pp. 168–179, 2002.
[16] L. Oneto, D. Anguita, A. Ghio, and S. Ridella, “The impact of unlabeled patterns in rademacher com-
plexity theory for kernel classifiers,” in Advances in Neural Information Processing Systems, pp. 585–593,
2011.
[17] V. Koltchinskii and D. Panchenko, “Rademacher processes and bounding the risk of function learning,”
in High Dimensional Probability II, pp. 443–457, Springer, 2000.
[18] C. Cortes, M. Mohri, and A. Rostamizadeh, “Multi-class classification with maximum margin multiple
kernel,” in ICML-13, pp. 46–54, 2013.
[19] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien, “Lp-norm multiple kernel learning,” The Journal of
Machine Learning Research, vol. 12, pp. 953–997, 2011.
[20] K. Crammer and Y. Singer, “On the algorithmic implementation of multiclass kernel-based vector ma-
chines,” The Journal of Machine Learning Research, vol. 2, pp. 265–292, 2002.
[21] P. L. Bartlett and S. Mendelson, “Rademacher and gaussian complexities: Risk bounds and structural
results,” J. Mach. Learn. Res., vol. 3, pp. 463–482, 2002.
[22] M. Ledoux and M. Talagrand, Probability in Banach Spaces: isoperimetry and processes, vol. 23. Berlin:
Springer, 1991.
[23] S. I. Hill and A. Doucet, “A framework for kernel-based multi-category classification.,” J. Artif. Intell.
Res.(JAIR), vol. 30, pp. 525–564, 2007.
[24] C. A. Micchelli and M. Pontil, “Learning the kernel function via regularization,” Journal of Machine
Learning Research, pp. 1099–1125, 2005.
[25] S. S. Keerthi, S. Sundararajan, K.-W. Chang, C.-J. Hsieh, and C.-J. Lin, “A sequential dual method for
large scale multi-class linear svms,” in 14th ACM SIGKDD, pp. 408–416, ACM, 2008.
[26] J. D. Rennie and R. Rifkin, “Improving multiclass text classification with the support vector machine,”
tech. rep., AIM-2001-026, MIT, 2001.
[27] K. Lang, “Newsweeder: Learning to filter netnews,” in Proceedings of the 12th international conference
on machine learning, pp. 331–339, 1995.
[28] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, “Rcv1: A new benchmark collection for text categorization
research,” The Journal of Machine Learning Research, vol. 5, pp. 361–397, 2004.
[29] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, “Caltech-UCSD Birds
200,” Tech. Rep. CNS-TR-2010-001, California Institute of Technology, 2010.
[30] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, “Caffe:
Convolutional architecture for fast feature embedding,” arXiv preprint arXiv:1408.5093, 2014.
9
