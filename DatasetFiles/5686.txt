


Paper ID = 5686
Title = Adversarial Prediction Games for Multivariate Losses
Hong Wang Wei Xing Kaiser Asif Brian D. Ziebart
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607
{hwang27, wxing3, kasif2, bziebart}@uic.edu
Abstract
Multivariate loss functions are used to assess performance in many modern pre-
diction tasks, including information retrieval and ranking applications. Convex
approximations are typically optimized in their place to avoid NP-hard empir-
ical risk minimization problems. We propose to approximate the training data
instead of the loss function by posing multivariate prediction as an adversarial
game between a loss-minimizing prediction player and a loss-maximizing evalua-
tion player constrained to match specified properties of training data. This avoids
the non-convexity of empirical risk minimization, but game sizes are exponential
in the number of predicted variables. We overcome this intractability using the
double oracle constraint generation method. We demonstrate the efficiency and
predictive performance of our approach on tasks evaluated using the precision at
k, the F-score and the discounted cumulative gain.
1 Introduction
For many problems in information retrieval and learning to rank, the performance of a predictor is
evaluated based on the combination of predictions it makes for multiple variables. Examples in-
clude the precision when limited to k positive predictions (P@k), the harmonic mean of precision
and recall (F-score), and the discounted cumulative gain (DCG) for assessing ranking quality. These
stand in contrast to measures like the accuracy and (log) likelihood, which are additive over indepen-
dently predicted variables. Many multivariate performance measures are not concave functions of
predictor parameters, so maximizing them over empirical training data (or, equivalently, empirical
risk minimization over a corresponding non-convex multivariate loss function) is computationally
intractable [11] and can only be accomplished approximately using local optimization methods [10].
Instead, convex surrogates for the empirical risk are optimized using either an additive [21, 12, 22]
or a multivariate approximation [14, 24] of the loss function. For both types of approximations,
the gap between the application performance measure and the surrogate loss measure can lead to
substantial sub-optimality of the resulting predictions [4].
Rather than optimizing an approximation of the multivariate loss for available training data, we take
an alternate approach [26, 9, 1] that robustly minimizes the exact multivariate loss function using
approximations of the training data. We formalize this using a zero-sum game between a predictor
player and an adversarial evaluator player. Learned weights parameterize this gameâ€™s payoffs and
enable generalization from training data to new predictive settings. The key computational challenge
this approach poses is that the size of multivariate prediction games grows exponentially in the
number of variables. We leverage constraint generation methods developed for solving large zero-
sum games [20] and efficient methods for computing best responses [6] to tame this complexity.
In many cases, the structure of the multivariate loss function enables the zero-sum gameâ€™s Nash
equilibrium to be efficiently computed. We formulate parameter estimation as a convex optimization
problem and solve it using standard convex optimization methods. We demonstrate the benefits of
this approach on prediction tasks with P@k, F-score and DCG multivariate evaluation measures.
1
2 Background and Related Work
2.1 Notation and multivariate performance functions
We consider the general task of making a multivariate prediction for variables y =
{y1, y2, . . . , yn} âˆˆ Yn (with random variables denoted as Y = {Y1, Y2, . . . , Yn}) given some
contextual information x = {x1, x2, . . . , xn} âˆˆ X = {X1, X2, . . . , Xn} (with random variable,
X). Each xi is the information relevant to predicted variable yi. We denote the estimatorâ€™s predicted
values as yÌ‚ = {yÌ‚1, yÌ‚2, . . . , yÌ‚n}. The multivariate performance measure when predicting yÌ‚ when the
true multivariate value is actually y is represented as a scoring function: score(yÌ‚,y). Equivalently,
a complementary loss function for any score function based on the maximal score can be defined as:
loss(yÌ‚,y) = maxyâ€²,yâ€²â€² score(yâ€²,yâ€²â€²)âˆ’ score(yÌ‚,y).
For information retrieval, a vector of retrieved items from the pool of n items can be represented
as yÌ‚ âˆˆ {0, 1}n and a vector of relevant items as y âˆˆ {0, 1}n with x = {x1, x2, . . . , xn} denot-
ing side contextual information (e.g., search terms and document contents). Precision and recall
are important measures for information retrieval systems. However, maximizing either leads to
degenerate solutions (predict all to maximize recall or predict none to maximize precision). The
precision when limited to exactly k positive predictions, P@k(yÌ‚,y) = yÌ‚Â·yk where ||yÌ‚||1 = k,
is one popular multivariate performance measure that avoids these extremes. Another is the F-
score, which is the harmonic mean of the precision and recall often used in information re-
trieval tasks. Using this notation, the F-score for a set of items can be simply represented as:
F1(yÌ‚,y) =
2yÌ‚Â·y
||yÌ‚ ||1+||y||1 and F1(0,0) = 1.
In other information retrieval tasks, a ranked list of retrieved items is desired. This can be rep-
resented as a permutation, Ïƒ, where Ïƒ(i) denotes the ith-ranked item (and Ïƒâˆ’1(j) denotes the
rank of the jth item). Evaluation measures that emphasize the top-ranked items are used, e.g.,
to produce search engine results attuned to actual usage. The discounted cumulative gain (DCG)
measures the performance of item rankings with k relevancy scores, yi âˆˆ {0, . . . , k âˆ’ 1} as:
DCG(ÏƒÌ‚,y) =
âˆ‘n
i=1
2
yÏƒÌ‚(i)âˆ’1
log2(i+1)
or DCGâ€²(ÏƒÌ‚,y) = yÏƒÌ‚(1) +
âˆ‘n
i=2
yÏƒÌ‚(i)
log2 i
.
2.2 Multivariate empirical risk minimization
Empirical risk minimization [28] is a common supervised learning approach that seeks a predictor
PÌ‚ (yÌ‚|x) (from, e.g., a set of predictors Î“) that minimizes the loss under the empirical distribution of
training data, denoted PÌƒ (y,x): minPÌ‚ (yÌ‚|x)âˆˆÎ“ EPÌƒ (y,x)PÌ‚ (yÌ‚|x)[loss(YÌ‚,Y)]. Multivariate losses are of-
ten not convex and finding the optimal solution is computationally intractable for expressive classes
of predictors Î“ typically specified by some set of parameters Î¸ (e.g., linear discriminant functions:
PÌ‚ (yÌ‚|x) = 1 if Î¸ Â· Î¦(x, yÌ‚) > Î¸ Â· Î¦(x, yâ€²) âˆ€yâ€² 6= yÌ‚).
Given these difficulties, convex surrogates to the multivariate loss are instead employed that are
additive over yÌ‚i and yi (i.e., loss(yÌ‚,y) =
âˆ‘
i loss(yÌ‚i, yi)). Employing the logarithmic loss,
loss(yÌ‚i, yi) = âˆ’ log PÌ‚ (YÌ‚i = yi) yields the logistic regression model [9]. Using the hinge loss
yields support vector machines [5]. Structured support vector machines [27] employ a convex ap-
proximation of the multivariate loss over a training dataset D using the hinge loss function:
min
Î¸,Î¾iâ‰¥0
||Î¸||2 + Î±
âˆ‘
i
Î¾i such that âˆ€i,yâ€² âˆˆ Y , Î¸ Â· [Î¦(x(i),y(i))âˆ’ Î¦(x(i),yâ€²)] â‰¥ âˆ†(yâ€²,y(i))âˆ’ Î¾i.
In other words, linear parameters Î¸ for feature functions Î¦(Â·, Â·) are desired that make the example
label y(i) have a potential value Î¸ Â· Î¦(x(i),y(i)) that is better than all alternative labels yâ€² by at
least the multivariate loss between yâ€² and y(i), denoted âˆ†(yâ€²,y(i)). When this is not possible for
a particular example, a hinge loss penalty Î¾i is incurred that grows linearly with the difference in
potentials. Parameter Î± controls a trade-off between obtaining a predictor with lower hinge loss or
better discrimination between training examples (the margin). The size of set Y is often too large
for explicit construction of the constraint set to be computationally tractable. Instead, constraint
generation methods are employed to find a smaller set of active constraints. This can be viewed as
either finding the most-violated constraint [27] or as a loss-augmented inference problem [25]. Our
2
approach employs similar constraint generation techniquesâ€”in the inference procedure rather than
the parameter learning procedureâ€”to improve its efficiency.
3 Multivariate Prediction Games
We formulate a minimax game for multivariate loss optimization, describe our approach for lim-
iting the computational complexity of solving this game, and describe algorithms for estimating
parameters of the game and making predictions using this framework.
3.1 Game formulation
Following a recent adversarial formulation for classification [1], we view multivariate prediction as
a two-player game between player YÌ‚ making predictions and player YÌŒ determining the evaluation
distribution. Player YÌ‚ first stochastically chooses a predictive distribution of variable assignments,
PÌ‚ (yÌ‚|x), to maximize a multivariate performance measure, then player YÌŒ stochastically chooses
an evaluation distribution, PÌŒ (yÌŒ|x), that minimizes the performance measure. Further, player YÌŒ
must choose the relevant items in a way that (approximately) matches in expectation with a set of
statistics, Î¦(x,y), measured from labeled data. We denote this set as Îž.
Definition 1. The multivariate prediction game (MPG) for n predicted variables is:
max
PÌ‚ (yÌ‚|x)
min
PÌŒ (yÌŒ|x)âˆˆÎž
EPÌƒ (x)PÌ‚ (yÌ‚|x)PÌŒ (yÌŒ|x)
[
score(YÌ‚, YÌŒ)
]
, (1)
where PÌ‚ (yÌ‚|x) and PÌŒ (yÌŒ|x) are distributions over combinations of labels for the n predicted vari-
ables and the set Îž corresponds to the constraint: EPÌƒ (x)P (yÌŒ|x)
[
Î¦(X, YÌŒ)
]
= EPÌƒ (y,x) [Î¦(X,Y)] .
Since the set Îž constrains the adversaryâ€™s multivariate label distribution over the entire distribution
of inputs PÌƒ (x), solving this game directly is impractical when the number of training examples is
large. Instead, we employ the method of Lagrange multipliers in Theorem 1, which allows the set
of games to be independently solved given Lagrange multipliers Î¸.
Theorem 1. The multivariate prediction gameâ€™s value (Definition 1) can be equivalently obtained
by solving a set of unconstrained maximin games parameterized by Lagrange multipliers Î¸:
max
PÌ‚ (yÌ‚|x)
min
PÌŒ (yÌŒ|x)âˆˆÎž
EPÌƒ (x)PÌ‚ (yÌ‚|x)PÌŒ (yÌŒ|x)
[
score(YÌ‚, YÌŒ)
]
(a)
= min
PÌŒ (yÌŒ|x)âˆˆÎž
max
PÌ‚ (yÌ‚|x)
EPÌƒ (x)PÌ‚ (yÌ‚|x)PÌŒ (yÌŒ|x)
[
score(YÌ‚, YÌŒ)
]
(b)
= max
Î¸
ï£«ï£¬ï£¬ï£­EPÌƒ (y,x) [Î¸ Â· Î¦(X,Y)] + âˆ‘
xâˆˆX
PÌƒ (x) min
PÌŒ (yÌŒ|x)
max
PÌ‚ (yÌ‚|x)
ï£«ï£¬ï£¬ï£­score(yÌ‚, yÌŒ)âˆ’ Î¸ Â· Î¦(x, yÌŒ)ï¸¸ ï¸·ï¸· ï¸¸
C â€²
yÌ‚,yÌŒ
ï£¶ï£·ï£·ï£¸
ï£¶ï£·ï£·ï£¸ , (2)
where: Î¦(x,y) is a vector of features characterizing the set of prediction variables {yi} and pro-
vided contextual variables {xi} each related to predicted variable yi.
Proof (sketch). Equality (a) is a consequence of duality in zero-sum games [29]. Equality (b) is
obtained by writing the Lagrangian and taking the dual. Strong Lagrangian duality is guaranteed
when a feasible solution exists on the relative interior of the convex constraint set Îž [2]. (A small
amount of slack corresponds to regularization of the Î¸ parameter in the dual and guarantees the
strong duality feasibility requirement is satisfied in practice.)
The resulting gameâ€™s payoff matrix can be expressed as the original game scores of Eq. (1) aug-
mented with Lagrangian potentials. The combination defines a new payoff matrix with entries
C â€²yÌ‚,yÌŒ = score(yÌ‚, yÌŒ)âˆ’ Î¸ Â· Î¦(x, yÌŒ), as shown in Eq. (2).
3.2 Example multivariate prediction games and small-scale solutions
Examples of the Lagrangian payoff matrices for the P@2, F-score, and DCG games are shown in Ta-
ble 1 for three variables. We employ additive feature functions, Î¦(x, yÌŒ) =
âˆ‘n
i=1 Ï†(xi) I(yÌŒi = 1),
3
Table 1: The payoff matrices for the zero-sum games between player YÌŒ choosing columns and
player YÌ‚ choosing rows with three variables for: precision at k (top); F-score (middle) and DCG
with binary relevance values, yÌŒi âˆˆ {0, 1}, and we let lg 3 , log2 3 (bottom).
P@2 000 001 010 011 100 101 110 111
011 0 12âˆ’Ïˆ3
1
2âˆ’Ïˆ2 1âˆ’Ïˆ2âˆ’Ïˆ3 0âˆ’Ïˆ1
1
2âˆ’Ïˆ1âˆ’Ïˆ3
1
2âˆ’Ïˆ1âˆ’Ïˆ2 1âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
101 0 12âˆ’Ïˆ3 0âˆ’Ïˆ2
1
2âˆ’Ïˆ2âˆ’Ïˆ3
1
2âˆ’Ïˆ1 1âˆ’Ïˆ1âˆ’Ïˆ3
1
2âˆ’Ïˆ1âˆ’Ïˆ2 1âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
110 0 0âˆ’Ïˆ3 12âˆ’Ïˆ2
1
2âˆ’Ïˆ2âˆ’Ïˆ3
1
2âˆ’Ïˆ1
1
2âˆ’Ïˆ1âˆ’Ïˆ3 1âˆ’Ïˆ1âˆ’Ïˆ2 1âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
F1 000 001 010 011 100 101 110 111
000 1 0âˆ’Ïˆ3 0âˆ’Ïˆ2 0âˆ’Ïˆ2 âˆ’ Ïˆ3 0âˆ’Ïˆ1 0âˆ’Ïˆ1âˆ’Ïˆ3 0âˆ’Ïˆ1âˆ’Ïˆ2 0âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
001 0 1âˆ’Ïˆ3 0âˆ’Ïˆ2 23âˆ’Ïˆ2 âˆ’ Ïˆ3 0âˆ’Ïˆ1
2
3âˆ’Ïˆ1âˆ’Ïˆ3 0âˆ’Ïˆ1âˆ’Ïˆ2
1
2âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
010 0 0âˆ’Ïˆ3 1âˆ’Ïˆ2 23âˆ’Ïˆ2 âˆ’ Ïˆ3 0âˆ’Ïˆ1 0âˆ’Ïˆ1âˆ’Ïˆ3
2
3âˆ’Ïˆ1âˆ’Ïˆ2
1
2âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
011 0 23âˆ’Ïˆ3
2
3âˆ’Ïˆ2 1âˆ’Ïˆ2 âˆ’ Ïˆ3 0âˆ’Ïˆ1
1
2âˆ’Ïˆ1âˆ’Ïˆ3
1
2âˆ’Ïˆ1âˆ’Ïˆ2
4
5âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
100 0 0âˆ’Ïˆ3 0âˆ’Ïˆ2 0âˆ’Ïˆ2 âˆ’ Ïˆ3 1âˆ’Ïˆ1 23âˆ’Ïˆ1âˆ’Ïˆ3
2
3âˆ’Ïˆ1âˆ’Ïˆ2
1
2âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
101 0 23âˆ’Ïˆ3 0âˆ’Ïˆ2
1
2âˆ’Ïˆ2 âˆ’ Ïˆ3
2
3âˆ’Ïˆ1 1âˆ’Ïˆ1âˆ’Ïˆ3
1
2âˆ’Ïˆ1âˆ’Ïˆ2
4
5âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
110 0 0âˆ’Ïˆ3 23âˆ’Ïˆ2
1
2âˆ’Ïˆ2 âˆ’ Ïˆ3
2
3âˆ’Ïˆ1
1
2âˆ’Ïˆ1âˆ’Ïˆ3 1âˆ’Ïˆ1âˆ’Ïˆ2
4
5âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
111 0 12âˆ’Ïˆ3
1
2âˆ’Ïˆ2
4
5âˆ’Ïˆ2 âˆ’ Ïˆ3
1
2âˆ’Ïˆ1
4
5âˆ’Ïˆ1âˆ’Ïˆ3
4
5âˆ’Ïˆ1âˆ’Ïˆ2 1âˆ’Ïˆ1âˆ’Ïˆ2 âˆ’ Ïˆ3
DCG 000 001 010 011 100 101 110 111
123 0 12âˆ’Ïˆ3
1
lg 3âˆ’Ïˆ2
1
2+
1
lg 3âˆ’Ïˆ2âˆ’Ïˆ3 1âˆ’Ïˆ1
3
2âˆ’Ïˆ1âˆ’Ïˆ3 1+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2
3
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
132 0 1lg 3âˆ’Ïˆ3
1
2 âˆ’ Ïˆ2
1
2+
1
lg 3âˆ’Ïˆ2âˆ’Ïˆ3 1âˆ’Ïˆ1 1+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ3
3
2âˆ’Ïˆ1âˆ’Ïˆ2
3
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
213 0 12âˆ’Ïˆ3 1âˆ’Ïˆ2
3
2âˆ’Ïˆ2âˆ’Ïˆ3
1
lg 3âˆ’Ïˆ1
1
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ3 1+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2
3
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
231 0 1lg 3âˆ’Ïˆ3 1âˆ’Ïˆ2 1+
1
lg 3âˆ’Ïˆ2âˆ’Ïˆ3
1
2âˆ’Ïˆ1
1
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ3
3
2âˆ’Ïˆ1âˆ’Ïˆ2
3
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
312 0 1âˆ’Ïˆ3 12 âˆ’ Ïˆ2
3
2âˆ’Ïˆ2âˆ’Ïˆ3
1
lg 3âˆ’Ïˆ1 1+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ3
1
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2
3
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
321 0 1âˆ’Ïˆ3 1lg 3âˆ’Ïˆ2 1+
1
lg 3âˆ’Ïˆ2âˆ’Ïˆ3
1
2âˆ’Ïˆ1
3
2âˆ’Ïˆ1âˆ’Ïˆ3
1
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2
3
2+
1
lg 3âˆ’Ïˆ1âˆ’Ïˆ2âˆ’Ïˆ3
in these examples (with indicator function I(Â·)). We compactly represent the Lagrangian potential
terms for each game with potential variables, Ïˆi , Î¸ Â· Ï†(Xi = xi) when YÌŒi = 1 (and 0 otherwise).
Zero-sum games such as these can be solved using a pair of linear programs that have a constraint
for each pure action (set of variable assignments) in the game [29]:
max
v,PÌ‚ (yÌ‚|x)â‰¥0
v such that v â‰¤
âˆ‘
yÌ‚âˆˆY
PÌ‚ (yÌ‚|x)C â€²yÌ‚,yÌŒ âˆ€yÌ‚ âˆˆ Y and
âˆ‘
yÌ‚âˆˆY
PÌ‚ (yÌ‚|x) = 1; (3)
min
v,PÌŒ (yÌŒ|x)â‰¥0
v such that v â‰¥
âˆ‘
yÌŒâˆˆY
PÌŒ (yÌŒ|x)C â€²yÌ‚,yÌŒ âˆ€yÌŒ âˆˆ Y and
âˆ‘
yÌŒâˆˆY
PÌŒ (yÌŒ|x) = 1, (4)
whereC â€² is the Lagrangian-augmented payoff and v is the value of the game. The second player to
act in a zero-sum game can maximize/minimize using a pure strategy (i.e., a single value assignment
to all variables). Thus, these LPs consider only the set of pure strategies of the opponent to find the
first playerâ€™s mixed equilibrium strategy. The equilibrium strategy for the predictor is a distribution
over rows and the equilibrium strategy for the adversary is a distribution over columns.
The size of each gameâ€™s payoff matrix grows exponentially with the number of variables, n: (2n)
(
n
k
)
for the precision at k game; (2n)2 for the F-score game; and (n! kn) for the DCG game with k
possible relevance levels. These sizes make explicit construction of the game matrix impractical for
all but the smallest of problems.
3.3 Large-scale strategy inference
More efficient methods for obtaining Nash equilibria are needed to scale our MPG approach to
large prediction tasks with exponentially-sized payoff matrices. Though much attention has focused
on efficiently computing -Nash equilibria (e.g., in O(1/) time or O(ln(1/)) time [8]), which
guarantee each player a payoff within  of optimal, we employ an approach for finding an exact
equilibrium that works well in practice despite not having as strong theoretical guarantees [20].
4
Table 2: The reduced preci-
sion at k game with Ïˆ1 =
Ïˆ2 = Ïˆ3 = 0.4 (top) and F-
score game with Ïˆ1 = Ïˆ2 =
Ïˆ3 = 0 (bottom).
011 101 110
011 0.2 -0.3 -0.3
101 -0.3 0.2 -0.3
111 -0.3 -0.3 0.2
000 001 010 100
000 0 1 1 1
111 1 12
1
2
1
2
Consider the reduced game matrices of Table 2. The Nash equi-
librium for the precision at k game with Lagrangian potentials
Ïˆ1 = Ïˆ2 = Ïˆ3 = 0.4 is: PÌ‚ (yÌ‚|x) =
[
1
3
1
3
1
3
]
and PÌŒ (yÌŒ|x) =[
1
3
1
3
1
3
]
; with a game value of âˆ’ 215 . The Nash equilibrium for
the reduced F-score game with no learning (i.e., Ïˆ1 = Ïˆ2 = Ïˆ3 =
0) is: PÌ‚ (yÌ‚|x) =
[
1
3
2
3
]
and PÌŒ (yÌŒ|x) =
[
1
3
2
9
2
9
2
9
]
; with a
game value of 23 . The reduced game equilibrium is also an equilib-
rium of the original game. Though the exact size of the subgame
and its specific actions depends on the values of Ïˆ, often a compact
sub-game with identical equilibrium or close approximation exists
[18]. Motivated by the compactness of the reduced game, we em-
ploy a constraint generation approach known as the double oracle
algorithm [20] to iteratively construct an appropriate reduced game
that provides the correct equilibrium but avoids the computational
complexity of the original exponentially sized game.
Algorithm 1 Constraint generation game solver
Input: Lagrange potentials for each variable, Ïˆ = {Ïˆ1, Ïˆ2, . . . , Ïˆn}; initial action sets SÌ‚0 and SÌŒ0
Output: Nash equilibrium,
(
PÌ‚ (yÌ‚|x), PÌŒ (yÌŒ|x)
)
1: Initialize Player YÌ‚ â€™s action set SÌ‚ â† SÌ‚0 and Player YÌŒ â€™s action set SÌŒ â† SÌŒ0
2: C â€² â† buildPayoffMatrix(SÌ‚, SÌŒ, Ïˆ) . Using Eq. (2) for the sub-game matrix of SÌ‚ Ã— SÌŒ
3: repeat
4: [PÌ‚ (yÌ‚|x), vNash1 ]â† solveZeroSumGameYÌ‚ (C
â€²) . Using the LP of Eq. (3)
5: [aÌŒ, vÌŒBR]â† findBestResponseAction(P (yÌ‚|x), Ïˆ) . aÌŒ denotes the best response action
6: if (vNash1 6= vÌŒBR) then . Check if best response provides improvement
7: SÌŒ â† SÌŒ âˆª aÌŒ
8: C â€² â† buildPayoffMatrix(SÌ‚, SÌŒ, Ïˆ) . Add new row to game matrix
9: end if
10: [PÌŒ (yÌŒ|x), vNash2 ]â† solveZeroSumGameYÌŒ (C
â€²) . Using the LP of Eq. (4)
11: [aÌ‚, vÌ‚BR]â† findBestResponseAction(P (yÌŒ|x), Ïˆ)
12: if (vNash2 6= vÌ‚BR) then
13: SÌ‚ â† SÌ‚ âˆª aÌ‚
14: C â€² â† buildPayoffMatrix(SÌ‚, SÌŒ, Ïˆ) . Add new column to game matrix
15: end if
16: until (vNash1 = vNash2 = vÌ‚BR = vÌŒBR) . Stop if neither best response provides improvement
17: return [PÌ‚ (yÌ‚|x), PÌŒ (yÌŒ|x)]
Neither player can improve upon their strategy with additional pure
strategies when Algorithm 1 terminates, thus the mixed strategies it returns are a Nash equilibrium
pair [20]. Additionally, the algorithm is efficient in practice so long as each playerâ€™s strategy is
compact (i.e., the number of actions with non-zero probability is a polynomial subset of the la-
bel combinations) and best responses to opponentsâ€™ strategies can be obtained efficiently (i.e., in
polynomial time) for each player. Additionally, this algorithm can be modified to find approximate
equilibria by limiting the number of actions for each playerâ€™s set SÌ‚ and SÌŒ.
3.4 Efficiently computing best responses
The tractability of our approach largely rests on our ability to efficiently find best responses to oppo-
nent strategies: argmaxyÌ‚âˆˆYÌ‚ EPÌŒ (yÌŒ|x)[C
â€²
yÌ‚,YÌŒ ] and argminyÌŒâˆˆYÌŒ EPÌ‚ (yÌ‚|x)[C
â€²
YÌ‚ ,yÌŒ
]. For some combinations
of loss functions and features, finding the best response is trivial using, e.g., a greedy selection algo-
rithm. Other loss function/feature combinations require specialized algorithms or are NP-hard. We
illustrate each situation.
Precision at k best response Many best responses can be obtained using greedy algorithms that
are based on marginal probabilities of the opponentâ€™s strategy. For example, the expected payoff in
5
the precision at k game for the estimator player setting yÌ‚i = 1 is PÌŒ (yÌŒi = 1|x). Thus, the set of top k
variables with the largest marginal label probability provides the best response. For the adversaryâ€™s
best response, the Lagrangian terms must also be included. Since k is a known variable, as long as
the value of each included term, PÌ‚ (yÌ‚i = 1, ||yÌ‚||1 = k|x)âˆ’ kÏˆi, is negative, the sum is the smallest,
and the corresponding response is the best for the adversary.
F-score game best response We leverage a recently developed method for efficiently maximizing
the F-score when a distribution over relevant documents is given [6]. The key insight is that the
problem can be separated into an inner greedy maximization over item sets of a certain size k and an
outer maximization to select the best set size k from {0, . . . , n}. This method can be directly applied
to find the best response of the estimator player, YÌ‚ , since the Lagrangian terms of the cost matrix
are invariant to the choice of yÌ‚. Algorithm 2 obtains the best response for the adversary player, YÌŒ ,
using slight modifications to incorporate the Lagrangian potentials into the objective function.
Algorithm 2 Lagrangian-augmented F-measure Maximizer for adversary player YÌŒ
Input: vector PÌ‚ of estimator probabilities and Lagrange potentials Ïˆ (Ïˆ1, Ïˆ2, ..., Ïˆn)
1: define matrixW with elementW s,k = 1s+k , s, k âˆˆ {1, ..., n}
2: construct matrix F = PÌ‚ Ã—W âˆ’ 12Ïˆ
T Ã— 1n . 1n is the all ones 1Ã— n vector
3: for k = 1 to n do
4: solve the inner optimization problem:
5: a(k)
âˆ—
= argminaâˆˆAk 2
âˆ‘n
i=1 aifik . Ak = {a âˆˆ {0, 1}n|
âˆ‘n
i=1 ai = k}
6: by setting a(k)i = 1 for the k-th column of F â€™s smallest k elements, and ai = 0 for the rest;
7: store a value of Eyâˆ¼p(YÌ‚|x)[F (y,a
(k)âˆ—)] = 2
âˆ‘n
i=1 a
(k)âˆ—
i fik
8: end for
9: for k = 0 take a(k)
âˆ—
= 0n, and Eyâˆ¼P (YÌ‚|x)[F (y,0
n)] = p(YÌ‚ = 0n|x)
10: solve the outer optimization problem:
11: aâˆ— = argminaâˆˆ{a(0)âˆ— ,...,a(n)âˆ—} Eyâˆ¼p(YÌ‚|x)[F (y,a)]
12: return aâˆ— and Eyâˆ¼p(YÌ‚|x)[F (y,a
âˆ—)]
Order inversion best response Another common loss measure when comparing two rankings is
the number of pairs of items with inverted order across rankings (i.e., one variable may occur before
another in one ranking, but not in the other ranking). Only the marginal probabilities of pairwise
orderings, PÌ‚ (ÏƒÌ‚âˆ’1(i) > ÏƒÌ‚âˆ’1(j)) ,
âˆ‘
ÏƒÌ‚ PÌ‚ (ÏƒÌ‚) I(Ïƒ
âˆ’1(i) > Ïƒâˆ’1(j)), are needed to construct the
portion of the payoff received for ÏƒÌŒ ranking item i over item j, PÌ‚ (ÏƒÌ‚âˆ’1(i) > ÏƒÌ‚âˆ’1(j))(1 + Ïˆi>j),
where Ïˆi>j is a Lagrangian potential based on pair-wise features for ranking item i over item j.
One could construct a fully connected directed graph with edges weighted by these portions of the
payoff for ranking pairs of items. The best response for ÏƒÌŒ corresponds to a set of acyclic edges
with the smallest sum of edge weights. Unfortunately, this problem is NP-hard in general because
the NP-complete minimum feedback arc set problem [15], which seeks to form an acyclic graph by
removing the set of edges with the minimal sum of edge weights, can be reduced to it.
DCG best response Although we cannot find an efficient algorithm to get the best response using
order inversion, solving best response of DCG has a known efficient algorithm. In this problem the
maximizer is a permutation of the documents while the minimizer is the relevance score of each
document pair. The estimatorâ€™s best response ÏƒÌ‚ maximizes:âˆ‘
yÌŒ
P (yÌŒ|x)
(
nâˆ‘
i=1
2yÌŒÏƒÌ‚(i) âˆ’ 1
log2(i+ 1)
âˆ’ Î¸ Â· Ï†(x, yÌŒ)
)
=
nâˆ‘
i=1
1
log2(i+ 1)
(âˆ‘
yÌŒ
P (yÌŒ|x)2yÌŒÏƒÌ‚(i) âˆ’ 1
)
âˆ’ c,
where c is a constant that has no relationship with ÏƒÌ‚. Since 1/log2(i+ 1) is monotonically decreas-
ing, computing and sorting
âˆ‘
yÌŒ P (yÌŒ|x)2yÌŒi âˆ’ 1 with descending order and greedily assign the order
to ÏƒÌ‚ is optimal. The adversaryâ€™s best response using additive features minimizes:âˆ‘
ÏƒÌ‚
P (ÏƒÌ‚|x)
nâˆ‘
i=1
2yÌŒÏƒÌ‚(i) âˆ’ 1
log2(i+ 1)
âˆ’
nâˆ‘
i=1
Î¸i Â· Ï†i(xi, yÌŒi) =
nâˆ‘
i=1
(âˆ‘
ÏƒÌ‚
P (ÏƒÌ‚|x) 2
yÌŒi âˆ’ 1
log2(Ïƒ
âˆ’1(i) + 1)
âˆ’ Î¸i Â· Ï†i(xi, yÌŒi)
)
.
6
Thus, by using the expectation of a function of each variableâ€™s rank, 1/(log2(Ïƒ
âˆ’1(i) + 1), which is
easily computed from PÌ‚ (Ïƒ), each variableâ€™s relevancy score yÌŒi can be independently chosen.
3.5 Parameter estimation
Predictive model parameters, Î¸, must be chosen to ensure that the adversarial distribution is similar
to training data. Though adversarial prediction can be posed as a convex optimization problem
[1], the objective function is not smooth. General subgradient methods require O(1/2) iterations
to provide an  approximation to the optima. We instead employ L-BFGS [19], which has been
empirically shown to converge at a faster rate in many cases despite lacking theoretical guarantees
for non-smooth objectives [16]. We also employ L2 regularization to avoid overfitting to the training
data sample. The addition of the smooth regularizer often helps to improve the rate of convergence.
The gradient in these optimizations with L2 regularization, âˆ’Î»2 ||Î¸||
2, for training dataset D =
{(x(i),y(i))} is the difference between feature moments with additional regularization term:
1
|D|
âˆ‘|D|
j=1
(
Î¦(x(i),y(i))âˆ’
âˆ‘
yÌŒâˆˆY PÌŒ (yÌŒ|x(i))Î¦(x(i), yÌŒ)
)
âˆ’ Î»Î¸. The adversarial strategies PÌŒ (Â·|x(i))
needed for calculating this gradient are computed via Alg. 1.
4 Experiments
We evaluate our approach, Multivariate Prediction Games (MPG), on the three performance mea-
sures of interest in this work: precision at k, F-score, and DCG. Our primary point of comparison
is with structured support vector machines (SSVM)[27] to better understand the trade-offs between
convexly approximating the loss function with the hinge loss versus adversarially approximating the
training data using our approach. We employ an optical recognition of handwritten digits (OPTDIG-
ITS) dataset [17] (10 classes, 64 features, 3,823 training examples, 1,797 test examples), an income
prediction dataset (â€˜a4aâ€™ ADULT1 [17] (two classes, 123 features, 3,185 training examples, 29,376
test examples), and query-document pairs from the million query TREC 2007 (MQ2007) dataset
of LETOR4.0 [23] (1700 queries, 41.15 documents on average per query, 46 features per docu-
ment). Following the same evaluation method used in [27] for OPTDIGITS, the multi-class dataset
is converted into multiple binary datasets and we report the macro-average of the performance of all
classes on test data. For OPTDIGITS/ADULT, we use a random 13 of the training data as a holdout
validation data to select the L2 regularization parameter trade-off C âˆˆ {2âˆ’6, 2âˆ’5, ..., 26}.
Table 3: Precision at k (top) and F-score
performance (bottom).
Precision@k OPTDIGITS ADULT
MPG 0.990 0.805
SSVM 0.956 0.638
SSVMâ€™ 0.989 0.805
F-score OPTDIGITS ADULT
MPG 0.920 0.697
SSVM 0.915 0.673
LR 0.914 0.639
We evaluate the performance of our approach and com-
parison methods (SSVM variants2 and logistic regression
(LR)) using precision at k, where k is half the number of
positive examples (i.e. k = 12POS), and F-score. For
precision at k, we restrict the pure strategies of the adver-
sary to select k positive labels. This prevents adversary
strategies with no positive labels. From the results in Ta-
ble 3, we see that our approach, MPG, works better than
SSVM on the OPTDGITS datasets: slightly better on pre-
cision at k and more significantly better on F-measure.
For the ADULT dataset, MPG provides equivalent per-
formance for precision at k and better performance on F-
measure. The nature of the running time required for validation and testing is very different for
SSVM, which must find the maximizing set of variable assignments, and MPG, which must interac-
tively construct a game and its equilibrium. Model validation and testing require â‰ˆ 30 seconds for
SSVM on the OPTDIGITS dataset and â‰ˆ 3 seconds on the ADULT dataset, while requiring â‰ˆ 9
seconds and â‰ˆ 25 seconds for MPG precision at k and â‰ˆ 1397 seconds and â‰ˆ 252 seconds for
MPG F-measure optimization, respectively. For precision at k, MPG is within an order of magni-
1 http://www.csie.ntu.edu.tw/Ëœcjlin/libsvmtools/datasets/binary.html)
2For precision at k, the original SSVMâ€™s implementation uses the restriction k during training, but not
during testing. We modified the code by ordering SSVMâ€™s prediction value for each test example, and select
the top k predictions as positives, the rest are considered as negatives. We denote the original implementation
as SSVM, and the modified version as SSVMâ€™.
7
tude (better for OPTDIGITS, worse for ADULT). For the more difficult problem of maximizing the
F-score of ADULT over 29, 376 test examples, the MPG game becomes quite large and requires
significantly more computational time. Though our MPG method is not as finely optimized as ex-
isting SSVM implementations, this difference in run times will remain as the game formulation is
inherently more computationally demanding for difficult prediction tasks.
Figure 1: NDCG@K as K increases.
We compare the performance of our approach and com-
parison methods using five-fold cross validation on the
MQ2007 dataset. We measure performance using Nor-
malized DCG (NDCG), which divides the realized DCG
by the maximum possible DCG for the dataset, based on a
slightly different variant of DCG employed by LETOR4.0:
DCGâ€²â€²(ÏƒÌ‚,y) = 2yÏƒÌ‚(1) âˆ’1 +
âˆ‘n
i=2
2
yÏƒÌ‚(i)âˆ’1
log2 i
. The compari-
son methods are: RankSVM-Struct [13], part of SVMstruct
which uses structured SVM to predict the rank; ListNet
[3], a list-wise ranking algorithm employing cross en-
tropy loss; AdaRank-NDCG [30], a boosting method us-
ing â€˜weak rankersâ€™ and data reweighing to achieve good
NDCG performance; AdaRank-MAP uses Mean Average
Precision (MAP) rather than NDCG; and RankBoost [7],
which reduces ranking to binary classification problems on instance pairs.
Table 4: MQ2007 NDCG Results.
Method Mean NDCG
MPG 0.5220
RankSVM 0.4966
ListNet 0.4988
AdaRank-NDCG 0.4914
AdaRank-MAP 0.4891
RankBoost 0.5003
Table 4 reports the NDCG@K averaged over all values of K
(between 1 and, on average 41) while Figure 1 reports the re-
sults for each value of K between 1 and 10. From this, we can
see that our MPG approach provides better rankings on aver-
age than the baseline methods except when K is very small
(K = 1, 2). In other words, the adversary focuses most of its
effort in reducing the score received from the first item in the
ranking, but at the expense of providing a better overall NDCG
score for the ranking as a whole.
5 Discussion
We have extended adversarial prediction games [1] to settings with multivariate performance mea-
sures in this paper. We believe that this is an important step in demonstrating the benefits of this
approach in settings where structured support vector machines [14] are widely employed. Our fu-
ture work will investigate improving the computational efficiency of adversarial methods and also
incorporating structured statistical relationships amongst variables in the constraint set in addition
to multivariate performance measures.
Acknowledgments
This material is based upon work supported by the National Science Foundation under Grant No.
#1526379, Robust Optimization of Loss Functions with Application to Active Learning.
References
[1] Kaiser Asif, Wei Xing, Sima Behpour, and Brian D. Ziebart. Adversarial cost-sensitive classification. In
Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2015.
[2] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
[3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach
to listwise approach. In Proceedings of the International Conference on Machine Learning, pages 129â€“
136. ACM, 2007.
[4] Corinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In Advances in Neural
Information Processing Systems, pages 313â€“320, 2004.
[5] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273â€“297, 1995.
8
[6] Krzysztof J Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke HuÌˆllermeier. An exact algorithm
for F-measure maximization. In Advances in Neural Information Processing Systems, pages 1404â€“1412,
2011.
[7] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. An efficient boosting algorithm for com-
bining preferences. The Journal of machine learning research, 4:933â€“969, 2003.
[8] Andrew Gilpin, Javier PenÌƒa, and Tuomas Sandholm. First-order algorithm with o (ln (1/e)) convergence
for e-equilibrium in two-person zero-sum games. In AAAI Conference on Artificial Intelligence, pages
75â€“82, 2008.
[9] Peter D. GruÌˆnwald and A. Phillip Dawid. Game theory, maximum entropy, minimum discrepancy, and
robust Bayesian decision theory. Annals of Statistics, 32:1367â€“1433, 2004.
[10] Tamir Hazan, Joseph Keshet, and David A McAllester. Direct loss minimization for structured prediction.
In Advances in Neural Information Processing Systems, pages 1594â€“1602, 2010.
[11] Klaus-Uwe Hoffgen, Hans-Ulrich Simon, and Kevin S Vanhorn. Robust trainability of single neurons.
Journal of Computer and System Sciences, 50(1):114â€“125, 1995.
[12] Martin Jansche. Maximum expected F-measure training of logistic regression models. In Proceedings of
the Conference on Human Language Technology and Empirical Methods in Natural Language Process-
ing, pages 692â€“699. Association for Computational Linguistics, 2005.
[13] Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the Interna-
tional Conference on Knowledge Discovery and Data Mining, pages 133â€“142. ACM, 2002.
[14] Thorsten Joachims. A support vector method for multivariate performance measures. In Proceedings of
the International Conference on Machine Learning, pages 377â€“384. ACM, 2005.
[15] Richard M. Karp. Reducibility among combinatorial problems. Springer, 1972.
[16] Adrian S Lewis and Michael L Overton. Nonsmooth optimization via BFGS. 2008.
[17] M. Lichman. UCI machine learning repository, 2013.
[18] Richard J Lipton and Neal E Young. Simple strategies for large zero-sum games with applications to
complexity theory. In Proc. of the ACM Symposium on Theory of Computing, pages 734â€“740. ACM,
1994.
[19] Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503â€“528, 1989.
[20] H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost functions
controlled by an adversary. In Proceedings of the International Conference on Machine Learning, pages
536â€“543, 2003.
[21] David R Musicant, Vipin Kumar, and Aysel Ozgur. Optimizing F-measure with support vector machines.
In FLAIRS Conference, pages 356â€“360, 2003.
[22] Shameem Puthiya Parambath, Nicolas Usunier, and Yves Grandvalet. Optimizing F-measures by cost-
sensitive classification. In Advances in Neural Information Processing Systems, pages 2123â€“2131, 2014.
[23] Tao Qin and Tie-Yan Liu. Introducing LETOR 4.0 datasets. arXiv preprint arXiv:1306.2597, 2013.
[24] Mani Ranjbar, Greg Mori, and Yang Wang. Optimizing complex loss functions in structured prediction.
In Proceedings of the European Conference on Computer Vision, pages 580â€“593. Springer, 2010.
[25] Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. Learning structured prediction
models: A large margin approach. In Proceedings of the International Conference on Machine Learning,
pages 896â€“903. ACM, 2005.
[26] Flemming TopsÃ¸e. Information theoretical optimization techniques. Kybernetika, 15(1):8â€“27, 1979.
[27] Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector ma-
chine learning for interdependent and structured output spaces. In Proceedings of the International Con-
ference on Machine Learning, page 104. ACM, 2004.
[28] Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural Information
Processing Systems, pages 831â€“838, 1992.
[29] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton Uni-
versity Press, 1947.
[30] Jun Xu and Hang Li. Adarank: a boosting algorithm for information retrieval. In Proc. of the International
Conference on Research and Development in Information Retrieval, pages 391â€“398. ACM, 2007.
9
