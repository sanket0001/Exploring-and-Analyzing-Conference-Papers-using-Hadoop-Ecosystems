


Paper ID = 5995
Title = A Market Framework for Eliciting Private Data
Bo Waggoner
Harvard SEAS
bwaggoner@fas.harvard.edu
Rafael Frongillo
University of Colorado
raf@colorado.edu
Jacob Abernethy
University of Michigan
jabernet@umich.edu
Abstract
We propose a mechanism for purchasing information from a sequence of partici-
pants. The participants may simply hold data points they wish to sell, or may have
more sophisticated information; either way, they are incentivized to participate as
long as they believe their data points are representative or their information will
improve the mechanismâ€™s future prediction on a test set. The mechanism, which
draws on the principles of prediction markets, has a bounded budget and mini-
mizes generalization error for Bregman divergence loss functions. We then show
how to modify this mechanism to preserve the privacy of participantsâ€™ informa-
tion: At any given time, the current prices and predictions of the mechanism reveal
almost no information about any one participant, yet in total over all participants,
information is accurately aggregated.
1 Introduction
A firm that relies on the ability to make difficult predictions can gain a lot from a large collection
of data. The goal is often to estimate values y âˆˆ Y given observations x âˆˆ X according to an
appropriate class of hypotheses F describing the relationship between x and y (for example, y = a Â·
x+ b for linear regression). In classic statistical learning theory, the goal is formalized as attempting
to approximately solve
min
fâˆˆF
E
x,y
Loss(f ; (x, y)) (1)
where Loss(Â·) is an appropriate inutility function and (x, y) is drawn from an unknown distribution.
In the present paper we are concerned with the case in which the data are not drawn or held by a
central authority but are instead inherently distributed. By this we mean that the data is (disjointly)
partitioned across a set of agents, with agent i privately possessing some portion of the dataset Si,
and agents have no obvious incentive to reveal this data to the firm seeking it. The vast swaths of data
available in our personal email accounts could provide massive benefits to a range of companies, for
example, but users are typically loathe to provide account credentials, even when asked politely.
We will be concerned with the design of financial mechanisms that provide a community of agents,
each holding a private set of data, an incentive to contribute to the solution of a large learning or
prediction task. Here we use the term â€˜mechanismâ€™ to mean an algorithmic interface that can receive
and answer queries, as well as engage in monetary exchange (deposits and payouts). Our aim will
be to design such a mechanism that satisfies the following three properties:
1. The mechanism is efficient in that it approaches a solution to (1) as the amount of data and
participation grows while spending a constant, fixed total budget.
2. The mechanism is incentive-compatible in the sense that agents are rewarded when their
contributions provide marginal value in terms of improved hypotheses, and are not re-
warded for bad or misleading information.
3. The mechanism provides reasonable privacy guarantees, so that no agent j (or outside
observer) can manipulate the mechanism in order to infer the contributions of agent i 6= j.
1
Ultimately we would like our mechanism to approach the performance of a learning algorithm that
had direct access to all the data, while only spending a constant budget to acquire data and improve
predictions and while protecting participantsâ€™ privacy.
Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20],
popular for some time in the field of economics and recently studied in great detail in computer
science [8, 16, 6, 15, 18, 1]. A prediction market is a mechanism designed for the purpose of
information aggregation, particularly when there is some underlying future event about which many
members of the population may have private and useful information. For instance, it may elicit
predictions about which team will win an upcoming sporting event, or which candidate will win an
election. These predictions are eventually scored on the actual outcome of the event.
Applying these prediction market techniques allows participants to essentially â€œtrade in a marketâ€
based on their data. (This approach is similar to prior work on crowdsourcing contests [3].) Members
of the population have private information, just as with prediction markets â€” in this case, data points
or beliefs â€” and the goal is to incentivize them to reveal and aggregate that information into a final
hypothesis or prediction. Their final profits are tied to the outcome of a test set of data, with each
participant being paid in accordance with how much their information improved the performance
on the test set. Our techniques depart from the framework of [3] in two significant aspects: (a) we
focus on the particular problem of data aggregation, and most of our results take advantage of kernel
methods; and (b) our mechanisms are the first to combine differential privacy guarantees with data
aggregation in a prediction-market framework.
This framework will provide efficiency and truthfulness. We will also show how to achieve privacy
in many scenarios. We will give mechanisms where the prices and predictions published satisfy
(, Î´)-differential privacy [10] with respect to each participantâ€™s data. The mechanismâ€™s output can
still give reasonable predictions while no observer can infer much about any participantâ€™s input data.
2 Mechanisms for Eliciting and Aggregating Data
We now give a broad description of the mechanism we will study. In brief, we imagine a central
authority (the mechanism, or market) maintaining a hypothesis f t representing the current aggrega-
tion of all the contributions made thus far. A new (or returning) participant may query f t at no cost,
perhaps evaluating the quality of the predictions on a privately-held dataset, and can then propose an
update df t+1 to f t that possibly requires an investment (a â€œbetâ€). Bets are evaluated at the close of
the market when a true data sample is generated (analogous to a test set), and payouts are distributed
according to the quality of the updates.
After describing this initial framework as Mechanism 1, which is based loosely on the setting of
[3], we turn our attention to the special case in which our hypotheses must lie in a Reproducing
Kernel Hilbert Space (RKHS) [17] for a given kernel k(Â·, Â·). This kernel-based â€œnonparametric
mechanismâ€ is particularly well-suited for the problem of data aggregation, as the betting space of
the participants consists essentially of updates of the form df t = Î±tk(zt, Â·), where zt is the data
object offered by the participant and Î±t âˆˆ R is the â€œmagnitudeâ€ of the bet.
A drawback of Mechanism 1 is the lack of privacy guarantees associated with the betting protocol:
utilizing oneâ€™s data to make bets or investments in the mechanism can lead to a loss of privacy for
the owner of that data. When a participant submits a bet of the form df t = Î±tk(zt, Â·), where zt
could contain sensitive personal information, another participant may be able to infer zt by querying
the mechanism. One of the primary contributions of the present work, detailed in Section 3, is a
technique to allow for productive participation in the mechanism while maintaining a guarantee on
the privacy of the data submitted.
2.1 The General Template
There is a space of examplesXÃ—Y , where x âˆˆ X are features and y âˆˆ Y are labels. The mechanism
designer chooses a function space F consisting of f : X Ã— Y â†’ R, and assumed to have Hilbert
space structure; one may view F as either the hypothesis class or the associated loss class, that is
where fh(x, y) measures the loss/performance of hypothesis h on observation x and label y. In each
case we will refer to f âˆˆ F as a hypothesis, eliding the distinction between fh and h.
2
The pricing scheme of the mechanism relies on a convex cost function Cx(Â·) : F â†’ R which is
parameterized by elements x âˆˆ X but whose domain is the set of hypotheses F . The cost function
is publicly available and determined in advance. The interaction with the mechanism is a sequential
process of querying and betting. On round t âˆ’ 1 the mechanism publishes a hypothesis f tâˆ’1, the
â€œstateâ€ of the market, which participants may query. Each participant arrives sequentially, and on
round t a participant may place a â€œbetâ€ df t âˆˆ F , also called a â€œtradeâ€ or â€œupdateâ€, modifying the
hypothesis f tâˆ’1 â†’ f t = f tâˆ’1 + df t. Finally participation ends and the mechanism samples (or
reveals) a test example1 (x, y) from the underlying distribution and pays (or charges) each participant
according to the relative performance of their marginal contributions. Precisely, the total reward for
participant tâ€™s bet df t is the value df t(x, y) minus the cost Cx(f t)âˆ’ Cx(f tâˆ’1).
Mechanism 1: The Market Template
MARKET announces f0 âˆˆ F
for t = 1, 2, . . . , T do
PARTICIPANT may query functionsâˆ‡fCx(f tâˆ’1) and f tâˆ’1(x, y) for examples (x, y)
PARTICIPANT t may submit a bet df t âˆˆ F to MARKET
MARKET updates state f t = f tâˆ’1 + df t
MARKET observes a true sample (x, y)
for t = 1, 2, . . . , T do
PARTICIPANT t receives payment df t(x, y) + Cx(f tâˆ’1)âˆ’ Cx(f t)
The design of cost-function prediction markets has been an area of active research over the past
several years, starting with [8] and many further refinements and generalizations [1, 6, 15]. The
general idea is that the mechanism can efficiently provide price quotes via a function C(Â·) which
acts as a potential on the space of outstandings shares; see [1] for a thorough review. In the present
work we have added an additional twist which is that the function Cx(Â·) is given an additional
parameterization of the observation x. We will not dive too deeply into the theoretical aspects of
this generalization, but this is a straightforward extension of existing theory.
Key special case: exponential family mechanism. For those more familiar with statistics and
machine learning, there is a natural and canonical family of problems that can be cast within the
general framework of Mechanism 1, which we will call the exponential family prediction mech-
anism following [2]. Assume that F can be parameterized as F = {fÎ¸ : Î¸ âˆˆ Rd}, that we
are given a sufficient statistics summary function Ï† : X Ã— Y â†’ Rd, and that function eval-
uation is given by fÎ¸(x, y) = ã€ˆÎ¸, Ï†(x, y)ã€‰. We let Cx(f) := log
âˆ«
Y exp(f(x, y))dy so that
Cx(fÎ¸) = log
âˆ«
Y exp(ã€ˆÎ¸, Ï†(x, y)ã€‰dy. In other words, we have chosen our mechanism to encode
a particular exponential family model, with Cx(Â·) chosen as the conditional log partition function
over the distribution on y given x. If the market has settled on a function fÎ¸, then one may interpret
that as the aggregate market belief on the distribution of X Ã— Y is
pÎ¸(x, y) = exp(ã€ˆÎ¸, Ï†(x, y)ã€‰ âˆ’A(Î¸)) where A(Î¸) = log
âˆ«
XÃ—Y exp(ã€ˆÎ¸, Ï†(x, y)ã€‰) dx dy.
How may we view this as a â€œmarket aggregateâ€ belief? Notice that if a trader observes the market
state of fÎ¸ and she is considering a bet of the form df = fÎ¸ âˆ’ fÎ¸â€² , the eventual profit will be
fÎ¸â€²(x, y)âˆ’ fÎ¸(x, y) + Cx(fÎ¸)âˆ’ Cx(fÎ¸â€²) = log
pÎ¸â€²(y|x)
pÎ¸(y|x)
.
I.e., the profit is precisely the conditional log likelihood ratio of the update Î¸ â†’ Î¸â€².
Example: Logistic regression. Let X = Rk, Y = {âˆ’1, 1}, and take F to be the set of func-
tions fÎ¸(x, y) = y Â· (Î¸>x) for Î¸ âˆˆ Rk. Then by our construction, Cx(f) = log(exp(f(x, 1)) +
exp(f(x,âˆ’1))) = log(exp(Î¸>x) + exp(âˆ’Î¸>x)), and we let f0 = f0 â‰¡ 0. The payoff of a
participant placing a bet which moves the market state to f1 = fÎ¸, upon outcome (x, y), is:
fÎ¸(x, y) + Cx(f0)âˆ’ Cx(fÎ¸) = yÎ¸>x+ log(2)âˆ’ log(exp(Î¸>x) + exp(âˆ’Î¸>x))
= log(2)âˆ’ log(1 + exp(âˆ’2yÎ¸>x)) ,
1This can easily be extended to a test set by taking the average performance over the test set.
3
which is simply negative logistic loss of the parameter choice 2Î¸. A participant wishing to maximize
profit under a belief distribution p(x, y) should therefore choose Î¸ via logistic regression,
Î¸âˆ— = arg min
Î¸
E
(x,y)âˆ¼p
[
log(1âˆ’ exp(2yÎ¸>x))
]
. (2)
2.2 Properties of the Market
We next describe two nice properties of Mechanism 1: incentive-compatibility and bounded bud-
get. Recall that, for the exponential family markets discussed above, a trader moving the market
hypothesis from f tâˆ’1 to f t was compensated according to the conditional log-likelihood ratio of
f tâˆ’1 and f t on the test data point. The implication is that traders are incentivized to minimize a
KL divergence between the marketâ€™s estimate of the distribution and the true underlying distribu-
tion. We refer to this property as incentive-compatibility because tradersâ€™ interests are aligned with
the mechanism designerâ€™s. This property indeed holds generally for Mechanism 1, where the KL
divergence is replaced with a general Bregman divergence corresponding to the Fenchel conjugate
of Cx(Â·); see Proposition 1 in the appendix for details.
Given that the mechanism must make a sequence of (possibly negative) payments to traders, a natural
question is whether there is the potential for large downside for the mechanism in terms of total
payment (budget). In the context of the exponential family mechanism, this question is easy to
answer: after a sequence of bets moving the market state parameter Î¸0 â†’ Î¸1 â†’ . . . â†’ Î¸final, the
total loss to the mechanism corresponds to the total payouts made to traders,âˆ‘
i
fÎ¸i+1(x, y)âˆ’ fÎ¸i(x, y) + Cx(fÎ¸i)âˆ’ Cx(fÎ¸i+1) = log
pÎ¸final(y|x)
pÎ¸0(y|x)
;
that is, the worst-case loss is exactly the worst-case conditional log-likelihood ratio. In the context
of logistic regression this quantity can always be guaranteed to be no more than log 2 as long as
the initial parameter is set to Î¸ = 0. For Mechanism 1 more generally, one has tight bounds on
the worst-case loss following from such results from prediction markets [1, 8], and we give a more
detailed statement in Proposition 2 in the appendix.
Price sensitivity parameter Î»C . In choosing the cost function family C = {Cx : x âˆˆ X}, an
important consideration is the â€œscaleâ€ of each Cx, or how quickly changes in the market hypothesis
f t translate to changes in the â€œinstantaneous pricesâ€ âˆ‡Cx(f t) (which give the marginal cost for an
infinitesimal bet df t+1). Formally, this is captured by the price sensitivity Î»C , defined as the upper
bound on the operator norm (with respect to the L1 norm) of the Hessian of the cost function Cx
(over all x). A choice of small Î»C translates to a small worst-case budget required by the mechanism.
However, it means that the market prices are sensitive in that the same update df t changes the prices
much more quickly. When we consider protecting the privacy of trader updates in Section 3, we will
see that privacy imposes restrictions on the price sensitivity.
2.3 A Nonparametric Mechanism via Kernel Methods
The framework we have discussed thus far has involved a general function space F as the â€œstateâ€
of the mechanism, and the contributions by participants are in the form of modifications to these
functions. One of the downsides of this generic template is that participants may not be able to reason
about F , and they may have information about the optimal f only through their own privately-held
dataset S âŠ‚ X Ã—Y . A more specific class of functions would be those parameterized by actual data.
This brings us to a well-studied type of non-parametric hypothesis class, namely the reproducing
kernel Hilbert space (RKHS). We can design a market based on an RKHS, which we will refer to
as a kernel market, that brings together a number of ideas including recent work of [21] as well as
kernel exponential families [4].
We have a positive semidefinite kernel k : Z Ã— Z â†’ R and associated reproducing kernel Hilbert
space F , with basis {fz(Â·) = k(z, Â·) : z âˆˆ Z}. The reproducing property is that for all f âˆˆ F ,
ã€ˆf, k(z, Â·)ã€‰ = f(z). Now each hypothesis f âˆˆ F can be expressed as f(Â·) =
âˆ‘
s Î±sk(zs, Â·) for
some collection of points {(Î±s, zs)}.
The kernel approach has several nice properties. One is a natural extension of the exponential family
mechanism using an RKHS as a building block of the class of exponential family distributions [4]. A
4
key assumption in the exponential family mechanism is that evaluating f can be viewed as an inner
product in some feature space; this is precisely what one has given a kernel framework. Specifically,
assume we have some PSD kernel k : X Ã— X â†’ R, where Y = {âˆ’1, 1}. Then we can define the
associated classification kernel kÌ‚ : (X Ã— Y) Ã— (X Ã— Y) â†’ R according to kÌ‚((x, y), (xâ€², yâ€²)) :=
yyâ€²k(x, xâ€²). Under certain conditions [4], we again can take Cx(f) = log
âˆ«
Y exp(f(x, y))dy, and
for any f in the RKHS associated to kÌ‚, we have an associated distribution of the form pf (x, y) âˆ
exp(f(x, y)). And again, a participant updating the market from f tâˆ’1 to f t is rewarded by the
conditional log-likelihood ratio of f tâˆ’1 and f t on the test data.
The second nice property mirrors one of standard kernel learning methods, namely that under cer-
tain conditions one need only search the subset of the RKHS spanned by the basis {k((xi, yi), Â·) :
(xi, yk) âˆˆ S}, where S is the set of available data; this is a direct result of the Representer Theo-
rem [17]. In the context of the kernel market, this suggests that participants need only interact with
the mechanism by pushing updates that lie in the span of their own data. In other words, we only
need to consider updates of the form df = Î±k((x, y), Â·). This naturally suggests the idea of directly
purchasing data points from traders.
Buying Data Points. So far, we have supposed that a participant knows what trade df t she prefers
to make. But what if she simply has a data point (x, y) drawn from the underlying distribution?
We would like to give this trader a â€œsimpleâ€ trading interface in which she can sell her data to the
mechanism without having to reason about the correct df t for this data point.
Our proposal is to mimic the behavior of natural learning algorithms, such as stochastic gradient
descent, when presented with (x, y). The market can offer the trader the purchase bundle corre-
sponding to the update of the learning algorithm on this data point. In principle, this approach can
be used with any online learning algorithm. In particular, stochastic gradient descent gives a clean
update rule, which we now describe. The expected profit (which is the negative of expected loss)
for trade df t is Ex
[
Cx(f
tâˆ’1 + df t)âˆ’ Cx(f tâˆ’1)âˆ’ Ey|x[df t(x, y)]
]
. Given a draw (x, y), the loss
function on which to take a gradient step is âˆ’
(
Cx(f
tâˆ’1 + df t)âˆ’ Cx(f tâˆ’1)âˆ’ df t(x, y)
)
, whose
gradient is âˆ’âˆ‡ftâˆ’1Cx + Î´x,y (where Î´x,y is the indicator on data point x, y). This suggests that the
market offer the participant the trade df t = 
(
âˆ‡ftâˆ’1Cx âˆ’ Î´x,y
)
, where  can be chosen arbitrarily
as a â€œlearning rateâ€. This can be interpreted as buying a unit of shares in the participantâ€™s data point
(x, y), then â€œhedgingâ€ by selling a small amount of all other shares in proportion to their current
prices (recall that the current prices are given byâˆ‡ftCx).
In the kernel setting, the choice of stochastic gradient descent may be somewhat problematic, be-
cause it can result in non-sparse share purchases. It may instead be desirable to use algorithms that
guarantee sparse updatesâ€”a modern discussion of such approaches can be found in [22, 23].
Given this framework, participants with access to a private set of samples from the true underlying
distribution can simply opt for this â€œstandard bundleâ€ corresponding to their data point, which is
precisely a stochastic gradient descent update. With a small enough learning rate, and assuming
that the data point is truly independent of the current hypothesis (i.e. (x, y) has not been previously
incorporated), the trade is guaranteed to make at least some positive profit in expectation. More
sophisticated alternative strategies are also possible of course, but even the proposed simple bet type
has earning potential.
3 Protecting Participantsâ€™ Privacy
We now extend the mechanism to protect privacy of the participants: An adversary observing the
hypotheses and prices of the mechanism, and even controlling the trades of other participants, should
not be able to infer too much about any one traderâ€™s update df t. This is especially relevant when
participants sell data to the mechanism and this data can be sensitive, e.g. medical data.
Here, privacy is formalized by (, Î´)-differential privacy, to be defined shortly. One intuitive charac-
terization is that, for any prior distribution some adversary has about a traderâ€™s data, the adversaryâ€™s
posterior belief after observing the mechanism would be approximately the same even if the trader
did not participate at all. The idea is that, rather than posting the exact prices and trades made in the
market, we will publish noisy versions, with the random noise giving the above guarantee.
5
A naive approach would be to add independent noise to each participantâ€™s trade. However, this would
require a prohibitively-large amount of noise; the final market hypothesis would be determined by
the random noise just as much as by the data and trades. The central challenge is to add carefully
correlated noise that is large enough to hide the effects of any one participantâ€™s data point, but not
so large that the prices (equivalently, hypothesis) become meaningless. We show this is possible
by adjusting the â€œprice sensitivityâ€ Î»C of the mechanism, a measure of how fast prices change
in response to trades defined in 2.2. It will turn out to suffice to set the price sensitivity to be
O(1/polylog T ) when there are T participants. This can roughly be interpreted as saying that any
one participant does not move the market price noticeably (so their privacy is protected), but just
O(polylog T ) traders together can move the prices completely.
We now formally define differential privacy and discuss two useful tools at our disposal.
3.1 Differential Privacy and Tools
Differential privacy in our context is defined as follows. Consider a randomized function M op-
erating on inputs of the form ~f = (df1, . . . , dfT ) and having outputs of the form s. Then M is
(, Î´)-differentially private if, for any coordinate t of the vector, any two distinct df t1, df
t
2, and any
(measurable) set of outputs S, we have Pr[M(fâˆ’t, df t1) âˆˆ S)] â‰¤ e Pr[M(fâˆ’t, df t2) âˆˆ S] + Î´. The
notation fâˆ’t means the vector ~f with the tth entry removed.
Intuitively, M is private if modifying the tth entry in the vector to a different entry does not change
the distribution on outputs too much. In our case, the data to be protected will be the trade df t of each
participant t, and the space of outputs will be the entire sequence of prices/predictions published by
the mechanism.
To preserve privacy, each trade must have a bounded size (e.g. consist only of one data point). To
enforce this, we define the following parameter chosen by the mechanism designer:
âˆ† = max
allowed df
âˆš
ã€ˆdf, dfã€‰, (3)
where the maximum is over all trades df allowed by the mechanism. That is, âˆ† is a scalar capturing
the maximum allowed size of any one trade. For instance, if all trades are restricted to be of the form
df = Î±k(z, Â·), then we would have âˆ† = maxÎ±,z Î±
âˆš
k(z, z).
We next describe the two tools we require.
Tool 1: Private functions via Gaussian processes. Given a current market state f t = f0 +df1 +
Â· Â· Â· + df t, where f t lies in a RKHS, we construct a â€œprivateâ€ version fÌ‚ t such that queries to fÌ‚ t are
â€œaccurateâ€ â€” close to the outputs of f t â€” but also private with respect to each df j . In fact, it will
become convenient to privately output partial sums of trades, so we wish to output a fÌ‚t1:t2 that is
private and approximates ft1:t2 =
âˆ‘t2
j=t1
df j . This is accomplished by the following construction
due to [11].
Theorem 1 ([11], Corollary 9). LetG be the sample path of a Gaussian process with mean zero and
whose covariance is given by the kernel function k.2 Then
fÌ‚t1:t2 = ft1:t2 + âˆ†
âˆš
2 ln(2/Î´)
 G . (4)
is (, Î´)-differentially private with respect to each df j for j âˆˆ {t1, . . . , t2}.
In general, fÌ‚t1:t2 may be an infinite-dimensional object and thus impossible to finitely represent.
In this case, the theorem implies that releasing the results of any number of queries fÌ‚t1:t2(z) is
differentially private. (Of course, the more queries that are released, the larger the chance of high
error on some query.) This is computationally feasible as each sampleG(z) is simply a sample from
a Gaussian having known covariance with the previous samples drawn.
Unfortunately, it would not be sufficient to independently release fÌ‚1:t at each time t, because the
amount of noise required would be prohibitive. This leads us to our next tool.
2Formally, each G(z) is a random variable and, for any finite subset of Z , the corresponding variables are
distributed as a multivariate normal with covariance given by k.
6
â€¢
df0
â€¢
df1
â€¢
df2
â€¢
df3
â€¢
df4
â€¢
df5
â€¢
df6
â€¢
df7
â€¢
df8
â€¢
df9
â€¢
df10
â€¢
df11
â€¢
df12
â€¢
df13
â€¢
df14
â€¢
df15
â€¢
df16
Figure 1: Picturing the continual observation technique for preserving privacy. Each df t is a trade (e.g. a data
point sold to the market). The goal is to release, at each time step t, a noisy version of f t =
âˆ‘t
j=1 df
j . To do
so, start at t and follow the arrow back to s(t). Take the partial sum of df j for j from s(t) to t and add some
random noise. Trace the next arrow from s(t) to s(s(t)) to get another partial sum and add noise to that sum
as well. Repeat until 0 is reached, then add together all the noisy partial sums to get the output at time t, which
will equal f t plus noise. The key point is that we can re-use many of the noisy partial sums in many different
time steps. For instance, the noisy partial sum from 0 to 8 can be re-used when releasing all of f9, . . . , f15.
Meanwhile, each df t participates in few noisy partial sums (the number of arrows passing above it).
Tool 2: Continual observation technique. The idea of this technique, pioneered by [9, 5], is to
construct fÌ‚ t =
âˆ‘t
j=0 df
t by adding together noisy partial sums of the form fÌ‚t1:t2 as constructed in
Equation 4. The idea for choosing these partial sums is pictured in Figure 1: For a function s(t) that
returns an integer smaller than t, we take fÌ‚ t = fÌ‚s(t)+1:t + fÌ‚s(s(t))+1:s(t) + Â· Â· Â·+ fÌ‚0:0. Specifically,
s(t) is determined by writing t in binary, then flipping the rightmost â€œoneâ€ bit to zero. This is
pictured in Figure 1. The intuition behind why this technique helps is twofold. First, the total noise
in fÌ‚ t is the sum of noises of its partial sums, and it turns out that there are at most dlog T e terms.
Second, the total noise we need to add to protect privacy is governed by how many different partial
sums each df j participates in, and it turns out that this number is also at most dlog T e. This allows
for much better privacy and accuracy guarantees than naively treating each step independently.
3.2 Mechanism and Results
Combining our market template in Mechanism 1 with the above privacy tools, we obtain Mecha-
nism 2. There are some key differences. First, we have a bound Q on the total number of queries.
(Each query x returns the instantaneous prices in the market for x.) This is because each query
reveals information about the participants, so intuitively, allowing too many queries must sacrifice
either privacy or accuracy. Fortunately, this bound Q can be an arbitrarily large polynomial in the
number of traders without affecting the quality of the results. Second, we have PAC-style guaran-
tees on accuracy: with probability 1âˆ’ Î³, all price queries return values within Î± of their true prices.
Third, it is no longer straightforward to compute and represent the market pricesâˆ‡Cx(fÌ‚ t) unless Y
is finite. We leave the more general analysis of Mechanism 2 to future work.
Either exactly or approximately, Mechanism 2 inherits the desirable properties of Mechanism 1, such
as bounded budget and incentive-compatitibility (that is, participants are incentivized to minimize
the risk of the market hypothesis). In addition, we show that it preserves privacy while maintaining
accuracy, for an appropriate choice of the price sensitivity Î»C .
Theorem 2. Consider Mechanism 2, where âˆ† is the maximimum trade size (Equation 3) and d =
|Y|. Then Mechanism 2 is (, Î´) differentially private and, with T traders and Q price queries,
has the following accuracy guarantee: with probability 1âˆ’ Î³, for each query x the returned prices
satisfy â€–âˆ‡Cx(fÌ‚ t)âˆ’âˆ‡Cx(f t)â€–âˆ â‰¤ Î± by setting
Î»C =
Î±
2dâˆ†2
âˆš
ln QdÎ³ ln
2 log T
Î´ log(T )
3
.
If one for example takes Î´, Î³ = exp [âˆ’polylog(Q,T )], then except for a superpolynomially low fail-
ure probability, Mechanism 2 answers all queries to within accuracy Î± by setting the price sensitivity
to be Î»C = O (Î±/polylog(Q,T )). We note, however, that this is a somewhat weaker guarantee
than is usually desired in the differential privacy literature, where ideally Î´ is exponentially small.
7
Mechanism 2: Privacy Protected Market
Parameters: , Î´ (privacy), Î±, Î³ (accuracy), k (kernel), âˆ† (trade size 3), Q (#queries), T (#traders)
MARKET announces fÌ‚0 = f0, sets r = 0, sets C with Î»C = Î»C(, Î´, Î±, Î³,âˆ†, Q, T ) (Theorem 2)
for t = 1, 2, . . . , T do
PARTICIPANT t proposes a bet df t
MARKET updates true position f t = f tâˆ’1 + df t
MARKET instantiates fÌ‚s(t)+1,t as defined in Equation 4
while r â‰¤ Q and some OBSERVER wishes to make a query do
OBSERVER r submits pricing query on x
MARKET returns pricesâˆ‡Cx(fÌ‚ t), where fÌ‚ t = fÌ‚s(t)+1:t + fÌ‚s(s(t))+1:s(t) + Â· Â· Â·+ fÌ‚0:0
MARKET sets r â† r + 1
MARKET observes a true sample (x, y)
for t = 1, 2, . . . , T do
PARTICIPANT receives payment f tâˆ’1(x, y)âˆ’ f t(x, y)âˆ’ Cx(fÌ‚ tâˆ’1 + df t) + Cx(fÌ‚ tâˆ’1)
Computingâˆ‡Cx(fÌ‚ t). We have already discussed limiting to finite |Y| in order to efficiently com-
pute the marginal prices âˆ‡Cx(fÌ‚ t). However, it is still not immediately clear how to compute these
prices, and hence how to implement Mechanism 2. Here, we show that the problem can be solved
when C comes from an exponential family, so that Cx(f) = log
âˆ«
Y exp [f(x, y)] dy. In this case,
the marginal prices given by the gradient of C have a nice exponential-weights form, namely the
price of shares in (x, y) is ptx(y) = âˆ‡yCx(f t) = e
f(x,y)âˆ‘
yâˆˆY e
f(x,y) . Thus evaluating the prices can be
done by evaluating f t(x, y) for each y âˆˆ Y .
We also note that the worst-case bound used here could be greatly improved by taking into account
the structure of the kernel. For â€œsmoothâ€ cases such as the Gaussian kernel, querying a second
point very close to the first one requires very little additional randomness and builds up very little
additional error. We gave only a worst-case bound that holds for all kernels.
Adding a transaction fee. In the appendix, we discuss the potential need for transaction fees.
Adding a small Î˜(Î±) fee suffices to deter arbitrage opportunities introduced by noisy pricing.
Discussion
The main contribution of this work was to bring together several tools to construct a mechanism
for incentivized data aggregation with â€œcontest-likeâ€ incentive properties, privacy guarantees, and
limited downside for the mechanism.
Our proposed mechanisms are also extensions of the prediction market literature. Building upon the
work of Abernethy et al. [1] we introduce the following innovations:
â€¢ Conditional markets. Our framework of Mechanism 1 can be interpreted as a prediction market
for conditional predictions p(y|x) rather than a classic market which would elicit the joint dis-
tribution p(x, y), or just the marginals. (This is similar to decision markets [12, 7], but without
out the associated incentive problems.) Naturally then, we couple conditional predictions with
restricted hypothesis spaces, allowing F to capture, e.g., a linear relationship between x and y.
â€¢ Nonparametric securities. We also extend to nonparametric hypothesis spaces using kernels,
following the kernel-based scoring rules of [21].
â€¢ Privacy guarantees. We provide the first private prediction market (to our knowledge), showing
that information about individual trades is not revealed. Our approach for preserving privacy also
holds in the classic prediction market setting with similar privacy and accuracy guarantees.
Many directions remain for future work. These mechanisms could be made more practical and
perhaps even better privacy guarantees derived, especially in nonparametric settings. One could also
explore the connections to similar settings, such as when agents have costs for acquiring data.
Acknoledgements J. Abernethy acknowledges the generous support of the US National Science
Foundation under CAREER Grant IIS-1453304 and Grant IIS-1421391.
8
References
[1] Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via convex
optimization, and a connection to online learning. ACM Transactions on Economics and Computation,
1(2), May 2013.
[2] Jacob Abernethy, Sindhu Kutty, SeÌbastien Lahaie, and Rahul Sami. Information aggregation in expo-
nential family markets. In Proceedings of the fifteenth ACM conference on Economics and computation,
pages 395â€“412. ACM, 2014.
[3] Jacob D Abernethy and Rafael M Frongillo. A collaborative mechanism for crowdsourcing prediction
problems. In Advances in Neural Information Processing Systems, pages 2600â€“2608, 2011.
[4] SteÌphane Canu and Alex Smola. Kernel methods and the exponential family. Neurocomputing, 69(7):714â€“
720, 2006.
[5] T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Transac-
tions on Information and System Security (TISSEC), 14(3):26, 2011.
[6] Y. Chen and J.W. Vaughan. A new understanding of prediction markets via no-regret learning. In Pro-
ceedings of the 11th ACM Conference on Electronic Commerce (EC), pages 189â€“198, 2010.
[7] Yiling Chen, Ian Kash, Mike Ruberry, and Victor Shnayder. Decision markets with good incentives. In
Internet and Network Economics, pages 72â€“83. Springer, 2011.
[8] Yiling Chen and David M. Pennock. A utility framework for bounded-loss market makers. In In Proceed-
ings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI), pages 49â€“56, 2007.
[9] Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N Rothblum. Differential privacy under continual
observation. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 715â€“724.
ACM, 2010.
[10] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and
Trends in Theoretical Computer Science, 2014.
[11] Rob Hall, Alessandro Rinaldo, and Larry Wasserman. Differential privacy for functions and functional
data. The Journal of Machine Learning Research, 14(1):703â€“727, 2013.
[12] R Hanson. Decision markets. Entrepreneurial Economics: Bright Ideas from the Dismal Science, pages
79â€“85, 2002.
[13] R. Hanson. Combinatorial information market design. Information Systems Frontiers, 5(1):105â€“119,
2003.
[14] R. Hanson. Logarithmic market scoring rules for modular combinatorial information aggregation. Journal
of Prediction Markets, 1(1):3â€“15, 2007.
[15] Abraham Othman and Tuomas Sandholm. Automated market makers that enable new settings: extending
constant-utility cost functions. In Proceedings of the Second Conference on Auctions, Market Mechanisms
and their Applications (AMMA), pages 19â€“30, 2011.
[16] David M. Pennock and Rahul Sami. Computational aspects of prediction markets. In Noam Nisan,
Tim Roughgarden, Eva Tardos, and Vijay V. Vazirani, editors, Algorithmic Game Theory, chapter 26.
Cambridge University Press, 2007.
[17] Bernhard SchoÌˆlkopf and Alexander J Smola. Learning with kernels: Support vector machines, regular-
ization, optimization, and beyond. MIT press, 2002.
[18] Amos J. Storkey. Machine learning markets. In Proceedings of AI and Statistics (AISTATS), pages 716â€“
724, 2011.
[19] J. Wolfers and E. Zitzewitz. Prediction markets. Journal of Economic Perspectives, 18(2):107â€“126, 2004.
[20] Justin Wolfers and Eric Zitzewitz. Interpreting prediction market prices as probabilities. Technical report,
National Bureau of Economic Research, 2006.
[21] Erik Zawadzki and SeÌbastien Lahaie. Nonparametric scoring rules. In Proceedings of the Twenty-Ninth
AAAI Conference on Artificial Intelligence, 2015.
[22] Lijun Zhang, Rong Jin, Chun Chen, Jiajun Bu, and Xiaofei He. Efficient online learning for large-scale
sparse kernel logistic regression. In AAAI, 2012.
[23] Lijun Zhang, Jinfeng Yi, Rong Jin, Ming Lin, and Xiaofei He. Online kernel learning with a near optimal
sparsity bound. In Proceedings of the 30th International Conference on Machine Learning (ICML-13),
pages 621â€“629, 2013.
9
