


Paper ID = 5823
Title = Interpolating Convex and Non-Convex Tensor
Decompositions via the Subspace Norm
Qinqing Zheng
University of Chicago
qinqing@cs.uchicago.edu
Ryota Tomioka
Toyota Technological Institute at Chicago
tomioka@ttic.edu
Abstract
We consider the problem of recovering a low-rank tensor from its noisy obser-
vation. Previous work has shown a recovery guarantee with signal to noise ratio
O(ndK/2e/2) for recovering a Kth order rank one tensor of size n × · · · × n by
recursive unfolding. In this paper, we first improve this bound to O(nK/4) by a
much simpler approach, but with a more careful analysis. Then we propose a new
norm called the subspace norm, which is based on the Kronecker products of fac-
tors obtained by the proposed simple estimator. The imposed Kronecker structure
allows us to show a nearly idealO(
√
n+
√
HK−1) bound, in which the parameter
H controls the blend from the non-convex estimator to mode-wise nuclear norm
minimization. Furthermore, we empirically demonstrate that the subspace norm
achieves the nearly ideal denoising performance even with H = O(1).
1 Introduction
Tensor is a natural way to express higher order interactions for a variety of data and tensor decom-
position has been successfully applied to wide areas ranging from chemometrics, signal processing,
to neuroimaging; see [15, 18] for a survey. Moreover, recently it has become an active area in the
context of learning latent variable models [3].
Many problems related to tensors, such as, finding the rank, or a best rank-one approaximation of
a tensor is known to be NP hard [11, 8]. Nevertheless we can address statistical problems, such as,
how well we can recover a low-rank tensor from its randomly corrupted version (tensor denoising)
or from partial observations (tensor completion). Since we can convert a tensor into a matrix by
an operation known as unfolding, recent work [25, 19, 20, 13] has shown that we do get nontrivial
guarantees by using some norms or singular value decompositions. More specifically, Richard &
Montanari [20] has shown that when a rank-one Kth order tensor of size n × · · · × n is corrupted
by standard Gaussian noise, a nontrivial bound can be shown with high probability if the signal to
noise ratio β/σ % ndK/2e/2 by a method called the recursive unfolding1. Note that β/σ %
√
n
is sufficient for matrices (K = 2) and also for tensors if we use the best rank-one approximation
(which is known to be NP hard) as an estimator. On the other hand, Jain & Oh [13] analyzed the
tensor completion problem and proposed an algorithm that requiresO(n3/2·polylog(n)) samples for
K = 3; while information theoretically we need at least Ω(n) samples and the intractable maximum
likelihood estimator would require O(n · polylog(n)) samples. Therefore, in both settings, there is
a wide gap between the ideal estimator and current polynomial time algorithms. A subtle question
that we will address in this paper is whether we need to unfold the tensor so that the resulting matrix
become as square as possible, which was the reasoning underlying both [19, 20].
As a parallel development, non-convex estimators based on alternating minimization or nonlinear
optimization [1, 21] have been widely applied and have performed very well when appropriately
1We say an % bn if there is a constant C > 0 such that an ≥ C · bn.
1
Table 1: Comparison of required signal-to-noise ratio β/σ of different algorithms for recovering
a Kth order rank one tensor of size n × · · · × n contaminated by Gaussian noise with Standard
deviation σ. See model (2). The bound for the ordinary unfolding is shown in Corollary 1. The
bound for the subspace norm is shown in Theorem 2. The ideal estimator is proven in Appendix A.
Overlapped/
Latent nuclear
norm[23]
Recursive
unfolding[20]/
square
norm[19]
Ordinary un-
folding
Subspace norm (pro-
posed)
Ideal
O(n(K−1)/2) O(ndK/2e/2) O(nK/4) O(
√
n +
√
HK−1) O(
√
nK log(K))
set up. Therefore it would be of fundamental importance to connect the wisdom of non-convex
estimators with the more theoretically motivated estimators that recently emerged.
In this paper, we explore such a connection by defining a new norm based on Kronecker products of
factors that can be obtained by simple mode-wise singular value decomposition (SVD) of unfoldings
(see notation section below), also known as the higher-order singular value decomposition (HOSVD)
[6, 7]. We first study the non-asymptotic behavior of the leading singular vector from the ordinary
(rectangular) unfolding X(k) and show a nontrivial bound for signal to noise ratio β/σ % nK/4.
Thus the result also applies to odd order tensors confirming a conjecture in [20]. Furthermore, this
motivates us to use the solution of mode-wise truncated SVDs to construct a new norm. We propose
the subspace norm, which predicts an unknown low-rank tensor as a mixture of K low-rank tensors,
in which each term takes the form
foldk(M
(k)(P̂
(1)
⊗ · · · ⊗ P̂
(k−1)
⊗ P̂
(k+1)
⊗ · · · ⊗ P̂
(K)
)>),
where foldk is the inverse of unfolding (·)(k), ⊗ denotes the Kronecker product, and P̂
(k)
∈ Rn×H
is a orthonormal matrix estimated from the mode-k unfolding of the observed tensor, for k =
1, . . . ,K; H is a user-defined parameter, and M (k) ∈ Rn×HK−1 . Our theory tells us that with
sufficiently high signal-to-noise ratio the estimated P̂
(k)
spans the true factors.
We highlight our contributions below:
1. We prove that the required signal-to-noise ratio for recovering a Kth order rank one tensor from
the ordinary unfolding is O(nK/4). Our analysis shows a curious two phase behavior: with high
probability, when nK/4 - β/σ - nK/2, the error shows a fast decay as 1/β4; for β/σ % nK/2, the
error decays slowly as 1/β2. We confirm this in a numerical simulation.
2. The proposed subspace norm is an interpolation between the intractable estimators that directly
control the rank (e.g., HOSVD) and the tractable norm-based estimators. It becomes equivalent to
the latent trace norm [23] when H = n at the cost of increased signal-to-noise ratio threshold (see
Table 1).
3. The proposed estimator is more efficient than previously proposed norm based estimators, because
the size of the SVD required in the algorithm is reduced from n× nK−1 to n×HK−1.
4. We also empirically demonstrate that the proposed subspace norm performs nearly optimally for
constant order H .
Notation
Let X ∈ Rn1×n2×···×nK be a Kth order tensor. We will often use n1 = · · · = nK = n to
simplify the notation but all the results in this paper generalizes to general dimensions. The inner
product between a pair of tensors is defined as the inner products of them as vectors; i.e., 〈X ,W〉 =
〈vec(X ), vec(W)〉. For u ∈ Rn1 ,v ∈ Rn2 ,w ∈ Rn3 , u ◦ v ◦ w denotes the n1 × n2 × n3
rank-one tensor whose i, j, k entry is uivjwk. The rank of X is the minimum number of rank-one
tensors required to write X as a linear combination of them. A mode-k fiber of tensor X is an nk
dimensional vector that is obtained by fixing all but the kth index of X . The mode-k unfolding X(k)
of tensor X is an nk ×
∏
k′ 6=k nk′ matrix constructed by concatenating all the mode-k fibers along
columns. We denote the spectral and Frobenius norms for matrices by ‖ · ‖ and ‖ · ‖F , respectively.
2
2 The power of ordinary unfolding
2.1 A perturbation bound for the left singular vector
We first establish a bound on recovering the left singular vector of a rank-one n ×m matrix (with
m > n) perturbed by random Gaussian noise.
Consider the following model known as the information plus noise model [4]:
X̃ = βuv> + σE, (1)
where u and v are unit vectors, β is the signal strength, σ is the noise standard deviation, and
the noise matrix E is assumed to be random with entries sampled i.i.d. from the standard normal
distribution. Our goal is to lower-bound the correlation between u and the top left singular vector û
of X̃ for signal-to-noise ratio β/σ % (mn)1/4 with high probability.
A direct application of the classic Wedin perturbation theorem [28] to the rectangular matrix X̃ does
not provide us the desired result. This is because it requires the signal to noise ratio β/σ ≥ 2‖E‖.
Since the spectral norm of E scales as Op(
√
n+
√
m) [27], this would mean that we require β/σ %
m1/2; i.e., the threshold is dominated by the number of columns m, if m ≥ n.
Alternatively, we can view û as the leading eigenvector of X̃X̃
>
, a square matrix. Our key insight
is that we can decompose X̃X̃
>
as follows:
X̃X̃
>
= (β2uu> +mσ2I) + (σ2EE> −mσ2I) + βσ(uv>E> + Evu>).
Note that u is the leading eigenvector of the first term because adding an identity matrix does not
change the eigenvectors. Moreover, we notice that there are two noise terms: the first term is a
centered Wishart matrix and it is independent of the signal β; the second term is Gaussian distributed
and depends on the signal β.
This implies a two-phase behavior corresponding to either the Wishart or the Gaussian noise term
being dominant, depending on the value of β. Interestingly, we get a different speed of convergence
for each of these phases as we show in the next theorem (the proof is given in Appendix D.1).
Theorem 1. There exists a constant C such that with probability at least 1− 4e−n, if m/n ≥ C,
|〈û,u〉| ≥

1− Cnm
(β/σ)4
, if
√
m >
β
σ
≥ (Cnm) 14 ,
1− Cn
(β/σ)2
, if
β
σ
≥
√
m,
otherwise, |〈û,u〉| ≥ 1− Cn(β/σ)2 if β/σ ≥
√
Cn.
In other words, if X̃ has sufficiently many more columns than rows, as the signal to noise ratio β/σ
increases, û first converges to u as 1/β4, and then as 1/β2. Figure 1(a) illustrates these results.
We randomly generate a rank-one 100 × 10000 matrix perturbed by Gaussian noise, and measure
the distance between û and u. The phase transition happens at β/σ = (nm)1/4, and there are two
regimes of different convergence rates as Theorem 1 predicts.
2.2 Tensor Unfolding
Now let’s apply the above result to the tensor version of information plus noise model studied by
[20]. We consider a rank one n×· · ·×n tensor (signal) contaminated by Gaussian noise as follows:
Y = X ∗ + σE = βu(1) ◦ · · · ◦ u(K) + σE , (2)
where factors u(k) ∈ Rn, k = 1, . . . ,K, are unit vectors, which are not necessarily identical, and
the entries of E ∈ Rn×···×n are i.i.d samples from the normal distribution N (0, 1). Note that this is
slightly more general (and easier to analyze) than the symmetric setting studied by [20].
3
log(β/σ)
2 3 4 5 6
-10
-8
-6
-4
-2
0
n = 100,m = 10000
log(1− |〈u, û〉|)
−3.81 log(β/α) + 12.76
−2.38 log(β/α) + 6.18
log
(
(nm)1/4
)
log (
√
m)
(a) Synthetic experiment showing phase transition
at β/σ = (nm)1/4 and regimes with different rates
of convergence. See Theorem 1.
σ(
∏
i ni)
1
4
0 5 10 15 20 25 30
c
o
rr
e
la
ti
o
n
0
0.2
0.4
0.6
0.8
1 |〈u1, û1〉| - [20 40 60]
|〈u2, û2〉| - [20 40 60]
|〈u1, û1〉| - [40 80 120]
|〈u2, û2〉| - [40 80 120]
β1
β2
(b) Synthetic experiment showing phase transition
at β = σ(
∏
k nk)
1/4 for odd order tensors. See
Corollary 1.
Figure 1: Numerical demonstration of Theorem 1 and Corollary 1.
Several estimators for recovering X ∗ from its noisy version Y have been proposed (see Table 1).
Both the overlapped nuclear norm and latent nuclear norm discussed in [23] achives the relative
performance guarantee
|||X̂ − X ∗|||F /β ≤ Op
(
σ
√
nK−1/β
)
, (3)
where X̂ is the estimator. This bound implies that if we want to obtain relative error smaller than ε,
we need the signal to noise ratio β/σ to scale as β/σ %
√
nK−1/ε.
Mu et al. [19] proposed the square norm, defined as the nuclear norm of the matrix obtained by
grouping the first bK/2c indices along the rows and the last dK/2e indices along the columns.
This norm improves the right hand side of inequality (3) to Op(σ
√
ndK/2e/β), which translates to
requiring β/σ %
√
ndK/2e/ε for obtaining relative error ε. The intuition here is the more square the
unfolding is the better the bound becomes. However, there is no improvement for K = 3.
Richard and Montanari [20] studied the (symmetric version of) model (2) and proved that a recursive
unfolding algorithm achieves the factor recovery error dist(û(k),u(k)) = εwith β/σ %
√
ndK/2e/ε
with high probability, where dist(u,u′) := min(‖u − u′‖, ‖u + u′‖). They also showed that the
randomly initialized tensor power method [7, 16, 3] can achieve the same error ε with slightly worse
threshold β/σ % max(
√
n/ε2, nK/2)
√
K logK also with high probability.
The reasoning underlying both [19] and [20] is that square unfolding is better. However, if we take
the (ordinary) mode-k unfolding
Y (k) = βu
(k)
(
u(k−1) ⊗ · · · ⊗ u(1) ⊗ u(K) ⊗ · · · ⊗ u(k+1)
)>
+ σE(k), (4)
we can see (4) as an instance of information plus noise model (1) where m/n = nK−2. Thus the
ordinary unfolding satisfies the condition of Theorem 1 for n or K large enough.
Corollary 1. Consider a K(≥ 3)th order rank one tensor contaminated by Gaussian noise as in
(2). There exists a constant C such that if nK−2 ≥ C, with probability at least 1− 4Ke−n, we have
dist2(û(k),u(k)) ≤

2CnK
(β/σ)4
, if n
K−1
2 > β/σ ≥ C 14nK4 ,
2Cn
(β/σ)2
, if β/σ ≥ n
K−1
2 ,
for k = 1, . . . ,K,
where û(k) is the leading left singular vector of the rectangular unfolding Y (k).
This proves that as conjectured by [20], the threshold β/σ % nK/4 applies not only to the even
order case but also to the odd order case. Note that Hopkins et al. [10] have shown a similar
result without the sharp rate of convergence. The above corollary easily extends to more general
n1 × · · · × nK tensor by replacing the conditions by
√∏
` 6=k n` > β/σ ≥ (C
∏K
k=1 nk)
1/4 and
β/σ ≥
√∏
6̀=k n`. The result also holds when X ∗ has rank higher than 1; see Appendix E.
4
We demonstrate this result in Figure 1(b). The models behind the experiment are slightly more
general ones in which [n1, n2, n3] = [20, 40, 60] or [40, 80, 120] and the signal X ∗ is rank two with
β1 = 20 and β2 = 10. The plot shows the inner products 〈u(1)1 , û
(1)
1 〉 and 〈u
(1)
2 , û
(1)
2 〉 as a measure
of the quality of estimating the two mode-1 factors. The horizontal axis is the normalized noise
standard deviation σ(
∏K
k=1 nk)
1/4. We can clearly see that the inner product decays symmetrically
around β1 and β2 as predicted by Corollary 1 for both tensors.
3 Subspace norm for tensors
Suppose the true tensor X ∗ ∈ Rn×···×n admits a minimum Tucker decomposition [26] of rank
(R, . . . , R):
X ∗ =
∑R
i1=1
· · ·
∑R
iK=1
βi1i2...iKu
(1)
i1
◦ · · · ◦ u(K)iK . (5)
If the core tensor C = (βi1...iK ) ∈ RR×···×R is superdiagonal, the above decomposition reduces to
the canonical polyadic (CP) decomposition [9, 15]. The mode-k unfolding of the true tensor X ∗ can
be written as follows:
X∗(k) = U
(k)C(k)
(
U (1) ⊗ · · · ⊗U (k−1) ⊗U (k+1) ⊗ · · · ⊗U (K)
)>
, (6)
where C(k) is the mode-k unfolding of the core tensor C; U (k) is a n × R matrix U (k) =
[u
(k)
1 , . . . ,u
(k)
R ] for k = 1, . . . ,K. Note that U
(k) is not necessarily orthogonal.
Let X∗(k) = P
(k)Λ(k)Q(k)
>
be the SVD of X∗(k). We will observe that
Q(k) ∈ Span
(
P (1) ⊗ · · · ⊗ P (k−1) ⊗ P (k+1) ⊗ · · · ⊗ P (K)
)
(7)
because of (6) and U (k) ∈ Span(P (k)).
Corollary 1 shows that the left singular vectors P (k) can be recovered under mild conditions; thus
the span of the right singular vectors can also be recovered. Inspired by this, we define a norm that
models a tensor X as a mixture of tensors Z(1), . . . ,Z(K). We require that the mode-k unfolding
of Z(k), i.e. Z(k)(k), has a low rank factorization Z
(k)
(k) = M
(k)S(k)
>
, where M (k) ∈ Rn×HK−1 is a
variable, and S(k) ∈ RnK−1×HK−1 is a fixed arbitrary orthonormal basis of some subspace, which
we choose later to have the Kronecker structure in (7).
In the following, we define the subspace norm, suggest an approach to construct the right factor
S(k), and prove the denoising bound in the end.
3.1 The subspace norm
Consider a Kth order tensor of size n× · · ·n.
Definition 1. Let S(1), . . . ,S(K) be matrices such that S(k) ∈ RnK−1×HK−1 with H ≤ n. The
subspace norm for a Kth order tensor X associated with {S(k)}Kk=1 is defined as
|||X |||s :=
{
inf{M(k)}Kk=1
∑K
k=1 ‖M
(k)‖∗, if X ∈ Span({S(k)}Kk=1),
+∞, otherwise,
where ‖ · ‖∗ is the nuclear norm, and Span({S(k)}Kk=1) :=
{
X ∈ Rn×···×n :
∃M (1), . . . ,M (K),X =
∑K
k=1 foldk(M
(k)S(k)
>
)
}
.
In the next lemma (proven in Appendix D.2), we show the dual norm of the subspace norm has a sim-
ple appealing form. As we see in Theorem 2, it avoids the O(
√
nK−1) scaling (see the first column
of Table 1) by restricting the influence of the noise term in the subspace defined by S(1), . . . ,S(K).
Lemma 1. The dual norm of |||·|||s is a semi-norm
|||X |||s∗ = max
k=1,...,K
‖X(k)S(k)‖,
where ‖ · ‖ is the spectral norm.
5
3.2 Choosing the subspace
A natural question that arises is how to choose the matrices S(1), . . . ,S(k).
Lemma 2. Let the X∗(k) = P
(k)Λ(k)Q(k) be the SVD of X∗(k), where P
(k) is n × R and Q(k) is
nK−1 ×R. Assume that R ≤ n and U (k) has full column rank. It holds that for all k,
i) U (k) ∈ Span(P (k)),
ii) Q(k) ∈ Span
(
P (1) ⊗ · · · ⊗ P (k−1) ⊗ P (k+1) ⊗ · · · ⊗ P (K)
)
.
Proof. We prove the lemma in Appendix D.4.
Corollary 1 shows that when the signal to noise ratio is high enough, we can recover P (k) with high
probability. Hence we suggest the following three-step approach for tensor denoising:
(i) For each k, unfold the observation tensor in mode k and compute the top H left singular
vectors. Concatenate these vectors to obtain a n×H matrix P̂
(k)
.
(ii) Construct S(k) as S(k) = P̂
(1)
⊗ · · · ⊗ P̂
(k−1)
⊗ P̂
(k+1)
⊗ · · · ⊗ P̂
(K)
.
(iii) Solve the subspace norm regularized minimization problem
min
X
1
2
|||Y − X |||2F + λ|||X |||s, (8)
where the subspace norm is associated with the above defined {S(k)}Kk=1.
See Appendix B for details.
3.3 Analysis
Let Y ∈ Rn×···×n be a tensor corrupted by Gaussian noise with standard deviation σ as follows:
Y = X ∗ + σE . (9)
We define a slightly modified estimator X̂ as follows:
X̂ = arg min
X ,{M(k)}Kk=1
{1
2
|||Y − X |||2F + λ|||X |||s : X =
K∑
k=1
foldk
(
M (k)S(k)
>)
, {M (k)}Kk=1 ∈M(ρ)
}
(10)
where M(ρ) is a restriction of the set of matrices M (k) ∈ Rn×HK−1 , k = 1, . . . ,K defined as
follows:
M(ρ) :=
{
{M (k)}Kk=1 : ‖foldk(M
(k))(`)‖ ≤
ρ
K
(
√
n+
√
HK−1),∀k 6= `
}
.
This restriction makes sure that M (k), k = 1, . . . ,K, are incoherent, i.e., each M (k) has a spectral
norm that is as low as a random matrix when unfolded at a different mode `. Similar assumptions
were used in low-rank plus sparse matrix decomposition [2, 12] and for the denoising bound for the
latent nuclear norm [23].
Then we have the following statement (we prove this in Appendix D.3).
Theorem 2. Let Xp be any tensor that can be expressed as
Xp =
K∑
k=1
foldk
(
M (k)p S
(k)>
)
,
which satisfies the above incoherence condition {M (k)p }Kk=1 ∈ M(ρ) and let rk be the rank
of M (k)p for k = 1, . . . ,K. In addition, we assume that each S
(k) is constructed as S(k) =
6
P̂
(k−1)
⊗ · · · ⊗ P̂
(k+1)
with (P̂
(k)
)>P̂
(k)
= IH . Then there are universal constants c0
and c1 such that any solution X̂ of the minimization problem (10) with λ = |||Xp − X ∗|||s∗ +
c0σ
(√
n+
√
HK−1 +
√
2 log(K/δ)
)
satisfies the following bound
|||X̂ − X ∗|||F ≤ |||Xp −X
∗|||F + c1λ
√∑K
k=1
rk,
with probability at least 1− δ.
Note that the right-hand side of the bound consists of two terms. The first term is the approximation
error. This term will be zero if X ∗ lies in Span({S(k)}Kk=1). This is the case, if we choose S
(k) =
InK−1 as in the latent nuclear norm, or if the condition of Corollary 1 is satisfied for the smallest βR
when we use the Kronecker product construction we proposed. Note that the regularization constant
λ should also scale with the dual subspace norm of the residual Xp −X ∗.
The second term is the estimation error with respect to Xp. If we take Xp to be the orthogonal pro-
jection of X ∗ to the Span({S(k)}Kk=1), we can ignore the contribution of the residual to λ, because
(Xp − X ∗)(k)S(k) = 0. Then the estimation error scales mildly with the dimensions n, HK−1 and
with the sum of the ranks. Note that if we take S(k) = InK−1 , we have HK−1 = nK−1, and we
recover the guarantee (3) .
4 Experiments
In this section, we conduct tensor denoising experiments on synthetic and real datasets, to numeri-
cally confirm our analysis in previous sections.
4.1 Synthetic data
We randomly generated the true rank two tensorX ∗ of size 20×30×40 with singular values β1 = 20
and β2 = 10. The true factors are generated as random matrices with orthonormal columns. The
observation tensor Y is then generated by adding Gaussian noise with standard deviation σ to X ∗.
Our approach is compared to the CP decomposition, the overlapped approach, and the latent ap-
proach. The CP decomposition is computed by the tensorlab [22] with 20 random initializations.
We assume CP knows the true rank is 2. For the subspace norm, we use Algorithm 2 described
in Section 3. We also select the top 2 singular vectors when constructing Û
(k)
’s. We computed
the solutions for 20 values of regularization parameter λ logarithmically spaced between 1 and 100.
For the overlapped and the latent norm, we use ADMM described in [25]; we also computed 20
solutions with the same λ’s used for the subspace norm.
We measure the performance in the relative error defined as |||X̂ −X ∗|||F /|||X ∗|||F . We report the min-
imum error obtained by choosing the optimal regularization parameter or the optimal initialization.
Although the regularization parameter could be selected by leaving out some entries and measuring
the error on these entries, we will not go into tensor completion here for the sake of simplicity.
Figure 2 (a) and (b) show the result of this experiment. The left panel shows the relative error for 3
representative values of λ for the subspace norm. The black dash-dotted line shows the minimum
error across all the λ’s. The magenta dashed line shows the error corresponding to the theoretically
motivated choice λ = σ(maxk(
√
nk +
√
HK−1) +
√
2 log(K)) for each σ. The two vertical
lines are thresholds of σ from Corollary 1 corresponding to β1 and β2, namely, β1/(
∏
k nk)
1/4 and
β2/(
∏
k nk)
1/4. It confirms that there is a rather sharp increase in the error around the theoretically
predicted places (see also Figure 1(b)). We can also see that the optimal λ should grow linearly with
σ. For large σ (small SNR), the best relative error is 1 since the optimal choice of the regularization
parameter λ leads to predicting with X̂ = 0.
Figure 2 (b) compares the performance of the subspace norm to other approaches. For each method
the smallest error corresponding to the optimal choice of the regularization parameter λ is shown.
7
σ
0 0.5 1 1.5 2
R
e
la
ti
v
e
 E
rr
o
r
0
0.5
1
1.5
2
λ
1
 = 1.62
λ
2
 = 5.46
λ
3
 = 14.38
min error
suggested
(a)
σ
0 0.5 1 1.5 2
R
e
la
ti
v
e
 E
rr
o
r
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
cp
subspace
latent
overlap
optimistic
(b)
σ
0 500 1000 1500 2000
R
e
la
ti
v
e
 E
rr
o
r
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
cp
subspace
latent
overlap
suggested
optimistic
(c)
Figure 2: Tensor denoising. (a) The subspace approach with three representative λ’s on synthetic
data. (b) Comparison of different methods on synthetic data. (c) Comparison on amino acids data.
In addition, to place the numbers in context, we plot the line corresponding to
Relative error =
√
R
∑
k nk log(K)
|||X ∗|||F
· σ, (11)
which we call “optimistic”. This can be motivated from considering the (non-tractable) maximum
likelihood estimator for CP decomposition (see Appendix A).
Clearly, the error of CP, the subspace norm, and “optimistic” grows at the same rate, much slower
than overlap and latent. The error of CP increases beyond 1, as no regularization is imposed (see
Appendix C for more experiments). We can see that both CP and the subspace norm are behaving
near optimally in this setting, although such behavior is guaranteed for the subspace norm whereas
it is hard to give any such guarantee for the CP decomposition based on nonlinear optimization.
4.2 Amino acids data
The amino acid dataset [5] is a semi-realistic dataset commonly used as a benchmark for low rank
tensor modeling. It consists of five laboratory-made samples, each one contains different amounts
of tyrosine, tryptophan and phenylalanine. The spectrum of their excitation wavelength (250-300
nm) and emission (250-450 nm) are measured by fluorescence, which gives a 5 × 201 × 61 tensor.
As the true factors are known to be these three acids, this data perfectly suits the CP model. The true
rank is fed into CP and the proposed approach as H = 3. We computed the solutions of CP for 20
different random initializations, and the solutions of other approaches with 20 different values of λ.
For the subspace and the overlapped approach, λ’s are logarithmically spaced between 103 and 105.
For the latent approach, λ’s are logarithmically spaced between 104 and 106. Again, we include the
optimistic scaling (11) to put the numbers in context.
Figure 2(c) shows the smallest relative error achieved by all methods we compare. Similar to the
synthetic data, both CP and the subspace norm behaves near ideally, though the relative error of
CP can be larger than 1 due to the lack of regularization. Interestingly the theoretically suggested
scaling of the regularization parameter λ is almost optimal.
5 Conclusion
We have settled a conjecture posed by [20] and showed that indeed O(nK/4) signal-to-noise ratio
is sufficient also for odd order tensors. Moreover, our analysis shows an interesting two-phase be-
havior of the error. This finding lead us to the development of the proposed subspace norm. The
proposed norm is defined with respect to a set of orthonormal matrices P̂
(1)
, . . . , P̂
(K)
, which are
estimated by mode-wise singular value decompositions. We have analyzed the denoising perfor-
mance of the proposed norm, and shown that the error can be bounded by the sum of two terms,
which can be interpreted as an approximation error term coming from the first (non-convex) step,
and an estimation error term coming from the second (convex) step.
8
