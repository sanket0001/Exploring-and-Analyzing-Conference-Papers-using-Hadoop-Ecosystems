


Paper ID = 5977
Title = M-Best-Diverse Labelings
for Submodular Energies and Beyond
Alexander Kirillov1 Dmitrij Schlesinger1 Dmitry Vetrov2
Carsten Rother1 Bogdan Savchynskyy1
1 TU Dresden, Dresden, Germany 2 Skoltech, Moscow, Russia
alexander.kirillov@tu-dresden.de
Abstract
We consider the problem of finding M best diverse solutions of energy minimiza-
tion problems for graphical models. Contrary to the sequential method of Batra
et al., which greedily finds one solution after another, we infer all M solutions
jointly. It was shown recently that such jointly inferred labelings not only have
smaller total energy but also qualitatively outperform the sequentially obtained
ones. The only obstacle for using this new technique is the complexity of the cor-
responding inference problem, since it is considerably slower algorithm than the
method of Batra et al. In this work we show that the joint inference of M best
diverse solutions can be formulated as a submodular energy minimization if the
original MAP-inference problem is submodular, hence fast inference techniques
can be used. In addition to the theoretical results we provide practical algorithms
that outperform the current state-of-the-art and can be used in both submodular
and non-submodular case.
1 Introduction
A variety of tasks in machine learning can be formulated in the form of an energy minimization
problem, known also as maximum a posteriori (MAP) or maximum likelihood estimation (MLE)
inference in an undirected graphical models (related to Markov or conditional random fields). Its
modeling power and importance are well-recognized, which resulted into specialized benchmark,
i.e. [18] and computational challenges [8] for its solvers. This underlines the importance of finding
the most probable solution. Following [3] and [25] we argue, however, that finding M > 1 diverse
configurations with low energies is also of importance in a number of scenarios, such as: (a) Ex-
pressing uncertainty of the found solution [27]; (b) Faster training of model parameters [14]; (c)
Ranking of inference results [32]; (d) Empirical risk minimization [26].
We build on the new formulation for finding M -best-diverse-configurations, which was recently
proposed in [19]. In this formulation all M configurations are inferred jointly, contrary to the es-
tablished method [3], where a sequential greedy procedure is used. As shown in [19], the new
formulation does not only reliably produce configurations with lower total energy, but also leads
to better results in several application scenarios. In particular, for the image segmentation scenario
the results of [19] significantly outperform those of [3]. This is true even when [19] uses a plain
Hamming distance as a diversity measure and [3] uses more powerful diversity measures.
Our contributions.
• We show that finding M -best-diverse configurations of a binary submodular energy min-
imization can be formulated as a submodular MAP-inference problem, and hence can be solved
This project has received funding from the European Research Council (ERC) under the European Unions
Horizon 2020 research and innovation programme (grant agreement No 647769). D. Vetrov was supported by
RFBR proj. (No. 15-31-20596) and by Microsoft (RPD 1053945).
1
efficiently for any node-wise diversity measure.
• We show that for certain diversity measures, such as e.g. Hamming distance, the M -best-
diverse configurations of a multilabel submodular energy minimization can be formulated as a
submodular MAP-inference problem, which also implies applicability of efficient graph cut-based
solvers.
•We give the insight that if the MAP-inference problem is submodular then theM -best-diverse
configurations can be always fully ordered with respect to the natural partial order, induced in the
space of all configurations.
•We show experimentally that if the MAP-inference problem is submodular, we are quantita-
tively at least as good as [19] and considerably better than [3]. The main advantage of our method
is a major speed up over [19], up to the order of two magnitudes. Our method has the same order of
magnitude run-time as [3]. In the non-submodular case our results are slightly inferior to [19], but
the advantage with respect to gain in speed up still holds.
Related work. The importance of the considered problem may be justified by the fact that a proce-
dure of computing M -best solutions to discrete optimization problems was proposed in [23], which
dates back to 1972. Later, more efficient specialized procedures were introduced for MAP-inference
on a tree [29, Ch. 8], junction-trees [24] and general graphical models [33, 12, 2]. Such methods
are however not suited for scenarios where diversity of the solutions is required (like in machine
translation, search engines, producing M -best hypothesis in cascaded algorithms), since they do not
enforce it explicitly.
Structural Determinant Point Processes [22] is a tool to model probabilistic distributions over struc-
tured models. Unfortunately an efficient sampling procedure is feasible for tree-structured graphical
models only. The recently proposed algorithm [7] to find M best modes of a distribution is limited
to the same narrow class of problems.
Training of M independent graphical models to produce diverse solutions was proposed in [13, 15].
In contrast, we assume a single fixed model supporting reasonable MAP-solutions.
Along with [3], the most related to our work is the recent paper [25], which proposes a subclass of
new diversity penalties, for which the greedy nature of the algorithm [3] can be substantiated due
to submodularity of the used diversity measures. In contrast to [25] we do not limit ourselves to
diversity measures fulfilling such properties and moreover, we define a class of problems, for which
our joint inference approach leads to polynomially and efficiently solvable problems in practice.
We build on top of the work [19], which is explained in detail in Section 2.
Organization of the paper. Section 2 provides background necessary for formulation of our results:
energy minimization for graphical models and existing approaches to obtain diverse solutions. In
Section 3 we introduce submodularity for graphical models and formulate the main results of our
work. Finally, Section 4 and 5 are devoted to the experimental evaluation of our technique and
conclusions. Supplementary material contains proofs of all mathematical claims and the concurrent
submission [19].
2 Preliminaries
Energy minimization. Let 2A denote the powerset of a set A. The pair G = (V,F) is called
a hyper-graph and has V as a finite set of variable nodes and F ⊆ 2V as a set of factors. Each
variable node v ∈ V is associated with a variable yv taking its values in a finite set of labels
Lv . The set LA =
∏
v∈A Lv denotes a Cartesian product of sets of labels corresponding to
the subset A ⊆ V of variables. Functions θf : Lf → R, associated with factors f ∈ F , are
called potentials and define local costs on values of variables and their combinations. Potentials
θf with |f | = 1 are called unary, with |f | = 2 pairwise and |f | > 2 higher order. The set
{θf : f ∈ F} of all potentials is referred by θ. For any factor f ∈ F the corresponding set of
variables {yv : v ∈ f} will be denoted by yf . The energy minimization problem consists of find-
ing a labeling y∗ = {yv : v ∈ V} ∈ LV , which minimizes the total sum of corresponding potentials:
y∗ = arg min
y∈LV
E(y) = arg min
y∈LV
∑
f∈F
θf (yf ) . (1)
Problem (1) is also known as MAP-inference. Labeling y∗ satisfying (1) will be later called a solu-
tion of the energy-minimization or MAP-inference problem, shortly MAP-labeling or MAP-solution.
2
y11
θ1
y12
θ2
y13
θ3
y14
θ4
y21
θ1
y22
θ2
y23
θ3
y24
θ4
y31
θ1
y32
θ2
y33
θ3
y34
θ4
−∆M (y1,y2,y3)
E(y1)
E(y2)
E(y3)
θ1,2 θ2,3 θ3,4
θ1,2 θ2,3 θ3,4
θ1,2 θ2,3 θ3,4
(a) General diversity measure
y31
θ1
y32
θ2
y33
θ3
y34
θ4
θ1,2 θ2,3 θ3,4
y21
θ1
y22
θ2
y23
θ3
y24
θ4
θ1,2 θ2,3 θ3,4
y11
θ1
y12
θ2
y13
θ3
y14
θ4
θ1,2 θ2,3 θ3,4
−∆M1 (y
1
1 , y
2
1 , y
3
1)
−∆M2 (y
1
2 , y
2
2 , y
3
2)
−∆M3 (y
1
3 , y
2
3 , y
3
3)
−∆M4 (y
1
4 , y
2
4 , y
3
4)
(b) Node-wise diversity measure
−Jy11 6= y31K −Jy12 6= y32K −Jy13 6= y33K −Jy14 6= y34K
y31
θ1
y32
θ2
y33
θ3
y34
θ4
θ1,2 θ2,3 θ3,4
−Jy21 6= y31K −Jy22 6= y32K −Jy23 6= y33K −Jy24 6= y34K
y21
θ1
y22
θ2
y23
θ3
y24
θ4
θ1,2 θ2,3 θ3,4
−Jy11 6= y21K −Jy12 6= y22K −Jy13 6= y23K −Jy14 6= y24K
y11
θ1
y12
θ2
y13
θ3
y14
θ4
θ1,2 θ2,3 θ3,4
(c) Hamming distance diversity
Figure 1: Examples of factor graphs for 3 diverse solutions of the original MRF (1) with different
diversity measures. The circles represent nodes of the original model that are copied 3 times. For
clarity the diversity factors of order higher than 2 are shown as squares. Pairwise factors are depicted
by edges connecting the nodes. We omit λ for readability. (a) The most general diversity measure
(4), (b) the node-wise diversity measure (6), (c) Hamming distance as a diversity measure (5).
Finally, a model is defined by the triple (G,LV ,θ), i.e. the underlying hyper-graph, the sets of labels
and the potentials.
In the following, we use brackets to distinguish between upper index and power, i.e. (A)n means
the n-th power of A, whereas n is an upper index in the expression An. We will keep, however, the
standard notation Rn for the n-dimensional vector space.
Sequential Computation of M Best Diverse Solutions [3]. Instead of looking for a single labeling
with lowest energy, one might ask for a set of labelings with low energies, yet being significantly
different from each other. In order to find such M diverse labelings y1, . . . ,yM , the method
proposed in [3] solves a sequence of problems of the form
ym = argmin
y
[
E(y)− λ
m−1∑
i=1
∆(y,yi)
]
(2)
for m = 1, 2 . . . ,M , where λ > 0 determines a trade-off between diversity and energy, y1 is the
MAP-solution and the function ∆ : LV × LV → R defines the diversity of two labelings. In other
words, ∆(y,y′) takes a large value if y and y′ are diverse, in a certain sense, and a small value
otherwise. This problem can be seen as an energy minimization problem, where additionally to the
initial potentials θ the potentials−λ∆(·,yi), associated with an additional factor V , are used. In the
simplest and most commonly used form, ∆(y,y′) is represented by a sum of node-wise diversity
measures ∆v : Lv × Lv → R,
∆(y,y′) =
∑
v∈V
∆v(yv, y
′
v) , (3)
and the potentials are split to a sum of unary potentials, i.e. those associated with additional factors
{v}, v ∈ V . This implies that in case efficient graph-cut based inference methods (including α-
expansion [6], α-β-swap [6] or their generalizations [1, 10]) are applicable to the initial problem (1)
then they remain applicable to the augmented problem (2), which assures efficiency of the method.
Joint computation of M-best-diverse labelings. The notation fM ({y}) will be used as a shortcut
for fM (y1, . . . ,yM ), for any function fM : (LV)M → R.
Instead of the greedy sequential procedure (2), in [19] it was suggested to infer all M labelings
jointly, by minimizing
EM ({y}) =
M∑
i=1
E(yi)− λ∆M ({y}) (4)
for y1, . . . ,yM and some λ > 0. Function ∆M defines the total diversity of any M labelings.
It was shown in [19] that the M labelings obtained according to (4) have both lower total energy∑M
i=1E(y
i) and are better from the applied point of view, than those obtained by the sequential
method (2). Hence we will build on the formulation (4) in this work.
3
Though the expression (4) looks complicated, it can be nicely represented in the form (1) and hence
constitutes an energy minimization problem. To achieve this, one creates M copies (Gi,LiV ,θi) =
(G,LV ,θ) of the initial model (G,LV ,θ). The hyper-graph GM1 = (VM1 ,FM1 ) for the new task
is defined as follows. The set of nodes in the new graph is the union of the node sets from the
considered copies VM1 =
⋃M
i=1 Vi. Factors are FM1 =
⋃M
i=1 F i ∪ {VM1 }, i.e. again the union of
the initial ones extended by a special factor corresponding to the diversity penalty that depends on
all nodes of the new graph. Each node v ∈ Vi is associated with the label set Liv = Lv . The
corresponding potentials θM1 are defined as {−λ∆M ,θ1, . . . ,θM}, see Fig. 1a for illustration. The
model (GM1 ,LVM1 ,θ
M
1 ) corresponds to the energy (4). An optimal M -tuple of these labelings,
corresponding to a minimum of (4), is a trade-off between low energy of individual labelings yi and
their total diversity.
Complexity of the Diversity Problem (4). Though the formulation (4) leads to better results than
those of (2), minimization of EM is computationally demanding even if the original energy E can
be easily (approximatively) optimized. This is due to the intrinsic repulsive structure of the diversity
potentials −λ∆M : according to the intuitive meaning of the diversity, similar labels are penalized
more than different one. Consider the simplest case with the Hamming distance applied node-wise
as a diversity measure
∆M ({y}) =
M−1∑
i=1
M∑
j=i+1
∑
v∈V
∆v(y
i
v, y
j
v), where ∆v(y, y
′) = Jy 6= y′K . (5)
Here expression JAK equals 1 if A is true and 0 otherwise. The corresponding factor graph is
sketched in Fig. 1c. Such potentials can not be optimized with efficient graph-cut based methods
and moreover, as shown in [19], the bounds delivered by LP-relaxation [31] based solvers are very
loose in practice. Indeed, solutions delivered by such solvers are significantly inferior even to the
results of the sequential method (2).
To cope with this issue a clique encoding representation of (4) was proposed in [19]. In this rep-
resentation M -tuples of labels y1v , . . . , y
M
v (in the M nodes corresponding to the single initial node
v) were considered as the new labels. In this way the difficult diversity factors were incorporated
into the unary factors of the new representation and the pairwise factors were adjusted respectively.
This allowed to (approximately) solve the problem (4) with graph-cuts based techniques if those
techniques were applicable to the energy E of a single labeling. The disadvantage of the clique
encoding representation is the exponential growth of the label space, which was reflected in a sig-
nificantly higher inference time for the problem (4) compared to the procedure (2). In what follows,
we show an alternative transformation of the problem (4), which (i) does not have this drawback (its
size is basically the same as those of (4)) and (ii) allows to exactly solve (4) in the case the energy
E is submodular.
Node-wise Diversity. In what follows we will mainly consider the node-wise diversity measures,
i.e. those, which can be represented in the form
∆M ({y}) =
∑
v∈V
∆Mv ({y}v) (6)
for some node diversity measures ∆Mv : (Lv)
M → R, see Fig. 1b for illustration.
3 M-Best-Diverse Labelings for Submodular Problems
Submodularity. In what follows we will assume that the sets Lv , v ∈ V , of labels are completely
ordered. This implies that for any s, t ∈ Lv their maximum and minimum, denoted as s∨ t and s∧ t
respectively, are well-defined. Similarly let y1 ∨ y2 and y1 ∧ y2 denote the node-wise maximum
and minimum of any two labelings y1,y2 ∈ LA, A ⊆ V . Potential θf is called submodular, if for
any two labelings y1,y2 ∈ Lf it holds1:
θf (y1) + θf (y2) ≥ θf (y1 ∨ y2) + θf (y1 ∧ y2) . (7)
Potential θ will be called supermodular, if (−θ) is submodular.
1Pairwise binary potentials satisfying θf (0, 1) + θf (1, 0) ≥ θf (0, 0) + θf (1, 1) build an important special
case of this definition.
4
Energy E is called submodular if for any two labelings y1,y2 ∈ LV it holds:
E(y1) + E(y2) ≥ E(y1 ∨ y2) + E(y1 ∧ y2) . (8)
Submodularity of energy trivially follows from the submodularity of all its non-unary potentials θf ,
f ∈ F , |f | > 1. In the pairwise case the inverse also holds: submodularity of energy implies also
submodularity of all its (pairwise) potentials (e.g. [31, Thm. 12]). There are efficient methods for
solving energy minimization problems with submodular potentials, based on its transformation into
min-cut/max-flow problem [21, 28, 16] in case all potentials are either unary or pairwise or to a
submodular max-flow problem in the higher-order case [20, 10, 1].
Ordered M Solutions. In what follows we will write z ≤ z for any two vectors z1 and z
meaning that the inequality holds coordinate-wise.
For an arbitrary set A we will call a function f : (A)n → R of n variables permutation in-
variant if for any (x1, x2, . . . , xn) ∈ (A)n and any permutation π it holds f(x1, x2, . . . , xn) =
f(xπ(1), xπ(2), . . . , xπ(n)). In what follows we will consider mainly permutation invariant diversity
measures.
Let us consider two arbitrary labelings y1,y2 ∈ LV and their node-wise minimum y1 ∧ y2 and
maximum y1 ∨ y2. Since (y1v ∧ y2v , y1v ∨ y2v) is either equal to (y1v , y2v) or to (y2v , y1v), for any
permutation invariant node diversity measure it holds ∆2v(y
1
v , y
2
v) = ∆
2
v(y
1
v ∧ y2v , y1v ∨ y2v). This in
its turn implies ∆2(y1 ∧ y2,y1 ∨ y2) = ∆2(y1,y2) for any node-wise diversity measure of the
form (6). If E is submodular, then from (8) it additionally follows that
E2(y1 ∧ y2,y1 ∨ y2) ≤ E2(y1,y2) , (9)
where E2 is defined as in (4). Note, that (y1 ∧ y2) ≤ (y1 ∨ y2). Generalizing these considerations
to M labelings one obtains
Theorem 1. Let E be submodular and ∆M be a node-wise diversity measure with each component
∆Mv being permutation invariant. Then there exists an ordered M -tuple (y
1, . . . ,yM ), yi ≤ yj for
1 ≤ i < j ≤M , such that for any (z1, . . . ,zM ) ∈ (LV)M it holds
EM ({y}) ≤ EM ({z}) , (10)
where EM is defined as in (4).
Theorem 1 in particular claims that in the binary case Lv = {0, 1}, v ∈ V , the optimal M labelings
define nested subsets of nodes, corresponding to the label 1.
Submodular formulation of M-Best-Diverse problem. Due to Theorem 1, for submodular ener-
gies and node-wise diversity measures it is sufficient to consider only orderedM -tuples of labelings.
This order can be enforced by modifying the diversity measure accordingly:
∆̂Mv (y
1, . . . , yM ) :=
{
∆Mv (y
1, . . . , yM ), y1 ≤ y2 ≤ · · · ≤ yM
−∞, otherwise , (11)
and using it instead of the initial measure ∆Mv . Note that ∆̂
M
v is not permutation invariant. In
practice one can use sufficiently big numbers in place of∞ in (11). This implies
Lemma 1. Let E be submodular and ∆M be a node-wise diversity measure with each component
∆Mv being permutation invariant. Then any solution of the ordering enforcingM -best-diverse prob-
lem
ÊM ({y}) =
M∑
i=1
E(yi)− λ
∑
v∈V
∆̂Mv (y
1
v , . . . , y
M
v ) (12)
is a solution of the corresponding M -best-diverse problem (4)
EM ({y}) =
M∑
i=1
E(yi)− λ
∑
v∈V
∆Mv (y
1
v , . . . , y
M
v ) , (13)
where ∆̂Mv and ∆
M
v are related by (11).
We will say that a vector (y1, . . . , yM ) ∈ (Lv)M is ordered, if it holds y1 ≤ y2 ≤ · · · ≤ yM .
5
Given submodularity of E the submodularity (an hence – solvability) of EM in (13) would triv-
ially follow from the supermodularity of ∆M . However there hardly exist supermodular diver-
sity measures. The ordering provided by Theorem 1 and the corresponding form of the ordering-
enforcing diversity measure ∆̂M significantly weaken this condition, which is precisely stated by
the following lemma. In the lemma we substitute∞ of (11) with a sufficiently big values such as
C∞ ≥ max{y}EM ({y}) for the sake of numerical implementation. Moreover, this values will
differ from each other to keep ∆̂Mv supermodular.
Lemma 2. Let for any two ordered vectors y = (y1, . . . , yM ) ∈ (Lv)M and z = (z1, . . . , zM ) ∈
(Lv)
M it holds
∆v(y ∨ z) +∆v(y ∧ z) ≥ ∆v(y) +∆v(z), (14)
where y ∨ z and y ∧ z are element-wise maximum and minimum respectively. Then ∆̂v , defined as
∆̂v(y
1, . . . , yM ) = ∆v(y
1, . . . , yM )− C∞ ·
M−1∑
i=1
M∑
j=i+1
3max(0,y
i−yj) − 1
 (15)
is supermodular.
Note, eq. (11) and (15) are the same up to the infinity values in (11). Though condition (14) re-
sembles the supermodularity condition, it has to be fulfilled for ordered vectors only. The following
corollaries of Lemma 2 give two most important examples of the diversity measures fulfilling (14).
Corollary 1. Let |Lv| = 2 for all v ∈ V . Then the statement of Lemma 2 holds for arbitrary
∆v : (Lv)
M → R.
Corollary 2. Let ∆Mv (y1, . . . , yM ) =
∑M−1
i=1
∑M
j=i+1∆ij(y
i, yj). Then the condition of Lemma 2
is equivalent to
∆ij(y
i, yj) +∆ij(y
i + 1, yj + 1) ≥ ∆ij(yi + 1, yj) +∆ij(yi, yj + 1) for yi < yj (16)
and 1 ≤ i < j ≤M .
In particular, condition (16) is satisfied for the Hamming distance ∆ij(y, y′) = Jy 6= y′K.
The following theorem trivially summarizes Lemmas 1 and 2:
Theorem 2. Let energy E and diversity measure ∆M satisfy conditions of Lemmas 1 and 2. Then
the ordering enforcing problem (12) delivers solution to the M -best-diverse problem (13) and is
submodular. Moreover, submodularity of all non-unary potentials of the energy E implies submod-
ularity of all non-unary potentials of the ordering enforcing energy ÊM .
4 Experimental evaluation
We have tested our algorithms in two application scenarios: (a) interactive foreground/background
image segmentation, where annotation is available in the form of scribbles [3] and (b) Category level
segmentation on PASCAL VOC 2012 data [9].
As baselines we use: (i) the sequential method DivMBest (2) proposed in [3, 25] and (ii) the
clique-encoding CE method [19] for an (approximate) joint computation of M -best-diverse label-
ings. As mentioned in Section 2, this method addresses the energy EM defined in (4), however it
has the disadvantage that its label space grows exponentially with M .
Our method that solves the problem (12) with the Hamming diversity measure (5) by trans-
forming it into min-cut/max-flow problem [21, 28, 16] and running the solver [5] is denoted as
Joint-DivMBest.
Diversity measures used in experiments are: the Hamming distance (5) HD, Label Cost LC, Label
Transitions LT and Hamming Ball HB. The last three measures are higher order diversity potentials
introduced in [25] and used only in connection with the DivMBest algorithm. If not stated other-
wise, the Hamming distance (5) is used as a diversity measure. Both the clique encoding (CE) based
approaches and the submodularity-based methods proposed in this work use only the Hamming
distance as a diversity measure.
As [25] suggests, certain combinations of different diversity measures may lead to better results. To
denote such combinations, the signs ⊗ and ⊕ were used in [25]. We refer to [25] for a detailed
description of this notation and treat such combined methods as a black box for our comparison.
6
M=2 M=6 M=10
quality time quality time quality time
DivMBest 93.16 0.45 95.02 2.4 95.16 4.4
CE 95.13 2.9 96.01 47.6 96.19 1247
Joint-DivMBest 95.13 0.77 96.01 5.2 96.19 20.4
Table 1: Interactive segmentation: per-pixel accuracies (quality) for the best segmentation out of M
ones and run-time. Compare to the average quality 91.57 of a single labeling. Hamming distance
is used as a diversity measure. The run-time is in milliseconds (ms). Joint-DivMBest quantita-
tively outperforms DivMBest, and is equal to CE, however, it is considerably faster than CE.
4.1 Interactive segmentation
Instead of returning a single segmentation corresponding to a MAP-solution, diversity methods pro-
vide to the user a small number of possible low-energy results based on the scribbles. Following [3]
we model only the first iteration of such an interactive procedure, i.e. we consider user scribbles to
be given and compare the sets of segmentations returned by the compared diversity methods.
Authors of [3] kindly provided us their 50 graphical model instances, corresponding to the MAP-
inference problem (1). They are based on a subset of the PASCAL VOC 2010 [9] segmentation chal-
lenge with manually added scribbles. Pairwise potentials constitute contrast sensitive Potts terms [4],
which are submodular. This implies that (i) the MAP-inference is solvable by min-cut/max-flow al-
gorithms [21] and (ii) Theorem 2 is applicable and the M -best-diverse solutions can be found by
reducing the ordering preserving problem (12) to min-cut/max-flow and applying the corresponding
algorithm.
Quantitative comparison and run-time of the considered methods is provided in Table 1, where
each method was used with the parameter λ (see (2), (4)), optimally tuned via cross-validation.
Following [3], as a quality measure we used the per pixel accuracy of the best solution for each
sample averaged over all test images. Methods CE and Joint-DivMBest gave the same quality,
which confirms the observation made in [19], that CE returns an exact MAP solution for each sample
in this dataset. Combined methods with more sophisticated diversity measures return results that are
either inferior to DivMBest or only negligibly improved once, hence we omitted them. The run-
time provided is also averaged over all samples. The max-flow algorithm was used for DivMBest
and Joint-DivMBest and α-expansion for CE.
Summary. It can be seen that the Joint-DivMBest qualitatively outperforms DivMBest and
is equal to CE. However, it is considerably faster than the latter (the difference grows exponentially
with M ) and the runtime is of the same order of magnitude as the one of DivMBest.
4.2 Category level segmentation
The category level segmentation from PASCAL VOC 2012 challenge [9] contains 1449 validation
images with known ground truth, which we used for evaluation of diversity methods. Correspond-
ing pairwise models with contrast sensitive Potts terms of the form θuv(y, y′) = wuvJy 6= y′K,
uv ∈ F , were used in [25] and kindly provided to us by the authors. Contrary to interactive segmen-
tation, the label sets contain 21 elements and hence the respective MAP-inference problem (1) is not
submodular anymore. However it still can be approximatively solved by α-expansion or α-β-swap.
Since the MAP-inference problem (1) is not submodular in this experiment, Theorem 2 is not ap-
plicable. We used two ways to overcome it. First, we modified the diversity potentials according
to (15), as if Theorem 2 were to be correct. This basically means we were explicitly looking for
ordered M best diverse labelings. The resulting inference problem was addressed with α-β-swap
(since neither max-flow nor the α-expansion algorithms are applicable). We refer to this method as
to Joint-DivMBest-ordered. The second way to overcome the non-submodularity problem,
is based on learning. Using structured SVM technique we trained pairwise potentials with addi-
tional constraints enforcing their submodularity, as it is done in e.g. [11]. We kept the contrast terms
wuv and learned only a single submodular function θ̂(y, y′), which we used in place of Jy 6= y′K.
After the learning, all our potentials had the form θuv(y, y′) = wuv θ̂(y, y′), uv ∈ F . We refer to
7
MAP inference M=5 M=15 M=16
quality time quality time quality time
DivMBest α-exp[4] 51.21 0.01 52.90 0.03 53.07 0.03
HB∗ HB-HOP-MAP[30] 51.71 - 55.32 - - -
DivMBest∗⊕HB∗ HB-HOP-MAP[30] - - 55.89 - - -
HB∗⊗LC∗⊗LT∗ LT – coop. cuts[17] - - 56.97 - - -
DivMBest∗⊗HB∗⊗LC∗⊗LT∗ LT – coop. cuts[17] - - - - 57.39 -
CE α-exp[4] 54.22 733 - - - -
CE3 α-exp[4] 54.14 2.28 57.76 5.87 58.36 7.24
Joint-DivMBest-ordered α-β-swap[4] 53.81 0.01 56.08 0.08 56.31 0.08
Joint-DivMBest-learned max-flow[5] 53.85 0.38 56.14 35.47 56.33 38.67
Joint-DivMBest-learned α-exp[4] 53.84 0.01 56.08 0.08 56.31 0.08
Table 2: PASCAL VOC 2012. Intersection over union quality measure/running time. The best
segmentation out of M is considered. Compare to the average quality 43.51 of a single labeling.
Time is in seconds (s). Notation ’-’ correspond to absence of result due to computational reasons or
inapplicability of the method. (∗)- methods were not run by us and the results were taken from [25]
directly. The MAP-inference column references the slowest inference technique out of those used
by the method.
this method as to Joint-DivMBest-learned. For the model we use max-flow[5] as an exact
inference method and α-expansion[4] as a fast approximate inference method.
Quantitative comparison and run-time of the considered methods is provided in Table 2,
where each method was used with the parameter λ (see (2), (4)) optimally tuned via cross-
validation on the validation set in PASCAL VOC 2012. Following [3], we used the Intersec-
tion over union quality measure, averaged over all images. Among combined methods with
higher order diversity measures we selected only those providing the best results. The method
CE3 [19] is a hybrid of DivMBest and CE delivering a reasonable trade-off between run-
ning time and accuracy of inference for the model EM (4). Quantitative results delivered by
Joint-DivMBest-ordered and Joint-DivMBest-learned are very similar (though the
latter is negligibly better), significantly outperform those of DivMBest and only slightly infe-
rior to those of CE3. However the run-time for Joint-DivMBest-ordered and α-expansion
version of Joint-DivMBest-learned are comparable to those of DivMBest and outper-
form all other competitors due to use of the fast inference algorithms and linearly growing label
space, contrary to the label space of CE3, which grows as (Lv)3. Though we do not know ex-
act run-time for the combined methods (where ⊕ and ⊗ are used) we expect them to be signifi-
cantly higher then those for DivMBest and Joint-DivMBest-ordered because of the intrin-
sically slow MAP-inference techniques used. However contrary to the latter one the inference in
Joint-DivMBest-learned can be exact due to submodularity of the underlying energy.
5 Conclusions
We have shown that submodularity of the MAP-inference problem implies a fully ordered set of
M best diverse solutions given a node-wise permutation invariant diversity measure. Enforcing
such ordering leads to a submodular formulation of the joint M -best-diverse problem and implies
its efficient solvability. Moreover, we have shown that even in non-submodular cases, when the
MAP-inference is (approximately) solvable with efficient graph-cut based methods, enforcing this
ordering leads to the M -best-diverse problem, which is (approximately) solvable with graph-cut
based methods as well. In our test cases (and there are likely others), such an approximative tech-
nique lead to notably better results then those provided by the established sequential DivMBest
technique [3], whereas its run-time remains quite comparable to the run-time of DivMBest and is
much smaller than the run-time of other competitors.
8
References
[1] C. Arora, S. Banerjee, P. Kalra, and S. Maheshwari. Generalized flows for optimal inference in higher
order MRF-MAP. TPAMI, 2015.
[2] D. Batra. An efficient message-passing algorithm for the M-best MAP problem. arXiv:1210.4841, 2012.
[3] D. Batra, P. Yadollahpour, A. Guzman-Rivera, and G. Shakhnarovich. Diverse M-best solutions in markov
random fields. In ECCV. Springer Berlin/Heidelberg, 2012.
[4] Y. Boykov and M.-P. Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects
in N-D images. In ICCV, 2001.
[5] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-flow algorithms for energy
minimization in vision. TPAMI, 26(9):1124–1137, 2004.
[6] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. TPAMI,
23(11):1222–1239, 2001.
[7] C. Chen, V. Kolmogorov, Y. Zhu, D. N. Metaxas, and C. H. Lampert. Computing the M most probable
modes of a graphical model. In AISTATS, 2013.
[8] G. Elidan and A. Globerson. The probabilistic inference challenge (PIC2011).
[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object
Classes Challenge 2012 (VOC2012) Results.
[10] A. Fix, A. Gruber, E. Boros, and R. Zabih. A graph cut algorithm for higher-order Markov random fields.
In ICCV, 2011.
[11] V. Franc and B. Savchynskyy. Discriminative learning of max-sum classifiers. JMLR, 9:67–104, 2008.
[12] M. Fromer and A. Globerson. An lp view of the m-best map problem. In NIPS 22, 2009.
[13] A. Guzman-Rivera, D. Batra, and P. Kohli. Multiple choice learning: Learning to produce multiple
structured outputs. In NIPS 25, 2012.
[14] A. Guzman-Rivera, P. Kohli, and D. Batra. DivMCuts: Faster training of structural SVMs with diverse
M-best cutting-planes. In AISTATS, 2013.
[15] A. Guzman-Rivera, P. Kohli, D. Batra, and R. A. Rutenbar. Efficiently enforcing diversity in multi-output
structured prediction. In AISTATS, 2014.
[16] H. Ishikawa. Exact optimization for Markov random fields with convex priors. TPAMI, 2003.
[17] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In
CVPR, 2011.
[18] J. H. Kappes, B. Andres, F. A. Hamprecht, C. Schnörr, S. Nowozin, D. Batra, S. Kim, B. X. Kausler,
T. Kröger, J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A comparative study of modern
inference techniques for structured discrete energy minimization problems. IJCV, pages 1–30, 2015.
[19] A. Kirillov, B. Savchynskyy, D. Schlesinger, D. Vetrov, and C. Rother. Inferring M-best diverse labelings
in a single one. In ICCV, 2015.
[20] V. Kolmogorov. Minimizing a sum of submodular functions. Discrete Applied Mathematics, 2012.
[21] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? TPAMI, 2004.
[22] A. Kulesza and B. Taskar. Structured determinantal point processes. In NIPS 23, 2010.
[23] E. L. Lawler. A procedure for computing the K best solutions to discrete optimization problems and its
application to the shortest path problem. Management Science, 18(7), 1972.
[24] D. Nilsson. An efficient algorithm for finding the m most probable configurationsin probabilistic expert
systems. Statistics and Computing, 8(2):159–173, 1998.
[25] A. Prasad, S. Jegelka, and D. Batra. Submodular meets structured: Finding diverse subsets in
exponentially-large structured item sets. In NIPS 27, 2014.
[26] V. Premachandran, D. Tarlow, and D. Batra. Empirical minimum bayes risk prediction: How to extract
an extra few % performance from vision models with just three more parameters. In CVPR, 2014.
[27] V. Ramakrishna and D. Batra. Mode-marginals: Expressing uncertainty via diverse M-best solutions. In
NIPS Workshop on Perturbations, Optimization, and Statistics, 2012.
[28] D. Schlesinger and B. Flach. Transforming an arbitrary minsum problem into a binary one. TU Dresden,
Fak. Informatik, 2006.
[29] M. I. Schlesinger and V. Hlavac. Ten lectures on statistical and structural pattern recognition, volume 24.
Springer Science & Business Media, 2002.
[30] D. Tarlow, I. E. Givoni, and R. S. Zemel. Hop-map: Efficient message passing with high order potentials.
In AISTATS, 2010.
[31] T. Werner. A linear programming approach to max-sum problem: A review. TPAMI, 29(7), 2007.
[32] P. Yadollahpour, D. Batra, and G. Shakhnarovich. Discriminative re-ranking of diverse segmentations. In
CVPR, 2013.
[33] C. Yanover and Y. Weiss. Finding the M most probable configurations using loopy belief propagation. In
NIPS 17, 2004.
9
