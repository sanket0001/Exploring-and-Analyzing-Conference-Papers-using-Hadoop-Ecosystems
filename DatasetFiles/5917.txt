


Paper ID = 5917
Title = Sparse Linear Programming via
Primal and Dual Augmented Coordinate Descent
Ian E.H. Yen ∗ Kai Zhong ∗ Cho-Jui Hsieh † Pradeep Ravikumar ∗ Inderjit S. Dhillon ∗
∗ University of Texas at Austin † University of California at Davis
∗ {ianyen,pradeepr,inderjit}@cs.utexas.edu zhongkai@ices.utexas.edu
† chohsieh@ucdavis.edu
Abstract
Over the past decades, Linear Programming (LP) has been widely used in different
areas and considered as one of the mature technologies in numerical optimization.
However, the complexity offered by state-of-the-art algorithms (i.e. interior-point
method and primal, dual simplex methods) is still unsatisfactory for problems in
machine learning with huge number of variables and constraints. In this paper,
we investigate a general LP algorithm based on the combination of Augmented
Lagrangian and Coordinate Descent (AL-CD), giving an iteration complexity of
O((log(1/))2) with O(nnz(A)) cost per iteration, where nnz(A) is the number
of non-zeros in the m×n constraint matrix A, and in practice, one can further re-
duce cost per iteration to the order of non-zeros in columns (rows) corresponding
to the active primal (dual) variables through an active-set strategy. The algorithm
thus yields a tractable alternative to standard LP methods for large-scale problems
of sparse solutions and nnz(A)  mn. We conduct experiments on large-scale
LP instances from `1-regularized multi-class SVM, Sparse Inverse Covariance Es-
timation, and Nonnegative Matrix Factorization, where the proposed approach
finds solutions of 10−3 precision orders of magnitude faster than state-of-the-art
implementations of interior-point and simplex methods. 1
1 Introduction
Linear Programming (LP) has been studied since the early 19th century and has become one of
the representative tools of numerical optimization with wide applications in machine learning such
as `1-regularized SVM [1], MAP inference [2], nonnegative matrix factorization [3], exemplar-
based clustering [4, 5], sparse inverse covariance estimation [6], and Markov Decision Process [7].
However, as the demand for scalability keeps increasing, the scalability of existing LP solvers has
become unsatisfactory. In particular, most algorithms in machine learning targeting large-scale data
have a complexity linear to the data size [8, 9, 10], while the complexity of state-of-the-art LP
solvers (i.e. Interior-Point method and Primal, Dual Simplex methods) is still at least quadratic in
the number of variables or constraints [11].
The quadratic complexity comes from the need to solve each linear system exactly in both simplex
and interior point method. In particular, the simplex method, when traversing from one corner point
to another, requires solution to a linear system that has dimension linear to the number of variables
or constraints, while in an Interior-Point method, finding the Newton direction requires solving a
linear system of similar size. While there are sparse variants of LU and Cholesky decomposition that
can utilize the sparsity pattern of matrix in a linear system, the worst-case complexity for solving
such system is at least quadratic to the dimension except for very special cases such as a tri-diagonal
or band-structured matrix.
1Our solver has been released here: http://www.cs.utexas.edu/˜ianyen/LPsparse/
1
For interior point method (IPM), one remedy to the high complexity is employing an iterative method
such as Conjugate Gradient (CG) to solve each linear system inexactly. However, this can hardly
tackle the ill-conditioned linear systems produced by IPM when iterates approach boundary of con-
straints [12]. Though substantial research has been devoted to the development of preconditioners
that can help iterative methods to mitigate the effect of ill-conditioning [12, 13], creating a precon-
ditioner of tractable size is a challenging problem by itself [13]. Most commercial LP software thus
still relies on exact methods to solve the linear system.
On the other hand, some dual or primal (stochastic) sub-gradient descent methods have cheap cost
for each iteration, but require O(1/2) iterations to find a solution of  precision, which in practice
can even hardly find a feasible solution satisfying all constraints [14].
Augmented Lagrangian Method (ALM) was invented early in 1969, and since then there have been
several works developed Linear Program solver based on ALM [15, 16, 17]. However, the challenge
of ALM is that it produces a series of bound-constrained quadratic problems that, in the traditional
sense, are harder to solve than linear system produced by IPM or Simplex methods [17]. Specifically,
in a Projected-CG approach [18], one needs to solve several linear systems via CG to find solution
to the bound-constrained quadratic program, while there is no guarantee on how many iterations
it requires. On the other hand, Projected Gradient Method (PGM), despite its guaranteed iteration
complexity, has very slow convergence in practice. More recently, Multi-block ADMM [19, 20]
was proposed as a variant of ALM that, for each iteration, only updates one pass (or even less)
blocks of primal variables before each dual update, which however, requires a much smaller step
size in the dual update to ensure convergence [20, 21] and thus requires large number of iterations
for convergence to moderate precision. To our knowledge, there is still no report on a significant
improvement of ALM-based methods over IPM or Simplex method for Linear Programming.
In the recent years, Coordinate Descent (CD) method has demonstrated efficiency in many machine
learning problems with bound constraints or other non-smooth terms [9, 10, 22, 23, 24, 25] and has
solid analysis on its iteration complexity [26, 27]. In this work, we show that CD algorithm can
be naturally combined with ALM to solve Linear Program more efficiently than existing methods
on large-scale problems. We provide an O((log(1/))2) iteration complexity of the Augmented
Lagrangian with Coordinate Descent (AL-CD) algorithm that bounds the total number of CD up-
dates required for an -precise solution, and describe an implementation of AL-CD that has cost
O(nnz(A)) for each pass of CD. In practice, an active-set strategy is introduced to further reduce
cost of each iteration to the active size of variables and constraints for primal-sparse and dual-sparse
LP respectively, where a primal-sparse LP has most of variables being zero, and a dual-sparse LP
has few binding constraints at the optimal solution. Note, unlike in IPM, the conditioning of each
subproblem in ALM does not worsen over iterations [15, 16]. The AL-CD framework thus provides
an alternative to interior point and simplex methods when it is infeasible to exactly solving an n×n
(or m×m) linear system.
2 Sparse Linear Program
We are interested in solving linear programs of the form
min
x∈Rn
f(x) = cTx
s.t. AIx ≤ bI , AEx = bE
xj ≥ 0, j ∈ [nb]
(1)
where AI is mI by n matrix of coefficients and AE is mE by n. Without loss of generality, we
assume non-negative constraints are imposed on the first nb variables, denoted as xb, such that
x = [xb; xf ] and c = [cb; cf ]. The inequality and equality coefficient matrices can then be
partitioned as AI = [AI,b AI,f ] and AE = [AE,b AE,f ]. The dual problem of (1) then takes the
form
min
y∈Rm
g(y) = bT y
s.t. −ATb y ≤ cb , −ATf y = cf
yi ≥ 0, i ∈ [mI ].
(2)
2
where m = mI + mE , b = [bI ; bE ], Ab = [AI,b;AE,b], Af = [AI,f ;AE,f ], and y = [yI ; yE ]. In
most of LP occur in machine learning, m and n are both at scale in the order 105 ˜106, for which an
algorithm with costO(mn),O(n2) orO(m2) is unacceptable. Fortunately, there are usually various
types of sparsity present in the problem that can be utilized to lower the complexity.
First, the constraint matrix A = [AI ;AE ] are usually pretty sparse in the sense that nnz(A) mn,
and one can compute matrix-vector product Ax in O(nnz(A)). However, in most of current LP
solvers, not only matrix-vector product but also a linear system involving A needs to be solved,
which in general, has cost much more than O(nnz(A)) and can be up to O(min(n3,m3)) in the
worst case. In particular, the simplex-type methods, when moving from one corner to another,
requires solving a linear system that involves a sub-matrix of A with columns corresponding to the
basic variables [11], while in an interior point method (IPM), one also needs to solve a normal
equation system of matrix ADtAT to obtain the Newton direction, where Dt is a diagonal matrix
that gradually enforces complementary slackness as IPM iteration t grows [11]. While one remedy
to the high complexity is to employ iterative method such as Conjugate Gradient (CG) to solve the
system inexactly within IPM, this approach can hardly handle the ill-conditionedness occurs when
IPM iterates approaches boundary [12]. On the other hand, the Augmented Lagrangian approach
does not have such asymptotic ill-conditionedness and thus an iterative method with complexity
linear to O(nnz(A)) can be used to produce sufficiently accurate solution for each sub-problem.
Besides sparsity in the constraint matrix A, two other types of structures, which we termed primal
and dual sparsity, are also prevalent in the context of machine learning. A primal-sparse LP refers
to an LP with optimal solution x∗ comprising only few non-zero elements, while a dual-sparse LP
refers to an LP with few binding constraints at optimal, which corresponds to the non-zero dual
variables. In the following, we give two examples of sparse LP.
L1-Regularized Support Vector Machine The problem of L1-regularized multi-class Support
Vector Machine [1]
min
wm,ξi
λ
k∑
m=1
‖wm‖1 +
l∑
i=1
ξi
s.t. wTyixi − w
T
mxi ≥ emi − ξi, ∀(i,m)
(3)
where emi = 0 if yi = m, e
m
i = 1 otherwise. The task is dual-sparse since among all samples i and
class k, only those leads to misclassification will become binding constraints. The problem (3) is
also primal-sparse since it does feature selection through `1-penalty. Note the constraint matrix in
(3) is also sparse since each constraint only involves two weight vectors, and the pattern xi can be
also sparse.
Sparse Inverse Covariance Estimation The Sparse Inverse Covariance Estimation aims to find
a sparse matrix Ω that approximate the inverse of Covariance matrix. One of the most popular
approach to this solves a program of the form [6]
min
Ω∈Rd×d
‖Ω‖1
s.t. ‖SΩ− Id‖max ≤ λ
(4)
which is primal-sparse due to the ‖.‖1 penalty. The problem has a dense constraint matrix, which
however, has special structure where the coefficient matrix S can be decomposed into a product of
two low-rank and (possibly) sparse n by d matrices S = ZTZ. In case Z is sparse or n  d, this
decomposition can be utilized to solve the Linear Program much more efficiently. We will discuss
on how to utilize such structure in section 4.3.
3 Primal and Dual Augmented Coordinate Descent
In this section, we describe an Augmented Lagrangian method (ALM) that carefully tackles the
sparsity in a LP. The choice between Primal and Dual ALM depends on the type of sparsity present
in the LP. In particular, a primal AL method can solve a problem of few non-zero variables more
efficiently, while dual ALM will be more efficient for problem with few binding constraints. In the
following, we describe the algorithm only from the primal point of view, while the dual version can
be obtained by exchanging the roles of primal (1) and dual (2).
3
Algorithm 1 (Primal) Augmented Lagrangian Method
Initialization: y0 ∈ Rm and η0 > 0.
repeat
1. Solve (6) to obtain (xt+1, ξt+1) from yt.
2. Update yt+1 = yt + ηt
[
AIx
t+1 − bI + ξt+1
AEx
t+1 − bE
]
.
3. t = t+ 1.
4. Increase ηt by a constant factor if necessary.
until ‖[AIxt − bI ]+‖∞ ≤ p and ‖AExt − bE‖∞ ≤ .
3.1 Augmented Lagrangian Method (Dual Proximal Method)
Let g(y) be the dual objective function (2) that takes∞ if y is infeasible. The primal AL algorithm
can be interpreted as a dual proximal point algorithm [16] that for each iteration t solves
yt+1 = argmin
y
g(y) +
1
2ηt
‖y − yt‖2. (5)
Since g(y) is nonsmooth, (5) is not easier to solve than the original dual problem. However, the dual
of (5) takes the form:
min
x, ξ
F (x, ξ) = cTx+
ηt
2
∥∥∥∥ AIx− bI + ξAEx− bE + 1ηt
[
ytI
ytE
]∥∥∥∥2
s.t. xb ≥ 0, ξ ≥ 0,
(6)
which is a bound-constrained quadratic problem. Note given (x, ξ) as Lagrangian Multipliers of (5),
the corresponding y minimizing Lagrangian L(x, ξ, y) is
y(x, ξ) = ηt
[
AIx− bI + ξ
AEx− bE
]
+
[
ytI
ytE
]
, (7)
and thus one can solve (x∗, ξ∗) from (6) and find yt+1 through (7). The resulting algorithm is
sketched in Algorithm 1. For problem of medium scale, (6) is not easier to solve than a linear
system due to non-negative constraints, and thus an ALM is not preferred to IPM in the traditional
sense. However, for large-scale problem with m × n  nnz(A), the ALM becomes advantageous
since: (i) the conditioning of (6) does not worsen over iterations, and thus allows iterative methods
to solve it approximately in time proportional to O(nnz(A)). (ii) For a primal-sparse (dual-sparse)
problem, most of primal (dual) variables become binding at zero as iterates approach to the optimal
solution, which yields a potentially much smaller subproblem.
3.2 Solving Subproblem via Coordinate Descent
Given a dual solution yt, we employ a variant of Randomized Coordinate Descent (RCD) method
to solve subproblem (6). First, we note that, given x, the part of variables in ξ can minimized in
closed-form as
ξ(x) = [bI −AIx− ytI/ηt]+, (8)
where function [v]+ truncates each element of vector v to be non-negative as [v]+i = max{vi, 0}.
Then (6) can be re-written as
min
x
F̂ (x) = cTx+
ηt
2
∥∥∥∥ [AIx− bI + ytI/ηt]+AEx− bE + ytE/ηt
∥∥∥∥2
s.t. xb ≥ 0.
(9)
4
Algorithm 2 RCD for subproblem (6)
INPUT: ηt > 0 and (xt,0, wt,0, vt,0) satisfying
relation (11), (12).
OUTPUT: (xt,k, wt,k, vt,k)
repeat
1. Pick a coordinate j uniformly at random.
2. Compute ∇jF̂ (x),∇2j F̂ (x).
3. Obtain Newton direction d∗j .
4. Do line search (15) to find step size.
5. Update xt,k+1 ← xt,k + βrd∗j .
6. Maintain relation (11), (12).
7. k← k + 1.
until ‖d∗(x)‖∞ ≤ t.
Algorithm 3 PN-CG for subproblem (6)
INPUT: ηt > 0 and (xt,0, wt,0, vt,0) satisfying
relation (11), (12).
OUTPUT: (xt,k, wt,k, vt,k)
repeat
1. Identify active variables At,k.
2. Compute [∇jF (x)]At,k and set Dt,k.
3. Find Newton direction d∗At,k with CG.
4. Find step size via projected line search.
5. Update xt,k+1 ← (xt,k + βrd∗j )+.
6. Maintain relation (11), (12).
7. k← k + 1.
until ‖d∗At,k‖∞ ≤ t.
Denote the objective function as F̂ (x). The gradient of (9) can be expressed as
∇F̂ (x) = c+ ηtATI [w]+ + ηtATEv (10)
where
w = AIx− bI + ytI/ηt (11)
v = AEx− bE + ytE/ηt, (12)
and the (generalized) Hessian of (9) is
∇2F̂ (x) = ηtATI D(w)AI + ηtATEAE , (13)
where D(w) is an mI by mI diagonal matrix with Dii(w) = 1 if wi > 0 and Dii = 0 otherwise.
The RCD algorithm then proceeds as follows. In each iteration k, it picks a coordinate from j ∈
{1, .., n} uniformly at random and minimizes w.r.t. the coordinate. The minimization is conducted
by a single-variable Newton step, which first finds the Newton direction d∗j through minimizing a
quadratic approximation
d∗j = argmin
d
∇jF̂ (xt,k)d+
1
2
∇2j F̂ (xt,k)d2
s.t. xt,kj + d ≥ 0,
(14)
and then conducted a line search to find the smallest r ∈ {0, 1, 2, ...} satisfying
F̂ (xt,k + βrd∗jej)− F̂ (xt,k) ≤ σβr(∇jF̂ (xt,k)d∗j ). (15)
for some line-search parameter σ ∈ (0, 1/2], β ∈ (0, 1), where ej denotes a vector with only jth
element equal to 1 and all others equal to 0. Note the single-variable problem (14) has closed-form
solution
d∗j =
[
xt,kj −∇jF̂ (x
t,k
j )/∇
2
j F̂ (x
t,k
j )
]
+
− xt,kj , (16)
which in a naive implementation, takes O(nnz(A)) time due to the computation of (11) and (12).
However, in a clever implementation, one can maintain the relation (11), (12) as follows whenever
a coordinate xj is updated by βrd∗j[
wt,k+1
vt,k+1
]
=
[
wt,k
vt,k
]
+ βrd∗j
[
aIj
aEj
]
, (17)
where aj = [aIj ; a
E
j ] denotes the jth column of AI and AE . Then the gradient and (generalized)
second-derivative of jth coordinate
∇jF̂ (x) = cj + ηt〈aIj , [w]+〉+ ηt〈aEj , v〉
∇2j F̂ (x) = ηt
( ∑
i:wi>0
(aIi,j)
2 +
∑
i
(aEi,j)
2
)
(18)
5
can be computed in O(nnz(aj)) time. Similarly, for each coordinate update, one can evaluate the
difference of function value F̂ (xt,k + d∗jej) − F̂ (xt,k) in O(nnz(aj)) by only computing terms
related to the jth variable.
The overall procedure for solving subproblem is summarized in Algorithm 2. In practice, a random
permutation is used instead of uniform sampling to ensure that every coordinate is updated once
before proceeding to the next round, which can speed up convergence and ease the checking of stop-
ping condition ‖d∗(x)‖∞ ≤ t, and an active-set strategy is employed to avoid updating variables
with d∗j = 0. We describe details in section 4
3.3 Convergence Analysis
In this section, we prove the iteration complexity of AL-CD method. Existing analysis [26, 27]
shows that Randomized Coordinate Descent can be up to n times faster than Gradient-based methods
in certain conditions. However, to prove a global linear rate of convergence the analysis requires
objective function to be strongly convex, which is not true for our sub-problem (6). Here we follow
the approach in [28, 29] to show global linear convergence of Algorithm 2 by utilizing the fact that,
when restricted to a constant subspace, (6) is strongly convex. All proofs will be included in the
appendix.
Theorem 1 (Linear Convergence). Denote F ∗ as the optimum of (6) and x̄ = [x; ξ]. The iterates
{x̄k}∞k=0 of the RCD Algorithm 2 has
E[F (x̄k+1)]− F ∗ ≤
(
1− 1
γn
)(
E[F (x̄k)]− F ∗
)
, (19)
where
γ = max
{
16ηtMθ(F
0 − F ∗) , 2Mθ(1 + 4L2g) , 6
}
,
M = maxj∈[n̄] ‖āj‖2 is an upper bound on coordinate-wise second derivative, and Lg is local
Lipschitz-continuous constant of function g(z) = ηt‖z−b+yt/ηt‖2, and θ is constant of Hoffman’s
bound that depends on the polyhedron formed by the set of optimal solutions.
Then the following theorem gives a bound on the number of iterations required to find an 0-precise
solution in terms of the proximal minimization (5).
Theorem 2 (Inner Iteration Complexity). Denote y(x̄k) as the dual solution (7) corresponding to
the primal iterate x̄k. To guarantee
‖y(x̄k)− yt+1‖ ≤ 0 (20)
with probability 1− p, it suffices running RCD Algorithm 2 for number of iterations
k ≥ 2γn log
(√
2(F (x̄0)− F ∗)
ηtp
1
0
)
.
Now we prove the overall iteration complexity of AL-CD. Note that existing linear convergence
analysis of ALM on Linear Program [16] assumes exact solutions of subproblem (6), which is
not possible in practice. Our next theorem extends the linear convergence result to cases when
subproblems are solved inexactly, and in particular, shows the total number of coordinate descent
updates required to find an -accurate solution.
Theorem 3 (Iteration Complexity). Denote {ŷt}∞t=1 as the sequence of iterates obtained from inex-
act dual proximal updates, {yt}∞t=1 as that generated by exact updates, and yS∗ as the projection
of y to the set of optimal dual solutions. To guarantee ‖ŷt − ŷtS∗‖ ≤ 2 with probability 1 − p, it
suffices to run Algorithm 1 for
T = (1 +
1
α
) log
(
LR

)
(21)
outer iterations with ηt = (1 + α)L, and solve each sub-problem (6) by running Algorithm 2 for
k ≥ 2γn
(
log
(ω

)
+
3
2
log
(
(1 +
1
α
) log
LR

))
(22)
inner iterations, where L is a constant depending on the polyhedral set of optimal solutions, ω =√
2(1+α)L(F 0−F∗)
p , R = ‖proxηtg(y
0) − y0‖, and F 0, F ∗ are upper and lower bounds on the
initial and optimal function values of subproblem respectively.
6
3.4 Fast Asymptotic Convergence via Projected Newton-CG
The RCD algorithm converges to a solution of moderate precision efficiently, but in some problems
a higher precision might be required. In such case, we transfer the subproblem solver from RCD
to a Projected Newton-CG (PN-CG) method after iterates are close enough to the optimum. Note
the Projected Newton method does not have global iteration complexity but has fast convergence for
iterates very close to the optimal.
Denote F (x) as the objective in (9). Each iterate of PN-CG begins by finding the set of active
variables defined as
At,k = {j|xt,kj > 0 ∨∇jF (x
t,k) < 0}. (23)
Then the algorithm fixes xt,kj = 0,∀j /∈ At,k and solves a Newton linear system w.r.t. j ∈ At,k
[∇2At,kF (x
t,k)]d = −[∇At,kF (xt,k)] (24)
to obtain direction d∗ for the current active variables. Let dAt,k denotes a size-n vector taking value
in d∗ for j ∈ At,k and taking value 0 for j /∈ At,k. The algorithm then conducts a projected line
search to find smallest r ∈ {0, 1, 2, ...} satisfying
F ([xt,k + βrdAt,k ]+)− F (xt,k) ≤ σβr(∇jF (xt,k)dAt,k), (25)
and update x by xt,k+1 ← (xt,k + βrd∗j )+. Compared to interior point method, one key to the
tractability of this approach lies on the conditioning of linear system (24), which does not worsen
as outer iteration t increases, so an iterative Conjugate Gradient (CG) method can be used to obtain
accurate solution without factorizing the Hessian matrix. The only operation required within CG is
the Hessian-vector product
[∇2At,kF (x
t,k)]s = ηt [A
T
I D(w
t,k)AI +A
T
EAE ]At,k s, (26)
where the operator [.]At,k takes the sub-matrix with row and column indices belonging to At,k. For
a primal or dual-sparse LP, the product (26) can be evaluated very efficiently, since it only involves
non-zero elements in columns of AI , AE belonging to the active set, and rows of AI corresponding
to the binding constraints for which Dii(wt,k) > 0. The overall cost of the product (26) is only
O
(
nnz([AI ]Dt,k,At,k) + nnz([AE ]:,At,k)
)
,
where Dt,k = {i|wt,ki > 0} is the set of current binding constraints. Considering that the com-
putational bottleneck of PN-CG is on the CG iterations for solving linear system (24), the efficient
computation of product (26) reduces the overall complexity of PN-CG significantly. The whole
procedure is summarized in Algorithm 3.
4 Practical Issues
4.1 Precision of Subproblem Minimization
In practice, it is unnecessary to solve subproblem (6) to high precision, especially for iterations
of ALM in the beginning. In our implementation, we employ a two-phase strategy, where in the
first phase we limit the cost spent on each sub-problem (6) to be a constant multiple of nnz(A),
while in the second phase we dynamically increment the AL parameter ηt and inner precision t to
ensure sufficient decrease in the primal and dual infeasibility respectively. The two-phase strategy
is particularly useful for primal or dual-sparse problem, where sub-problem in the latter phase has
smaller active set that results in less computation cost even when solved to high precision.
4.2 Active-Set Strategy
Our implementation of Algorithm 2 maintains an active set of variables A, which initially contains
all variables, but during the RCD iterates, any variable xj binding at 0 with gradient ∇jF greater
than a threshold δ will be excluded from A till the end of each subproblem solving. A will be
re-initialized after each dual proximal update (7). Note in the initial phase, the cost spent on each
subproblem is a constant multiple of nnz(A), so if |A| is small one would spend more iterations on
the active variables to achieve faster convergence.
7
4.3 Dealing with Decomposable Constraint Matrix
When we have a m by n constraint matrix A = UV T that can be decomposed into product of an
m× r matrix U and a r × n matrix V T , if r  min{m,n} or nnz(U) + nnz(V ) nnz(A), we
can re-formulate the constraint Ax ≤ b as Uz ≤ b , V Tx = z with auxiliary variables z ∈ Rr.
This new representation reduce the cost of Hessian-vector product in Algorithm 3 and the cost of
each pass of CD in Algorithm 2 from O(nnz(A)) to O(nnz(U) + nnz(V )).
5 Numerical Experiments
Table 1: Timing Results (in sec. unless specified o.w.) on Multiclass L1-regularized SVM
Data nb mI P-Simp. D-Simp. Barrier D-ALCD P-ALCD
rcv1 4,833,738 778,200 > 48hr > 48hr > 48hr 3,452 3,155
news 2,498,415 302,765 > 48hr 37,912 > 48hr 148 395
sector 11,597,992 666,848 > 48hr 9,282 > 48hr 1,419 2,029
mnist 75,620 540,000 6,454 2,556 73,036 146 7,207
cod-rna.rf 69,537 59,535 86,130 5,738 > 48hr 3,130 2,676
vehicle 79,429 157,646 3,296 143.33 8,858 31 598
real-sim 114,227 72,309 > 48hr 49,405 89,476 179 297
Table 2: Timing Results (in sec. unless specified o.w.) on Sparse Inverse Covariance Estimation
Data nb mI mE nf P-Simp D-Simp Barrier D-ALCD P-ALCD
textmine 60,876 60,876 43,038 43,038 > 48hr > 48hr > 48hr 43,096 18,507
E2006 55,834 55,834 32,174 32,174 > 48hr > 48hr 94623 > 48hr 4,207
dorothea 47,232 47,232 1,600 1,600 3,980 103 82 47 38
Table 3: Timing Results (in sec. unless specified o.w.) for Nonnegative Matrix Factorization.
Data nb mI P-Simp. D-Simp. Barrier D-ALCD P-ALCD
micromass 2,896,770 4,107,438 > 96hr > 96hr 280,230 12,966 12,119
ocr 6,639,433 13,262,864 > 96hr > 96hr 284,530 40,242 > 96hr
In this section, we compare the AL-CD algorithm with state-of-the-art implementation of interior
point and primal, dual Simplex methods in commercial LP solver CPLEX, which is of top efficiency
among many LP solvers as investigated in [30]. For all experiments, the stopping criteria is set to
require both primal and dual infeasibility (in the `∞-norm) smaller than 10−3 and set the initial sub-
problem tolerance t = 10−2 and ηt = 1. The LP instances are generated from L1-SVM (3), Sparse
Inverse Covariance Estimation (4) and Nonnegative Matrix Factorization [3]. For the Sparse Inverse
Covariance Estimation problem, we use technique introduced in section 4.3 to decompose the low-
rank matrix S, and since (4) results in d independent problems for each column of the estimated
matrix, we report result on only one of them. The data source and statistics are included in the
appendix.
Among all experiments, we observe that the proposed primal, dual AL-CD methods become partic-
ularly advantageous when the matrix A is sparse. For example, for text data set rcv1, real-sim and
news in Table 1, the matrix A is particularly sparse and AL-CD can be orders of magnitude faster
than other approaches by avoiding solving n× n linear system exactly. In addition, the dual-ALCD
(also dual simplex) is more efficient in L1-SVM problem due to the problem’s strong dual sparsity,
while the primal-ALCD is more efficient on the primal-sparse Inverse Covariance estimation prob-
lem. For the Nonnegative Matrix Factorization problem, both the dual and primal LP solutions are
not particularly sparse due to the choice of matrix approximation tolerance (1% of #samples), but
the AL-CD approach is still comparably more efficient.
Acknowledgement We acknowledge the support of ARO via W911NF-12-1-0390, and the support
of NSF via grants CCF-1320746, CCF-1117055, IIS-1149803, IIS-1320894, IIS-1447574, DMS-
1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support
Research at the Interface of the Biological and Mathematical Sciences.
8
References
[1] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. NIPS, 2004.
[2] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
[3] N. Gillis and R. Luce. Robust near-separable nonnegative matrix factorization using linear optimization.
JMLR, 2014.
[4] A. Nellore and R. Ward. Recovery guarantees for exemplar-based clustering. arXiv., 2013.
[5] I. Yen, X. Lin, K. Zhong, P. Ravikumar, and I. Dhillon. A convex exemplar-based approach to MAD-
Bayes dirichlet process mixture models. In ICML, 2015.
[6] M. Yuan. High dimensional inverse covariance matrix estimation via linear programming. JMLR, 2010.
[7] D. Bello and G. Riano. Linear programming solvers for Markov decision processes. In Systems and
Information Engineering Design Symposium, pages 90–95, 2006.
[8] T. Joachims. Training linear svms in linear time. In KDD. ACM, 2006.
[9] C. Hsieh, K. Chang, C. Lin, S.S. Keerthi, and S. Sundararajan. A dual coordinate descent method for
large-scale linear SVM. In ICML, volume 307. ACM, 2008.
[10] G. Yuan, C. Hsieh K. Chang, and C. Lin. A comparison of optimization methods and software for large-
scale l1-regularized linear classification. JMLR, 11, 2010.
[11] J. Nocedal and S.J. Wright. Numerical Optimization. Springer, 2006.
[12] J. Gondzio. Interior point methods 25 years later. EJOR, 2012.
[13] J. Gondzio. Matrix-free interior point method. Computational Optimization and Applications, 2012.
[14] V.Eleuterio and D.Lucia. Finding approximate solutions for large scale linear programs. Thesis, 2009.
[15] Evtushenko, Yu. G, Golikov, AI, and N. Mollaverdy. Augmented lagrangian method for large-scale linear
programming problems. Optimization Methods and Software, 20(4-5):515–524, 2005.
[16] F. Delbos and J.C. Gilbert. Global linear convergence of an augmented lagrangian algorithm for solving
convex quadratic optimization problems. 2003.
[17] O. Güler. Augmented lagrangian algorithms for linear programming. Journal of optimization theory and
applications, 75(3):445–470, 1992.
[18] J. Moré J and G. Toraldo. On the solution of large quadratic programming problems with bound con-
straints. SIAM Journal on Optimization, 1(1):93–113, 1991.
[19] M. Hong and Z. Luo. On linear convergence of alternating direction method of multipliers. arXiv, 2012.
[20] H. Wang, A. Banerjee, and Z. Luo. Parallel direction method of multipliers. In NIPS, 2014.
[21] C.Chen, B.He, Y.Ye, and X.Yuan. The direct extension of admm for multi-block convex minimization
problems is not necessarily convergent. Mathematical Programming, 2014.
[22] I.Dhillon, P.Ravikumar, and A.Tewari. Nearest neighbor based greedy coordinate descent. In NIPS, 2011.
[23] I. Yen, C. Chang, T. Lin, S. Lin, and S. Lin. Indexed block coordinate descent for large-scale linear
classification with limited memory. In KDD. ACM, 2013.
[24] I. Yen, S. Lin, and S. Lin. A dual-augmented block minimization framework for learning with limited
memory. In NIPS, 2015.
[25] K. Zhong, I. Yen, I. Dhillon, and P. Ravikumar. Proximal quasi-Newton for computationally intensive
l1-regularized m-estimators. In NIPS, 2014.
[26] P. Richtárik and M. Takáč. Iteration complexity of randomized block-coordinate descent methods for
minimizing a composite function. Mathematical Programming, 144(1-2):1–38, 2014.
[27] Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341–362, 2012.
[28] P. Wang and C. Lin. Iteration complexity of feasible descent methods for convex optimization. The
Journal of Machine Learning Research, 15(1):1523–1548, 2014.
[29] I. Yen, C. Hsieh, P. Ravikumar, and I.S. Dhillon. Constant nullspace strong convexity and fast convergence
of proximal methods under high-dimensional settings. In NIPS, 2014.
[30] B. Meindl and M. Templ. Analysis of commercial and free and open source solvers for linear optimization
problems. Eurostat and Statistics Netherlands, 2012.
[31] A.J. Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the
National Bureau of Standards, 49(4):263–265, 1952.
[32] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
[33] I. Yen, T. Lin, S. Lin, P. Ravikumar, and I. Dhillon. Sparse random feature algorithm as coordinate descent
in Hilbert space. In NIPS, 2014.
9
