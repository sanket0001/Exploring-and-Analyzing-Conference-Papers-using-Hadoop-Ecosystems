


Paper ID = 5833
Title = Online Learning with Adversarial Delays
Kent Quanrudâˆ—and Daniel Khashabiâ€ 
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{quanrud2,khashab2}@illinois.edu
Abstract
We study the performance of standard online learning algorithms when the feed-
back is delayed by an adversary. We show that online-gradient-descent
[1] and follow-the-perturbed-leader [2] achieve regret O(
âˆš
D) in the
delayed setting, where D is the sum of delays of each roundâ€™s feedback. This
bound collapses to an optimal O(
âˆš
T ) bound in the usual setting of no delays
(where D = T ). Our main contribution is to show that standard algorithms for
online learning already have simple regret bounds in the most general setting of
delayed feedback, making adjustments to the analysis and not to the algorithms
themselves. Our results help affirm and clarify the success of recent algorithms in
optimization and machine learning that operate in a delayed feedback model.
1 Introduction
Consider the following simple game. Let K be a bounded set, such as the unit `1 ball or a collection
of n experts. Each round t, we pick a point xt âˆˆ K. An adversary then gives us a cost function ft,
and we incur the loss `t = ft(xt). After T rounds, our total loss is the sum LT =
âˆ‘T
t=1 `t, which
we want to minimize.
We cannot hope to beat the adversary, so to speak, when the adversary picks the cost function after
we select our point. There is margin for optimism, however, if rather than evaluate our total loss in
absolute terms, we compare our strategy to the best fixed point in hindsight. The regret of a strategy
x1, . . . , xT âˆˆ K is the additive difference R(T ) =
âˆ‘T
t=1 ft(xt)âˆ’ argminxâˆˆK
âˆ‘T
t=1 ft(x).
Surprisingly, one can obtain positive results in terms of regret. Kalai and Vempala showed that a
simple and randomized follow-the-leader type algorithm achieves R(T ) = O(
âˆš
T ) in expectation
for linear cost functions [2] (here, the big-O notation assumes that the diameter of K and the ftâ€™s
are bounded by constants). If K is convex, then even if the cost vectors are more generally convex
cost functions (where we incur losses of the form `t = ft(xt), with ft a convex function), Zinke-
vich showed that gradient descent achieves regret R(T ) = O(
âˆš
T ) [1]. There is a large body of
theoretical literature about this setting, called online learning (see for example the surveys by Blum
[3], Shalev-Shwartz [4], and Hazan [5]).
Online learning is general enough to be applied to a diverse family of problems. For example, Kalai
and Vempalaâ€™s algorithm can be applied to online combinatorial problems such as shortest paths
[6], decision trees [7], and data structures [8, 2]. In addition to basic machine learning problems
with convex loss functions, Zinkevich considers applications to industrial optimization, where the
âˆ—http://illinois.edu/~quanrud2/. Supported in part by NSF grants CCF-1217462, CCF-
1319376, CCF-1421231, CCF-1526799.
â€ http://illinois.edu/~khashab2/. Supported in part by a grant from Google.
1
value of goods is not known until after the goods are produced. Other examples of applications of
online learning include universal portfolios in finance [9] and online topic-ranking for multi-labeled
documents [10].
The standard setting assumes that the cost vector ft (or more generally, the feedback) is given to and
processed by the player before making the next decision in round t+ 1. Philosophically, this is not
how decisions are made in real life: we rush through many different things at the same time with no
pause for careful consideration, and we may not realize our mistakes for a while. Unsurprisingly, the
assumption of immediate feedback is too restrictive for many real applications. In online advertising,
online learning algorithms try to predict and serve ads that optimize for clicks [11]. The algorithm
learns by observing whether or not an ad is clicked, but in production systems, a massive number of
ads are served between the moment an ad is displayed to a user and the moment the user has decided
to either click or ignore that ad. In military applications, online learning algorithms are used by radio
jammers to identify efficient jamming strategies [12]. After a jammer attempts to disrupt a packet
between a transmitter and a receiver, it does not know if the jamming attempt succeeded until an
acknowledgement packet is sent by the receiver. In cloud computing, online learning helps devise
efficient resource allocation strategies, such as finding the right mix of cheaper (and inconsistent)
spot instances and more reliable (and expensive) on-demand instances when renting computers for
batch jobs [13]. The learning algorithm does not know how well an allocation strategy worked for
a batch job until the batch job has ended, by which time many more batch jobs have already been
launched. In finance, online learning algorithms managing portfolios are subject to information and
transaction delays from the market, and financial firms invest heavily to minimize these delays.
One strategy to handle delayed feedback is to pool independent copies of a fixed learning algorithm,
each of which acts as an undelayed learner over a subsequence of the rounds. Each round is dele-
gated to a single instance from the pool of learners, and the learner is required to wait for and process
its feedback before rejoining the pool. If there are no learners available, a new copy is instantiated
and added to the pool. The size of the pool is proportional to the maximum number of outstanding
delays at any point of decision, and the overall regret is bounded by the sum of regrets of the individ-
ual learners. This approach is analyzed for constant delays by Weinberger and Ordentlich [14], and
a more sophisticated analysis is given by Joulani et al. [15]. If Î± is the expected maximum number
of outstanding feedbacks, then Joulani et al. obtain a regret bound on the order of O(
âˆš
Î±T ) (in ex-
pectation) for the setting considered here. The blackbox nature of this approach begets simultaneous
bounds for other settings such as partial information and stochastic rewards. Although maintaining
copies of learners in proportion to the delay may be prohibitively resource intensive, Joulani et al.
provide a more efficient variant for the stochastic bandit problem, a setting not considered here.
Another line of research is dedicated to scaling gradient descent type algorithms to distributed set-
tings, where asynchronous processors naturally introduce delays in the learning framework. A clas-
sic reference in this area is the book of Bertsekas and Tsitskilis [16]. If the data is very sparse, so that
input instances and their gradients are somewhat orthogonal, then intuitively we can apply gradients
out of order without significant interference across rounds. This idea is explored by Recht et al. [17],
who analyze and test parallel algorithm on a restricted class of strongly convex loss functions, and
by Duchi et al. [18] and McMahan and Streeter [19], who design and analyze distributed variants
of adaptive gradient descent [20]. Perhaps the most closely related work in this area is by Langford
et al., who study the online-gradient-descent algorithm of Zinkevich when the delays are
bounded by a constant number of rounds [21]. Research in this area has largely moved on from the
simplistic models considered here; see [22, 23, 24] for more recent developments.
The impact of delayed feedback in learning algorithms is also explored by Riabko [25] under the
framework of â€œweak teachersâ€.
For the sake of concreteness, we establish the following notation for the delayed setting. For each
round t, let dt âˆˆ Z+ be a non-negative integer delay. The feedback from round t is delivered at the
end of round t + dt âˆ’ 1, and can be used in round t + dt. In the standard setting with no delays,
dt = 1 for all t. For each round t, let Ft = {u âˆˆ [T ] : u+ du âˆ’ 1 = t} be the set of rounds whose
feedback appears at the end of round t. We let D =
âˆ‘T
t=1 dt denote the sum of all delays; in the
standard setting with no delays, we have D = T .
In this paper, we investigate the implications of delayed feedback when the delays are adversarial
(i.e., arbitrary), with no assumptions or restrictions made on the adversary. Rather than design new
2
algorithms that may generate a more involved analysis, we study the performance of the classical
algorithms online-gradient-descent and follow-the-perturbed-leader, essen-
tially unmodified, when the feedback is delayed. In the delayed setting, we prove that both algo-
rithms have a simple regret bound of O(
âˆš
D). These bounds collapse to match the well-known
O(
âˆš
T ) regret bounds if there are no delays (i.e., where D = T ).
Paper organization In Section 2, we analyze the online-gradient-descent algorithm in
the delayed setting, giving upper bounds on the regret as a function of the sum of delays D. In
Section 3, we analyze the follow-the-perturbed-leader in the delayed setting and derive
a regret bound in terms ofD. Due to space constraints, extensions to online-mirror-descent
and follow-the-lazy-leader are deferred to the appendix. We conclude and propose future
directions in Section 4.
2 Delayed gradient descent
Convex optimization In online convex optimization, the input domain K is convex, and each
cost function ft is convex. For this setting, Zinkevich proposed a simple online algorithm, called
online-gradient-descent, designed as follows [1]. The first point, x1, is picked in K
arbitrarily. After picking the tth point xt, online-gradient-descent computes the gradient
âˆ‡ft|xt of the loss function at xt, and chooses xt+1 = Ï€K(xt âˆ’ Î·âˆ‡ft|xt) in the subsequent round,
for some parameter Î· âˆˆ R>0. Here, Ï€K is the projection that maps a point xâ€² to its nearest point
in K (discussed further below). Zinkevich showed that, assuming the Euclidean diameter of K
and the Euclidean lengths of all gradients âˆ‡ft|x are bounded by constants, online-gradient-
descent has an optimal regret bound of O(
âˆš
T ).
Delayed gradient descent In the delayed setting, the loss function ft is not necessarily given by
the adversary before we pick the next point xt+1 (or even at all). The natural generalization of
online-gradient-descent to this setting is to process the convex loss functions and apply
their gradients the moment they are delivered. That is, we update
xâ€²t+1 = xt âˆ’ Î·
âˆ‘
sâˆˆFt
âˆ‡fs|xs ,
for some fixed parameter Î·, and then project xt+1 = Ï€K(xâ€²t+1) back into K to choose our (t+1)th
point. In the setting of Zinkevich, we have Ft = {t} for each t, and this algorithm is exactly
online-gradient-descent. Note that a gradientâˆ‡fs|xs does not need to be timestamped by
the round s from which it originates, which is required by the pooling strategies of Weinberger and
Ordentlich [14] and Joulani et al. [15] in order to return the feedback to the appropriate learner.
Theorem 2.1. Let K be a convex set with diameter 1, let f1, . . . , fT be convex functions over K
with â€–âˆ‡ft|xâ€–2 â‰¤ L for all x âˆˆ K and t âˆˆ [T ], and let Î· âˆˆ R be a fixed parameter. In the presence
of adversarial delays, online-gradient-descent selects points x1, . . . , xT âˆˆ K such that
for all y âˆˆ K,
Tâˆ‘
t=1
ft(xt)âˆ’
Tâˆ‘
t=1
ft(y) = O
(
1
Î·
+ Î·L2(T +D)
)
,
where D denotes the sum of delays over all rounds t âˆˆ [T ].
For Î· = 1/L
âˆš
T +D, Theorem 2.1 implies a regret bound of O(L
âˆš
D + T ) = O(L
âˆš
D). This
choice of Î· requires prior knowledge of the final sum D. When this sum is not known, one can
calculate D on the fly: if there are Î´ outstanding (undelivered) cost functions at a round t, then D
increases by exactly Î´. Obviously, Î´ â‰¤ T and T â‰¤ D, so D at most doubles. We can therefore
employ the â€œdoubling trickâ€ of Auer et al. [26] to dynamically adjust Î· as D grows.
In the undelayed setting analyzed by Zinkevich, we have D = T , and the regret bound of Theorem
2.1 matches that obtained by Zinkevich. If each delay dt is bounded by some fixed value Ï„ , Theorem
2.1 implies a regret bound of O(L
âˆš
Ï„T ) that matches that of Langford et al. [21]. In both of these
special cases, the regret bound is known to be tight.
3
Before proving Theorem 2.1, we review basic definitions and facts on convexity. A function f :
K â†’ R is convex if
f((1âˆ’ Î±)x+ Î±y) â‰¤ (1âˆ’ Î±)f(x) + Î±f(y) âˆ€x, y âˆˆ K,Î± âˆˆ [0, 1].
If f is differentiable, then f is convex iff
f(x) +âˆ‡f |x Â· (y âˆ’ x) â‰¤ f(y) âˆ€x, y âˆˆ K. (1)
For f convex but not necessarily differentiable, a subgradient of f at x is any vector that can replace
âˆ‡f |x in equation (1). The (possible empty) set of gradients of f at x is denoted by âˆ‚f(x).
The gradient descent may occasionally update along a gradient that takes us out of the constrained
domain K. If K is convex, then we can simply project the point back into K.
Lemma 2.2. Let K be a closed convex set in a normed linear space X and x âˆˆ X a point, and let
xâ€² âˆˆ K be the closest point in K to x. Then, for any point y âˆˆ K,
â€–xâˆ’ yâ€–2 â‰¤ â€–x
â€² âˆ’ yâ€–2.
We let Ï€K denote the map taking a point x to its closest point in the convex set K.
Proof of Theorem 2.1. Let y = argminxâˆˆK(f1(x) + Â· Â· Â·+ fT (x)) be the best point in hindsight at
the end of all T rounds. For t âˆˆ [T ], by convexity of ft, we have,
ft(y) â‰¥ ft(xt) +âˆ‡ft|xt Â· (y âˆ’ xt).
Fix t âˆˆ [T ], and consider the distance between xt+1 and y. By Lemma 2.2, we know that
â€–xt+1 âˆ’ yâ€–2 â‰¤
âˆ¥âˆ¥xâ€²t+1 âˆ’ yâˆ¥âˆ¥2, where xâ€²t+1 = xt âˆ’ Î·âˆ‘sâˆˆFt âˆ‡fs|xs .
We split the sum of gradients applied in a single round and consider them one by one. For each
s âˆˆ Ft, letFt,s = {r âˆˆ Ft : r < s}, and let xt,s = xtâˆ’Î·
âˆ‘
râˆˆFt,s âˆ‡fr|xr . SupposeFt is nonempty,
and fix sâ€² = maxFt to be the last index in Ft. By Lemma 2.2, we have,
â€–xt+1 âˆ’ yâ€–22 â‰¤
âˆ¥âˆ¥xâ€²t+1 âˆ’ yâˆ¥âˆ¥22 = âˆ¥âˆ¥xt,sâ€² âˆ’ Î·âˆ‡fsâ€² |xsâ€² âˆ’ yâˆ¥âˆ¥22
= â€–xt,sâ€² âˆ’ yâ€–22 âˆ’ 2Î·
(
âˆ‡fsâ€² |xsâ€² Â· (xt,sâ€² âˆ’ y)
)
+ Î·2
âˆ¥âˆ¥âˆ‡fsâ€² |xsâ€²âˆ¥âˆ¥22.
Repeatedly unrolling the first term in this fashion gives
â€–xt+1 âˆ’ yâ€–22 â‰¤ â€–xt âˆ’ yâ€–
2
2 âˆ’ 2Î·
âˆ‘
sâˆˆFt
âˆ‡fs|xs Â· (xt,s âˆ’ y) + Î·2
âˆ‘
sâˆˆFt
â€–âˆ‡fs|xsâ€–
2
2.
For each s âˆˆ Ft, by convexity of f , we have,
âˆ’âˆ‡fs|xs Â· (xt,s âˆ’ y) = âˆ‡fs|xs Â· (y âˆ’ xt,s) = âˆ‡fs|xs Â· (y âˆ’ xs) +âˆ‡fs|xs Â· (xs âˆ’ xt,s)
â‰¤ fs(y)âˆ’ fs(xs) +âˆ‡fs|xs Â· (xs âˆ’ xt,s).
By assumption, we also have â€–âˆ‡fs|xsâ€–2 â‰¤ L for each s âˆˆ Ft. With respect to the distance between
xt+1 and y, this gives,
â€–xt+1 âˆ’ yâ€–22 â‰¤ â€–xt âˆ’ yâ€–
2
2 + 2Î·
âˆ‘
sâˆˆFt
(fs(y)âˆ’ fs(xs) +âˆ‡fs|xs Â· (xs âˆ’ xt,s)) + Î·2 Â· |Ft| Â· L2.
Solving this inequality for the regret terms
âˆ‘
sâˆˆFt fs(xs)âˆ’fs(y) and taking the sum of inequalities
over all rounds t âˆˆ [T ], we have,
Tâˆ‘
t=1
(ft(xt)âˆ’ ft(y)) =
Tâˆ‘
t=1
âˆ‘
sâˆˆFt
fs(xs)âˆ’ fs(y)
â‰¤ 1
2Î·
Â·
Tâˆ‘
t=1
(
â€–xt âˆ’ yâ€–22 âˆ’ â€–xt+1 âˆ’ yâ€–
2
2 + 2Î·
âˆ‘
sâˆˆFt
âˆ‡fs|xs Â· (xs âˆ’ xt,s) + Î·2 Â· |Ft| Â· L2
)
=
1
2Î·
(
Tâˆ‘
t=1
â€–xt âˆ’ yâ€–22 âˆ’ â€–xt+1 âˆ’ yâ€–
2
2
)
+
Î·
2
TL2 +
Tâˆ‘
t=1
âˆ‘
sâˆˆFt
âˆ‡fs|xs Â· (xs âˆ’ xt,s)
â‰¤ 1
2Î·
+
Î·
2
TL2 +
Tâˆ‘
t=1
âˆ‘
sâˆˆFt
âˆ‡fs|xs Â· (xs âˆ’ xt,s). (2)
4
The first two terms are familiar from the standard analysis of online-gradient-descent. It
remains to analyze the last sum, which we call the delay term.
Each summand âˆ‡fs|xs Â· (xs âˆ’ xt,s) in the delay term contributes loss proportional to the distance
between the point xs when the gradient âˆ‡fs|xs is generated and the point xt,s when the gradient is
applied. This distance is created by the other gradients that are applied in between, and the number
of such in-between gradients are intimately tied to the total delay, as follows. By Cauchy-Schwartz,
the delay term is bounded above by
Tâˆ‘
t=1
âˆ‘
sâˆˆFt
âˆ‡fs|xs Â· (xs âˆ’ xt,s) â‰¤
Tâˆ‘
t=1
âˆ‘
sâˆˆFt
â€–âˆ‡fs|xsâ€–2â€–xs âˆ’ xt,sâ€–2 â‰¤ L
Tâˆ‘
t=1
âˆ‘
sâˆˆFt
â€–xs âˆ’ xt,sâ€–2. (3)
Consider a single term â€–xs âˆ’ xt,sâ€–2 for fixed t âˆˆ [T ] and s âˆˆ Ft. Intuitively, the difference xt,sâˆ’xs
is roughly the sum of gradients received between round s and when we apply the gradient from round
s in round t. More precisely, by applying the triangle inequality and Lemma 2.2, we have,
â€–xt,s âˆ’ xsâ€–2 â‰¤ â€–xt,s âˆ’ xtâ€–2 + â€–xt âˆ’ xsâ€–2 â‰¤ â€–xt,s âˆ’ xtâ€–2 + â€–x
â€²
t âˆ’ xsâ€–2.
For the same reason, we have â€–xâ€²t âˆ’ xsâ€–2 â‰¤ â€–xâ€²t âˆ’ xtâˆ’1â€–2 +
âˆ¥âˆ¥xâ€²tâˆ’1 âˆ’ xsâˆ¥âˆ¥2, and unrolling in this
fashion, we have,
â€–xt,s âˆ’ xsâ€–2 â‰¤ â€–xt,s âˆ’ xtâ€–2 +
tâˆ’1âˆ‘
r=s
âˆ¥âˆ¥xâ€²r+1 âˆ’ xrâˆ¥âˆ¥2 â‰¤ Î· âˆ‘
pâˆˆFt,s
âˆ¥âˆ¥âˆ‡fp|xpâˆ¥âˆ¥2 + Î· tâˆ’1âˆ‘
r=s
âˆ‘
qâˆˆFr
âˆ¥âˆ¥âˆ‡fq|xqâˆ¥âˆ¥2
â‰¤ Î· Â· L Â·
(
|Ft,s|+
tâˆ’1âˆ‘
r=s
|Fr|
)
. (4)
After substituting equation (4) into equation (3), it remains to bound the sum
âˆ‘T
t=1
âˆ‘
sâˆˆFt(|Ft,s|+âˆ‘tâˆ’1
r=s|Fr|). Consider a single term |Ft,s| +
âˆ‘tâˆ’1
r=s|Fr| in the sum. This quantity counts, for a
gradient âˆ‡fs|xs from round s delivered just before round t â‰¥ s, the number of other gradients that
are applied while âˆ‡fs|xs is withheld. Fix two rounds s and t, and consider an intermediate round
r âˆˆ {s, . . . , t}. If r < t then fix q âˆˆ Fr, and if r = t then fix q âˆˆ Ft,s. The feedback from round q
is applied in a round r between round s and round t. We divide our analysis into two scenarios. In
one case, q â‰¤ s, and the gradient from round q appears only after s, as in the following diagram.
q //
âˆ‡fq|xq
%%Â· Â· Â· // s //
âˆ‡fs|xs
$$Â· Â· Â· // r // Â· Â· Â· // t
In the other case, q > s, as in the following diagram.
s //
âˆ‡fs|xs
))Â· Â· Â· // q //
âˆ‡fq|xq
""
Â· Â· Â· // r // Â· Â· Â· // t
For each round u, let du denote the number of rounds the gradient feedback is delayed (so u âˆˆ
Fu+du ). There are at most ds instances of the latter case, since q must lie in s+1, . . . , t. The first case
can be charged to dq . To bound the first case, observe that for fixed q, the number of indices s such
that q < s â‰¤ dq + q â‰¤ ds+ s is at most dq . That is, all instances of the second case for a fixed q can
be charged to dq . Between the two cases, we have
âˆ‘T
t=1
âˆ‘
sâˆˆFt(|Ft,s|+
âˆ‘tâˆ’1
r=s|Fr|) â‰¤ 2
âˆ‘T
t=1 dt,
and the delay term is bounded by
Tâˆ‘
t=1
âˆ‘
sâˆˆFt
âˆ‡fs|xs Â· (xs âˆ’ xt,s) â‰¤ 2Î· Â· L2
Tâˆ‘
t=1
dt.
With respect to the overall regret, this gives,
Tâˆ‘
t=1
(f(xt)âˆ’ f(y)) â‰¤
1
2Î·
+ Î· Â· L2
(
T
2
+ 2
Tâˆ‘
t=1
dt
)
= O
(
1
Î·
+ Î·L2D
)
,
as desired. 
5
Remark 2.3. The delay term
âˆ‘T
t=1
âˆ‘
sâˆˆFt âˆ‡fs|xs Â· (xs âˆ’ xt,s) is a natural point of entry for a
sharper analysis based on strong sparseness assumptions. The distance xs âˆ’ xt,s is measured by its
projection against the gradient âˆ‡fs|xs , and the preceding proof assumes the worst case and bounds
the dot product with the Cauchy-Schwartz inequality. If, for example, we assume that gradients
are pairwise orthogonal and analyze online-gradient-descent in the unconstrained setting,
then the dot productâˆ‡fs|xs Â· (xs âˆ’ xt,s) is 0 and the delay term vanishes altogether.
3 Delaying the Perturbed Leader
Discrete online linear optimization In discrete online linear optimization, the input domainK âŠ‚
Rn is a (possibly discrete) set with bounded diameter, and each cost function ft is of the form
ft(x) = ct Â· x for a bounded-length cost vector ct. The previous algorithm online-gradient-
descent does not apply here because K is not convex.
A natural algorithm for this problem is follow-the-leader. Each round t, let yt =
argminxâˆˆK xÂ·(c1+Â· Â· Â·+ct) be the optimum choice over the first t cost vectors. The algorithm pick-
ing yt in round t is called be-the-leader, and can be shown to have zero regret. Of course, be-
the-leader is infeasible since the cost vector ct is revealed after picking yt. follow-the-
leader tries the next best thing, picking ytâˆ’1 in round t. Unfortunately, this strategy can have
linear regret, largely because it is a deterministic algorithm that can be manipulated by an adversary.
Kalai and Vempala [2] gave a simple and elegant correction called follow-the-perturbed-
leader. Let  > 0 be a parameter to be fixed later, and let Q = [0, 1/]n be the cube of length
1/. Each round t, follow-the-perturbed-leader randomly picks a vector c0 âˆˆ Q by the
uniform distribution, and then selects xt = argminxâˆˆK x Â· (c0 + Â· Â· Â· + ctâˆ’1) to optimize over the
previous costs plus the random perturbation c0. With the diameter of K and the lengths â€–ctâ€– of each
cost vector held constant, Kalai and Vempala showed that follow-the-perturbed-leader
has regret O(
âˆš
T ) in expectation.
Following the delayed and perturbed leader More generally, follow-the-perturbed-
leader optimizes over all information available to the algorithm, plus some additional noise to
smoothen the worst-case analysis. If the cost vectors are delayed, we naturally interpret follow-
the-perturbed-leader to optimize over all cost vectors ct delivered in time for round t when
picking its point xt. That is, the tth leader becomes the best choice with respect to all cost vectors
delivered in the first t rounds:
ydt = argmin
xâˆˆK
tâˆ‘
s=1
âˆ‘
râˆˆFs
cr Â· x
(we use the superscript d to emphasize the delayed setting). The tth perturbed leader optimizes over
all cost vectors delivered through the first t rounds in addition to the random perturbation c0 âˆˆ Q:
yÌƒdt = argmin
xâˆˆK
(
c0 Â· x+
tâˆ‘
s=1
âˆ‘
râˆˆFs
cr Â· x
)
.
In the delayed setting, follow-the-perturbed-leader chooses xt = yÌƒdtâˆ’1 in round t. We
claim that follow-the-perturbed-leader has a direct and simple regret bound in terms of
the sum of delays D, that collapses to Kalai and Vempalaâ€™s O(
âˆš
T ) regret bound in the undelayed
setting.
Theorem 3.1. Let K âŠ† Rn be a set with L1-diameter â‰¤ 1, c1, . . . , cT âˆˆ Rn with â€–ctâ€–1 â‰¤ 1 for all
t, and Î· > 0. In the presence of adversarial delays, follow-the-perturbed-leader picks
points x1, . . . , xT âˆˆ K such that for all y âˆˆ K,
Tâˆ‘
t=1
E[ct Â· xt] â‰¤
Tâˆ‘
t=1
ct Â· y +O
(
âˆ’1 + D
)
.
For  = 1/
âˆš
D, Theorem 3.1 implies a regret bound of O(
âˆš
D). When D is not known a priori, the
doubling trick can be used to adjust  dynamically (see the discussion following Theorem 2.1).
6
To analyze follow-the-perturbed-leader in the presence of delays, we introduce the no-
tion of a prophet, who is a sort of omniscient leader who sees the feedback immediately. Formally,
the tth prophet is the best point with respect to all the cost vectors over the first t rounds:
zt = argmin
xâˆˆK
(c1 + Â· Â· Â·+ ct) Â· x.
The tth perturbed prophet is the best point with respect to all the cost vectors over the first t rounds,
in addition to a perturbation c0 âˆˆ Q:
zÌƒt = argmin
xâˆˆK
(c0 + c1 + Â· Â· Â·+ ct) Â· x. (5)
The prophets and perturbed prophets behave exactly as the leaders and perturbed leaders in the
setting of Kalai and Vempala with no delays. In particular, we can apply the regret bound of Kalai
and Vempala to the (infeasible) strategy of following the perturbed prophet.
Lemma 3.2 ([2]). Let K âŠ† Rn be a set with L1-diameter â‰¤ 1, let c1, . . . , cT âˆˆ Rn be cost vectors
bounded by â€–ctâ€–1 â‰¤ 1 for all t, and let  > 0. If zÌƒ1, . . . , zÌƒTâˆ’1 âˆˆ K are chosen per equation (5),
then
âˆ‘T
t=1 E[ct Â· zÌƒiâˆ’1] â‰¤
âˆ‘T
t=1 ct Â· y +O
(
âˆ’1 + T
)
. for all y âˆˆ K.
The analysis by Kalai and Vempala observes that when there are no delays, two consecutive per-
turbed leaders yÌƒt and yÌƒt+1 are distributed similarly over the random noise [2, Lemma 3.2]. Instead,
we will show that yÌƒdt and zÌƒt are distributed in proportion to delays. We first require a technical
lemma that is implicit in [2].
Lemma 3.3. Let K be a set with L1-diameter â‰¤ 1, and let u, v âˆˆ Rn be vectors. Let y, z âˆˆ Rn
be random vectors defined by y = argminyâˆˆK(q + u) Â· y and z = argminzâˆˆK(q + v) Â· z, where
q is chosen uniformly at random from Q =
âˆn
i=1[0, r], for some fixed length r > 0. Then, for any
vector c,
E[c Â· z]âˆ’E[c Â· y] â‰¤
â€–v âˆ’ uâ€–1â€–câ€–âˆ
r
.
Proof. LetQâ€² = v+Q andQâ€²â€² = u+Q, and write y = argminyâˆˆK q
â€²â€² Â·y and z = argminzâˆˆK qâ€² Â·z,
where qâ€² âˆˆ Qâ€² and qâ€²â€² âˆˆ Qâ€²â€² are chosen uniformly at random. Then
E[c Â· z]âˆ’E[c Â· y] = Eqâ€²â€²âˆˆQâ€²â€² [c Â· z]âˆ’Eqâ€²âˆˆQâ€² [c Â· y].
Subtracting P[qâ€² âˆˆ Qâ€² âˆ©Qâ€²â€²]Eqâ€²âˆˆQâ€²âˆ©Qâ€²â€² [c Â· z] from both terms on the right, we have
Eqâ€²â€²âˆˆQâ€²â€² [c Â· z]âˆ’Eqâ€²âˆˆQâ€² [c Â· y]
= P[qâ€²â€² âˆˆ Qâ€²â€² \Qâ€²] Â·Eqâ€²â€²âˆˆQâ€²â€²\Qâ€² [c Â· z]âˆ’P[qâ€² âˆˆ Qâ€² \Qâ€²â€²] Â·Eqâ€²âˆˆQâ€²\Qâ€²â€² [c Â· y]
By symmetry, P[qâ€²â€² âˆˆ Qâ€²â€² \Qâ€²] = P[qâ€² âˆˆ Qâ€² \Qâ€²â€²], and we have,
E[c Â· z]âˆ’E[c Â· y] â‰¤ (P[qâ€²â€² âˆˆ Qâ€²â€² \Qâ€²])Eqâ€²â€²âˆˆQâ€²â€²\Qâ€²,qâ€²âˆˆQâ€²\Qâ€²â€² [c Â· (z âˆ’ y)].
By assumption, K has L1-diameter â‰¤ 1, so â€–y âˆ’ zâ€–1 â‰¤ 1, and by HÃ¶lderâ€™s inequality, we have,
E[c Â· z]âˆ’E[c Â· y] â‰¤ P[qâ€²â€² âˆˆ Qâ€²â€² \Qâ€²]â€–câ€–âˆ.
It remains to bound P[qâ€²â€² âˆˆ Qâ€²â€² \Qâ€²] = P[qâ€² âˆˆ Qâ€² \Qâ€²â€²]. If â€–v âˆ’ uâ€–1 â‰¤ r, we have,
vol(Qâ€² âˆ©Qâ€²â€²) =
nâˆ
i=1
(r âˆ’ |vi âˆ’ ui|) = vol(Qâ€²)
nâˆ
i=1
(
1âˆ’ |(vi âˆ’ ui)|
r
)
â‰¥ vol(Qâ€²)
(
1âˆ’
â€–v âˆ’ uâ€–1
r
)
.
Otherwise, if â€–uâˆ’ vâ€–1 > r, then vol(Qâ€² âˆ©Qâ€²â€²) = 0 â‰¥ vol(Qâ€²)(1 âˆ’ â€–v âˆ’ uâ€–1/r). In either case,
we have,
P[qâ€² âˆˆ Qâ€² \Qâ€²â€²] = vol(Q
â€² âˆ©Qâ€²â€²)
vol(Qâ€²)
â‰¤ 1âˆ’ vol(Q
â€² âˆ©Qâ€²â€²)
vol(Qâ€²)
â‰¤
â€–v âˆ’ uâ€–1
r
,
and the claim follows. 
Lemma 3.3 could also have been proven geometrically in similar fashion to Kalai and Vempala.
7
Lemma 3.4.
âˆ‘T
t=1 E[ct Â· zÌƒtâˆ’1] âˆ’ E
[
ct Â· yÌƒdtâˆ’1
]
â‰¤ D, where D is the sum of delays of all cost
vectors.
Proof. Let ut =
âˆ‘t
s=1 ct be the sum of all costs through the first t rounds, and vt =
âˆ‘
s:s+dsâ‰¤t ct
be the sum of cost vectors actually delivered through the first t rounds. Then the perturbed prophet
zÌƒtâˆ’1 optimizes over c0 + utâˆ’1 and yÌƒdtâˆ’1 optimizes over c0 + vtâˆ’1. By Lemma 3.3, for each t, we
have
Ec0âˆ¼Q [ct Â· zÌƒtâˆ’1]âˆ’Ec0âˆ¼Q
[
ct Â· yÌƒdtâˆ’1
]
â‰¤  Â· â€–utâˆ’1 âˆ’ vtâˆ’1â€–1â€–ctâ€–âˆ â‰¤  Â· |{s < t : s+ ds â‰¥ t}|
Summed over all T rounds, we have,
Tâˆ‘
t=1
Ec0 [ct Â· zÌƒt]âˆ’Ec0
[
ct Â· yÌƒdt
]
â‰¤ 
Tâˆ‘
t=1
|{s < t : s+ ds â‰¥ t}|.
The sum
âˆ‘T
t=1|{s < t : s+ ds â‰¥ t}| charges each cost vector cs once for every round it is delayed,
and therefore equals D. Thus,
âˆ‘T
t=1 Ec0 [ct Â· zÌƒt]âˆ’Ec0
[
ct Â· yÌƒdt
]
â‰¤ D, as desired. 
Now we complete the proof of Theorem 3.1.
Proof of Theorem 3.1. By Lemma 3.4 and Lemma 3.2, we have,
Tâˆ‘
t=1
E
[
ct Â· yÌƒdtâˆ’1
]
â‰¤
Tâˆ‘
t=1
E[ct Â· zÌƒtâˆ’1] + D â‰¤ argmin
xâˆˆK
Tâˆ‘
t=1
E[ct Â· x] +O(âˆ’1 + D),
as desired. 
4 Conclusion
We prove O(
âˆš
D) regret bounds for online-gradient-descent and follow-the-
perturbed-leader in the delayed setting, directly extending the O(
âˆš
T ) regret bounds known
in the undelayed setting. More importantly, by deriving a simple bound as a function of the de-
lays, without any restriction on the delays, we establish a simple and intuitive model for measuring
delayed learning. This work suggests natural relationships between the regret bounds of online
learning algorithms and delays in the feedback.
Beyond analyzing existing algorithms, we hope that optimizing over the regret as a function of D
may inspire different (and hopefully simple) algorithms that readily model real world applications
and scale nicely to distributed environments.
Acknowledgements We thank Avrim Blum for introducing us to the area of online learning and
helping us with several valuable discussions. We thank the reviewers for their careful and insightful
reviews: finding errors, referencing relevant works, and suggesting a connection to mirror descent.
References
[1] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proc. 20th
Int. Conf. Mach. Learning (ICML), pages 928â€“936, 2003.
[2] A. Kalai and S. Vempala. Efficient algorithms for online decision problems. J. Comput. Sys. Sci., 71:291â€“
307, 2005. Extended abstract in Proc. 16th Ann. Conf. Comp. Learning Theory (COLT), 2003.
[3] A. Blum. On-line algorithms in machine learning. In A. Fiat and G. Woeginger, editors, Online algo-
rithms, volume 1442 of LNCS, chapter 14, pages 306â€“325. Springer Berlin Heidelberg, 1998.
[4] S. Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn.,
4(2):107â€“194, 2011.
[5] E. Hazan. Introduction to online convex optimization. Internet draft available at http://ocobook.
cs.princeton.edu, 2015.
[6] E. Takimoto and M. Warmuth. Path kernels and multiplicative updates. J. Mach. Learn. Research, 4:773â€“
818, 2003.
8
[7] D. Helmbold and R. Schapire. Predicting nearly as well as the best pruning of a decision tree. Mach.
Learn. J., 27(1):61â€“68, 1997.
[8] A. Blum, S. Chawla, and A. Kalai. Static optimality and dynamic search optimality in lists and trees.
Algorithmica, 36(3):249â€“260, 2003.
[9] T. M. Cover. Universal portfolios. Math. Finance, 1(1):1â€“29, 1991.
[10] K. Crammer and Y. Singer. A family of additive online algorithms for category ranking. J. Mach. Learn.
Research, 3:1025â€“1058, 2003.
[11] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. QuiÃ±onero
Candela. Practical lessons from predicting clicks on ads at facebook. In Proc. 20th ACM Conf. Knowl.
Disc. and Data Mining (KDD), pages 1â€“9. ACM, 2014.
[12] S. Amuru and R. M. Buehrer. Optimal jamming using delayed learning. In 2014 IEEE Military Comm.
Conf. (MILCOM), pages 1528â€“1533. IEEE, 2014.
[13] I. Menache, O. Shamir, and N. Jain. On-demand, spot, or both: Dynamic resource allocation for executing
batch jobs in the cloud. In 11th Int. Conf. on Autonomic Comput. (ICAC), 2014.
[14] M.J. Weinberger and E. Ordentlich. On delayed prediction of individual sequences. IEEE Trans. Inf.
Theory, 48(7):1959â€“1976, 2002.
[15] P. Joulani, A. GyÃ¶rgy, and C. SzepesvÃ¡ri. Online learning under delayed feedback. In Proc. 30th Int.
Conf. Mach. Learning (ICML), volume 28, 2013.
[16] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-
Hall, 1989.
[17] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: a lock-free approach to parallelizing stochastic gradient
descent. In Adv. Neural Info. Proc. Sys. 24 (NIPS), pages 693â€“701, 2011.
[18] J. Duchi, M.I. Jordan, and B. McMahan. Estimation, optimization, and parallelism when data is sparse.
In Adv. Neural Info. Proc. Sys. 26 (NIPS), pages 2832â€“2840, 2013.
[19] H.B. McMahan and M. Streeter. Delay-tolerant algorithms for asynchronous distributed online learning.
In Adv. Neural Info. Proc. Sys. 27 (NIPS), pages 2915â€“2923, 2014.
[20] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. J. Mach. Learn. Research, 12:2121â€“2159, July 2011.
[21] J. Langford, A. J. Smola, and M. Zinkevich. Slow learners are fast. In Adv. Neural Info. Proc. Sys. 22
(NIPS), pages 2331â€“2339, 2009.
[22] J. Liu, S. J. Wright, C. RÃ©, V. Bittorf, and S. Sridhar. An asynchronous parallel stochastic coordiante
descent algorithm. J. Mach. Learn. Research, 16:285â€“322, 2015.
[23] J. C. Duchi, T. Chaturapruek, and C. RÃ©. Asynchronous stochastic convex optimization. CoRR,
abs/1508.00882, 2015. To appear in Adv. Neural Info. Proc. Sys. 28 (NIPS), 2015.
[24] S. J. Wright. Coordinate descent algorithms. Math. Prog., 151(3â€“34), 2015.
[25] D. Riabko. On the flexibility of theoretical models for pattern recognition. PhD thesis, University of
London, April 2005.
[26] N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, and M.K. Warmuth. How to use
expert advice. J. Assoc. Comput. Mach., 44(3):426â€“485, 1997.
9
