


Paper ID = 5838
Title = Multi-Layer Feature Reduction for Tree Structured
Group Lasso via Hierarchical Projection
Jie Wang1, Jieping Ye1,2
1Computational Medicine and Bioinformatics
2Department of Electrical Engineering and Computer Science
University of Michigan, Ann Arbor, MI 48109
{jwangumi, jpye}@umich.edu
Abstract
Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree
structured sparsity over the features, where each node encodes a group of features.
It has been applied successfully in many real-world applications. However, with
extremely large feature dimensions, solving TGL remains a significant challenge
due to its highly complicated regularizer. In this paper, we propose a novel Multi-
Layer Feature reduction method (MLFre) to quickly identify the inactive nodes
(the groups of features with zero coefficients in the solution) hierarchically in a
top-down fashion, which are guaranteed to be irrelevant to the response. Thus, we
can remove the detected nodes from the optimization without sacrificing accura-
cy. The major challenge in developing such testing rules is due to the overlaps
between the parents and their children nodes. By a novel hierarchical projec-
tion algorithm, MLFre is able to test the nodes independently from any of their
ancestor nodes. Moreover, we can integrate MLFre—that has a low computation-
al cost—with any existing solvers. Experiments on both synthetic and real data
sets demonstrate that the speedup gained by MLFre can be orders of magnitude.
1 Introduction
Tree structured group Lasso (TGL) [13, 30] is a powerful regression technique in uncovering the
hierarchical sparse patterns among the features. The key of TGL, i.e., the tree guided regularization,
is based on a pre-defined tree structure and the group Lasso penalty [29], where each node represents
a group of features. In recent years, TGL has achieved great success in many real-world applications
such as brain image analysis [10, 18], gene data analysis [14], natural language processing [27, 28],
and face recognition [12]. Many algorithms have been proposed to improve the efficiency of TGL
[1, 6, 11, 7, 16]. However, the application of TGL to large-scale problems remains a challenge due
to its highly complicated regularizer.
As an emerging and promising technique in scaling large-scale problems, screening has received
much attention in the past few years. Screening aims to identify the zero coefficients in the sparse
solutions by simple testing rules such that the corresponding features can be removed from the
optimization. Thus, the size of the data matrix can be significantly reduced, leading to substantial
savings in computational cost and memory usage. Typical examples include TLFre [25], FLAMS
[22], EDPP [24], Sasvi [17], DOME [26], SAFE [8], and strong rules [21]. We note that strong rules
are inexact in the sense that features with nonzero coefficients may be mistakenly discarded, while
the others are exact. Another important direction of screening is to detect the non-support vectors for
support vector machine (SVM) and least absolute deviation (LAD) [23, 19]. Empirical studies have
shown that the speedup gained by screening methods can be several orders of magnitude. Moreover,
the exact screening methods improve the efficiency without sacrificing optimality.
However, to the best of our knowledge, existing screening methods are only applicable to sparse
models with simple structures such as Lasso, group Lasso, and sparse group Lasso. In this paper, we
1
propose a novel Multi-Layer Feature reduction method, called MLFre, for TGL. MLFre is exact and
it tests the nodes hierarchically from the top level to the bottom level to quickly identify the inactive
nodes (the groups of features with zero coefficients in the solution vector), which are guaranteed to
be absent from the sparse representation. To the best of our knowledge, MLFre is the first screening
method that is applicable to TGL with the highly complicated tree guided regularization.
The major technical challenges in developing MLFre for TGL lie in two folds. The first is that
most existing exact screening methods are based on evaluating the norm of the subgradients of the
sparsity-inducing regularizers with respect to the variables or groups of variables of interests. How-
ever, for TGL, we only have access to a mixture of the subgradients due to the overlaps between
parents and their children nodes. Therefore, our first major technical contribution is a novel hier-
archical projection algorithm that is able to exactly and efficiently recover the subgradients with
respect to every node from the mixture (Sections 3 and 4). The second technical challenge is that
most existing exact screening methods need to estimate an upper bound involving the dual optimum.
This turns out to be a complicated nonconvex optimization problem for TGL. Thus, our second major
technical contribution is to show that this highly nontrivial nonconvex optimization problem admits
closed form solutions (Section 5). Experiments on both synthetic and real data sets demonstrate that
the speedup gained by MLFre can be orders of magnitude (Section 6). Please see supplements for
detailed proofs of the results in the main text.
Notation: Let ‖·‖ be the `2 norm, [p] = {1, . . . , p} for a positive integer p,G ⊆ [p], and Ḡ = [p]\G.
For u ∈ Rp, let ui be its ith component. For G ⊆ [p], we denote uG = [u]G = {v : vi = ui if i ∈
G, vi = 0 otherwise} and HG = {u ∈ Rp : uḠ = 0}. If G1, G2 ⊆ [n] and G1 ⊂ G2, we
emphasize that G2 \ G1 6= ∅. For a set C, let int C, ri C, bd C, and rbd C be its interior, relative
interior, boundary, and relative boundary, respectively [5]. If C is closed and convex, the projection
operator is PC(z) := argminu∈C‖z− u‖, and its indicator function is IC(·), which is 0 on C and∞
elsewhere. Let Γ0(Rp) be the class of proper closed convex functions on Rp. For f ∈ Γ0(Rp), let
∂f be its subdifferential and dom f := {z : f(z) <∞}. We denote by γ+ = max(γ, 0).
2 Basics
We briefly review some basics of TGL. First, we introduce the so-called index tree.
Definition 1. [16] For an index tree T of depth d, we denote the node(s) of depth i by Ti =
{Gi1, . . . , Gini}, where n0 = 1, G
0
1 = [p], G
i
j ⊂ [p], and ni ≥ 1, ∀ i ∈ [d]. We assume that
(i): Gij1 ∩G
i
j2
= ∅, ∀ i ∈ [d] and j1 6= j2 (different nodes of the same depth do not overlap).
(ii): If Gij is a parent node of G
i+1
` , then G
i+1
` ⊂ Gij .
When the tree structure is available (see supplement for an example), the TGL problem is
min
β
1
2‖y −Xβ‖
2 + λ
∑d
i=0
∑ni
j=1
wij‖βGij‖, (TGL)
where y ∈ RN is the response vector, X ∈ RN×p is the data matrix, βGij and w
i
j are the coefficients
vector and positive weight corresponding to node Gij , respectively, and λ > 0 is the regularization
parameter. We derive the Lagrangian dual problem of TGL as follows.
Theorem 2. For the TGL problem, let φ(β) =
∑d
i=0
∑ni
j=1 w
i
j‖βGij‖. The following hold:
(i): Let φij(β) = ‖βGij‖ and B
i
j = {ζ ∈ HGij : ‖ζ‖ ≤ w
i
j}. We can write ∂φ(0) as
∂φ(0) =
∑d
i=0
∑ni
j=1
wij∂φ
i
j(0) =
∑d
i=0
∑ni
j=1
Bij . (1)
(ii): Let F = {θ : XT θ ∈ ∂φ(0)}. The Lagrangian dual of TGL is
sup
θ
{
1
2‖y‖
2 − 12‖
y
λ − θ‖
2 : θ ∈ F
}
. (2)
(iii): Let β∗(λ) and θ∗(λ) be the optimal solution of problems (TGL) and (2), respectively. Then,
y = Xβ∗(λ) + λθ∗(λ), (3)
XT θ∗(λ) ∈
∑d
i=0
∑ni
j=1
wij∂φ
i
j(β
∗(λ)). (4)
The dual problem of TGL in (2) is equivalent to a projection problem, i.e., θ∗(λ) = PF (y/λ). This
geometric property plays a fundamentally important role in developing MLFre (see Section 5).
2
3 Testing Dual Feasibility via Hierarchical Projection
Although the dual problem in (2) has nice geometric properties, it is challenging to determine the
feasibility of a given θ due to the complex dual feasible set F . An alternative approach is to test
if XT θ = P∂φ(0)(XT θ). Although ∂φ(0) is very complicated, we show that P∂φ(0)(·) admits a
closed form solution by hierarchically splitting P∂φ(0)(·) into a sum of projection operators with
respect to a collection of simpler sets. We first introduce some notations. For an index tree T , let
Aij =
{∑
t,k
Btk : Gtk ⊆ Gij
}
,∀ i ∈ 0 ∪ [d], j ∈ [ni], (5)
Cij =
{∑
t,k
Btk : Gtk ⊂ Gij
}
,∀ i ∈ 0 ∪ [d], j ∈ [ni]. (6)
For a node Gij , the set Aij is the sum of Btk corresponding to all its descendant nodes and itself, and
the set Cij the sum excluding itself. Therefore, by the definitions of Aij , Bij , and Cij , we have
∂φ(0) = A01, Aij = Bij + Cij ,∀ non-leaf node Gij , Aij = Bij ,∀ leaf node Gij , (7)
which implies that P∂φ(0)(·) = PA01(·) = PB01+C01 (·). This motivates the first pillar of this paper,
i.e., Lemma 3, which splits PB01+C01 (·) into the sum of two projections onto B
0
1 and C01 , respectively.
Lemma 3. Let G ⊆ [p], B = {u ∈ HG : ‖u‖ ≤ γ} with γ > 0, C ⊆ HG a nonempty closed convex
set, and z an arbitrary point inHG. Then, the following hold:
(i): [2] PB(z) = min{1, γ/‖z‖}z if z 6= 0. Otherwise, PB(z) = 0.
(ii): IB+C(z) = IB(z−PC(z)), i.e., PC(z) ∈ argminu∈C IB(z− u).
(iii): PB+C(z) = PC(z) + PB(z−PC(z)).
By part (iii) of Lemma 3, we can split PA01(X
T θ) in the following form:
PA01(X
T θ) = PC01 (X
T θ) + PB01 (X
T θ −PC01 (X
T θ)). (8)
As PB01 (·) admits a closed form solution by part (i) of Lemma 3, we can compute PA01(X
T θ) if we
have PC01 (X
T θ) computed. By Eq. (5) and Eq. (6), for a non-leaf node Gij , we note that
Cij =
∑
k∈Ic(Gij)
Ai+1k , where Ic(G
i
j) = {k : Gi+1k ⊂ G
i
j}. (9)
Inspired by (9), we have the following result.
Lemma 4. Let {G` ⊂ [p]}` be a set of nonoverlapping index sets, {C` ⊆ HG`}` be a set of
nonempty closed convex sets, and C =
∑
` C`. Then, PC(z) =
∑
` PC`(zG`) for z ∈ Rp.
Remark 1. For Lemma 4, if all C` are balls centered at 0, then PC(z) admits a closed form solution.
By Lemma 4 and Eq. (9), we can further splits PC01 (X
T θ) in Eq. (8) in the following form.
PC01 (X
T θ) =
∑
k∈Ic(G01)
PA1k([X
T θ]G1k), where Ic(G
0
1) = {k : G1k ⊂ G01}. (10)
Consider the right hand side of Eq. (10). If G1k is a leaf node, Eq. (7) implies that A1k = B1k and
thus PA1k(·) admits a closed form solution by part (i) of Lemma 3. Otherwise, we continue to split
PA1k(·) by Lemmas (3) and (4). This procedure will always terminate as we reach the leaf nodes
[see the last equality in Eq. (7)]. Therefore, by a repeated application of Lemmas (3) and (4), the
following algorithm computes the closed form solution of PA01(·).
Algorithm 1 Hierarchical Projection: PA01(·).
Input: z ∈ Rp, the index tree T as in Definition 1, and positive weights wij for all nodes Gij in T .
Output: u0 = PA01(z), v
i for ∀ i ∈ 0 ∪ [d].
1: Set ui ← 0 ∈ Rp, ∀ i ∈ 0 ∪ [d+ 1], vi ← 0 ∈ Rp, ∀ i ∈ 0 ∪ [d].
2: for i = d to 0 do /*hierarchical projection*/
3: for j = 1 to ni do
4: viGij
= PBij (zGij − u
i+1
Gij
), (11)
uiGij
← ui+1
Gij
+ viGij
. (12)
5: end for
6: end for
3
The time complexity of Algorithm 1 is similar to that of solving its proximal operator [16],
i.e., O(
∑d
i=0
∑ni
j=1 |Gij |), where |Gij | is the number of features contained in the node Gij . As∑ni
j=1 |Gij | ≤ p by Definition 1, the time complexity of Algorithm 1 is O(pd), and thus O(p log p)
for a balanced tree, where d = O(log p). The next result shows that u0 returned by Algorithm 1 is
the projection of z onto A01. Indeed, we have more general results as follows.
Theorem 5. For Algorithm 1, the following hold:
(i): ui
Gij
= PAij
(
zGij
)
, ∀ i ∈ 0 ∪ [d], j ∈ [ni].
(ii): ui+1
Gij
= PCij
(
zGij
)
, for any non-leaf node Gij .
4 MLFre Inspired by the KKT Conditions and Hierarchical Projection
In this section, we motivate MLFre via the KKT condition in Eq. (4) and the hierarchical projection
in Algorithm 1. Note that for any node Gij , we have
wij∂φ
i
j(β
∗(λ)) =
{
{ζ ∈ HGij : ‖ζ‖ ≤ w
i
j}, if [β∗(λ)]Gij = 0,
wij [β
∗(λ)]Gij/‖[β
∗(λ)]Gij‖, otherwise.
(13)
Moreover, the KKT condition in Eq. (4) implies that
∃ {ξij ∈ wij∂φij(β∗(λ)) : ∀ i ∈ 0 ∪ [d], j ∈ [ni]} such that XT θ∗(λ) =
∑d
i=0
∑ni
j=1
ξij . (14)
Thus, if ‖ξij‖ < wij , we can see that [β∗(λ)]Gij = 0. However, we do not have direct access to ξ
i
j
even if θ∗(λ) is known, because XT θ∗(λ) is a mixture (sum) of all ξij as shown in Eq. (14). Indeed,
Algorithm 1 turns out to be much more useful than testing the feasibility of a given θ: it is able to
split all ξij ∈ wij∂φij(β∗(λ)) from XT θ∗(λ). This will serve as a cornerstone in developing MLFre.
Theorem 6 rigorously shows this property of Algorithm 1.
Theorem 6. Let vi, i ∈ 0 ∪ [d] be the output of Algorithm 1 with input XT θ∗(λ), and {ξij : i ∈
0 ∪ [d], j ∈ [ni]} be the set of vectors that satisfy Eq. (14). Then, the following hold.
(i) If [β∗(λ)]Gij = 0, and [β
∗(λ)]Glr 6= 0 for all G
l
r ⊃ Gij , then
PAij
(
[XT θ∗(λ)]Gij
)
=
∑
{(k,t):Gtk⊆G
i
j}
ξtk.
(ii) If Gij is a non-leaf node, and [β
∗(λ)]Gij 6= 0, then
PCij
(
[XT θ∗(λ)]Gij
)
=
∑
{(k,t):Gtk⊂G
i
j}
ξtk.
(iii) vi
Gij
∈ wij∂φij(β∗(λ)), ∀ i ∈ 0 ∪ [d], j ∈ [ni].
Combining Eq. (13) and part (iii) of Theorem 6, we can see that
‖viGij‖ < w
i
j ⇒ [β∗(λ)]Gij = 0. (15)
By plugging Eq. (11) and part (ii) of Theorem 5 into (15), we have [β∗(λ)]Gij = 0 if
(a):
∥∥∥PBij ([XT θ∗(λ)]Gij −PCij ([XT θ∗(λ)]Gij))∥∥∥ < wij , if Gij is a non-leaf node, (R1)
(b):
∥∥∥PBij ([XT θ∗(λ)]Gij)∥∥∥ < wij , if Gij is a leaf node. (R2)
Moreover, the definition of PBij implies that we can simplify (R1) and (R2) to the following form:∥∥∥[XT θ∗(λ)]Gij −PCij ([XT θ∗(λ)]Gij)∥∥∥ < wij ⇒ [β∗(λ)]Gij = 0, if Gij is a non-leaf node, (R1’)∥∥∥[XT θ∗(λ)]Gij∥∥∥ < wij ⇒ [β∗(λ)]Gij = 0, if Gij is a leaf node. (R2’)
However, (R1’) and (R2’) are not applicable to detect inactive nodes as they involve θ∗(λ). Inspired
by SAFE [8], we first estimate a set Θ containing θ∗(λ). Let [XTΘ]Gij = {[X
T θ]Gij : θ ∈ Θ} and
Sij(z) = zGij −PCij
(
zGij
)
. (16)
4
Then, we can relax (R1’) and (R2’) as
supζ
{∥∥Sij (ζ)∥∥ : ζGij ∈ Ξij ⊇ [XTΘ]Gij} < wij ⇒ [β∗(λ)]Gij = 0, if Gij is a non-leaf node, (R1∗)
supζ
{∥∥∥ζGij∥∥∥ : ζGij ∈ [XTΘ]Gij} < wij ⇒ [β∗(λ)]Gij = 0, if Gij is a leaf node. (R2∗)
In view of (R1∗) and (R2∗), we sketch the procedure to develop MLFre in the following three steps.
Step 1 We estimate a set Θ that contains θ∗(λ).
Step 2 We solve for the supreme values in (R1∗) and (R2∗), respectively.
Step 3 We develop MLFre by plugging the supreme values obtained in Step 2 to (R1∗) and (R2∗).
4.1 The Effective Interval of the Regularization Parameter λ
The geometric property of the dual problem in (2), i.e., θ∗(λ) = PF (y/λ), implies that θ∗(λ) =
y/λ if y/λ ∈ F . Moreover, (R1) for the root node G01 leads to β∗(λ) = 0 if y/λ is an interior point
of F . Indeed, the following theorem presents stronger results.
Theorem 7. For TGL, let λmax = max {λ : y/λ ∈ F} and S01(·) be defined by Eq. (16). Then,
(i): λmax = {λ : ‖S01(XTy/λ)‖ = w01}.
(ii): yλ ∈ F ⇔ λ ≥ λmax ⇔ θ
∗(λ) = yλ ⇔ β
∗(λ) = 0.
For more discussions on λmax, please refer to Section H in the supplements.
5 The Proposed Multi-Layer Feature Reduction Method for TGL
We follow the three steps in Section 4 to develop MLFre. Specifically, we first present an accurate
estimation of the dual optimum in Section 5.1, then we solve for the supreme values in (R1∗) and
(R2∗) in Section 5.2, and finally we present the proposed MLFre in Section 5.3.
5.1 Estimation of the Dual Optimum
We estimate the dual optimum by the geometric properties of projection operators [recall that
θ∗(λ) = PF (y/λ)]. We first introduce a useful tool to characterize the projection operators.
Definition 8. [2] For a closed convex set C and a point z0 ∈ C, the normal cone to C at z0 is
NC(z0) = {ζ : 〈ζ, z− z0〉 ≤ 0, ∀ z ∈ C}.
Theorem 7 implies that θ∗(λ) is known with λ ≥ λmax. Thus, we can estimate θ∗(λ) in terms of a
known θ∗(λ0). This leads to Theorem 9 that bounds the dual optimum by a small ball.
Theorem 9. For TGL, suppose that θ∗(λ0) is known with λ0 ≤ λmax. For λ ∈ (0, λ0), we define
n(λ0) =
{
y
λ0
− θ∗(λ0), if λ0 < λmax,
XS01
(
XT yλmax
)
, if λ0 = λmax,
r(λ, λ0) =
y
λ − θ
∗(λ0),
r⊥(λ, λ0) = r(λ, λ0)− 〈r(λ,λ0),n(λ0)〉‖n(λ0)‖2 n(λ0).
Then, the following hold:
(i): n(λ0) ∈ NF (θ∗(λ0)).
(ii): ‖θ∗(λ)− (θ∗(λ0) + 12r
⊥(λ, λ0))‖ ≤ 12‖r
⊥(λ, λ0)‖.
Theorem 9 indicates that θ∗(λ) lies inside the ball of radius 12‖r
⊥(λ, λ0)‖ centered at
o(λ, λ0) = θ
∗(λ0) +
1
2r
⊥(λ, λ0).
5.2 Solving the Nonconvex Optimization Problems in (R1∗) and (R2∗)
We solve for the supreme values in (R1∗) and (R2∗). For notational convenience, let
Θ = {θ : ‖θ − o(λ, λ0)‖ ≤ 12‖r
⊥(λ, λ0)‖}, (17)
Ξij = {ζ : ζ ∈ HGij , ‖ζ − [X
To(λ, λ0)]Gij‖ ≤
1
2‖r
⊥(λ, λ0)‖‖XGij‖2}. (18)
Theorem 9 implies that θ∗(λ) ∈ Θ, and thus [XTΘ]Gij ⊆ Ξ
i
j for all non-leaf nodes G
i
j . To develop
MLFre by (R1∗) and (R2∗), we need to solve the following optimization problems:
sij(λ, λ0) = supζ{‖Sij(ζ)‖ : ζ ∈ Ξij}, if Gij is a non-leaf node, (19)
sij(λ, λ0) = supζ{‖ζ‖ : ζ ∈ Ξij}, if Gij is a leaf node. (20)
5
Before we solve problems (19) and (20), we first introduce some notations.
Definition 10. For a non-leaf node Gij of an index tree T , let Ic(Gij) = {k : G
i+1
k ⊂ Gij}. If
Gij \ ∪k∈Ic(Gij)G
i+1
k 6= ∅, we define a virtual child node of Gij by G
i+1
j′ = G
i
j \ ∪k∈Ic(Gij)G
i+1
k for
j′ ∈ {ni+1 + 1, ni+1 + 2, . . . , ni+1 + n′i+1}, where n′i+1 is the number of virtual nodes of depth
i+ 1. We set the weights wij′ = 0 for all virtual nodes G
i
j′ .
Another useful concept is the so-called unique path between the nodes in the tree.
Lemma 11. [16] For any non-root node Gij , we can find a unique path from Gij to the root G01. Let
the nodes on this path be Glrl , where l ∈ 0 ∪ [i], r0 = 1, and ri = j. Then, the following hold:
Gij ⊂ Glrl , ∀ l ∈ 0 ∪ [i− 1]. (21)
Gij ∩Glr = ∅, ∀ r 6= rl, l ∈ [i− 1], r ∈ [ni]. (22)
Solving Problem (19) We consider the following equivalent problem of (19).
1
2 (s
i
j(λ, λ0))
2 = supζ{ 12‖S
i
j(ζ)‖2 : ζ ∈ Ξij}, if Gij is a non-leaf node. (23)
Although both the objective function and feasible set of problem (23) are convex, it is nonconvex as
we need to find the supreme value. We derive the closed form solutions of (19) and (23) as follows.
Theorem 12. Let c = [XTo(λ, λ0)]Gij , γ =
1
2‖r
⊥(λ, λ0)‖‖XGij‖2, and v
i, i ∈ 0 ∪ [d] be the
output of Algorithm 1 with input XTo(λ, λ0).
(i): Suppose that c /∈ Cij . Then, sij(λ, λ0) = ‖viGij‖+ γ.
(ii): Suppose that node Gij has a virtual child node. Then, for any c ∈ Cij , sij(λ, λ0) = γ.
(iii): Suppose that node Gij has no virtual child node. Then, the following hold.
(iii.a): If c ∈ rbd Cij , then sij(λ, λ0) = γ.
(iii.b): If c ∈ ri Cij , then, for any node Gtk ⊂ Gij , where t ∈ {i+ 1, . . . , d} and k ∈ [nt + n′t], let
the nodes on the path from Gtk to G
i
j be G
l
rl
, where l = i, . . . , t, ri = j, and rt = k, and
Γ(Gi+1ri+1 , G
t
k) =
∑t
l=i+1
(
wlrl − ‖v
l
Glrl
‖
)
. (24)
Then, sij(λ, λ0) =
(
γ −min{(k,t):Gtk⊂Gij} Γ(G
i+1
ri+1 , G
t
k)
)
+
.
Solving Problem (20) We can solve problem (20) by the Cauchy-Schwarz inequality.
Theorem 13. For problem (20), we have sij(λ, λ0) = ‖[XTo(λ, λ0)]Gij‖+
1
2‖r
⊥(λ, λ0)‖‖XGij‖2.
5.3 The Multi-Layer Screening Rule
In real-world applications, the optimal parameter values are usually unknown. Commonly used
approaches to determine an appropriate parameter value, such as cross validation and stability se-
lection, solve TGL many times along a grid of parameter values. This process can be very time
consuming. Motivated by this challenge, we present MLFre in the following theorem by plugging
the supreme values found by Theorems 12 and 13 into (R1∗) and (R2∗), respectively.
Theorem 14. For the TGL problem, suppose that we are given a sequence of parameter values
λmax = λ0 > λ1 > · · · > λK. For each integer k = 0, . . . ,K− 1, we compute θ∗(λk) from a given
β∗(λk) via Eq. (3). Then, for i = 1, . . . , d, MLFre takes the form of
sij(λk+1, λk) < w
i
j ⇒ [β∗(λ)]Gij = 0, ∀ j ∈ [ni]. (MLFre)
Remark 2. We apply MLFre to identify inactive nodes hierarchically in a top-down fashion. Note
that, we do not need to apply MLFre to node Gij if one of its ancestor nodes passes the rule.
Remark 3. To simplify notations, we consider TGL with a single tree, in the proof. However, all
major results are directly applicable to TGL with multiple trees, as they are independent from each
other. We note that, many sparse models, such as Lasso, group Lasso, and sparse group Lasso, are
special cases of TGL with multiple trees.
6
(a) synthetic 1, p = 20000 (b) synthetic 1, p = 50000 (c) synthetic 1, p = 100000
(d) synthetic 2, p = 20000 (e) synthetic 2, p = 50000 (f) synthetic 2, p = 100000
Figure 1: Rejection ratios of MLFre on two synthetic data sets with different feature dimensions.
6 Experiments
We evaluate MLFre on both synthetic and real data sets by two measurements. The first measure is
the rejection ratios of MLFre for each level of the tree. Let p0 be the number of zero coefficients in
the solution vector and Gi be the index set of the inactive nodes with depth i identified by MLFre.
The rejection ratio of the ith layer of MLFre is defined by ri =
∑
k∈Gi |G
i
k|
p0
, where |Gik| is the
number of features contained in node Gik. The second measure is speedup, namely, the ratio of the
running time of the solver without screening to the running time of solver with MLFre.
For each data set, we run the solver combined with MLFre along a sequence of 100 parameter values
equally spaced on the logarithmic scale of λ/λmax from 1.0 to 0.05. The solver for TGL is from the
SLEP package [15]. It also provides an efficient routine to compute λmax.
6.1 Simulation Studies
Table 1: Running time (in seconds) for solving
TGL along a sequence of 100 tuning parame-
ter values of λ equally spaced on the logarithmic
scale of λ/λmax from 1.0 to 0.05 by (a): the solver
[15] without screening (see the third column); (b):
the solver with MLFre (see the fifth column).
Dataset p solver MLFre MLFre+solver speedup
synthetic 1
20000 483.96 1.03 30.17 16.04
50000 1175.91 2.95 39.49 29.78
100000 2391.43 6.57 58.91 40.60
synthetic 2
20000 470.54 1.19 37.87 12.43
50000 1122.30 3.13 43.97 25.53
100000 2244.06 6.18 60.96 36.81
ADNI+GMV 406262 20911.92 81.14 492.08 42.50
ADNI+WMV 406262 21855.03 80.83 556.19 39.29
ADNI+WBV 406262 20812.06 82.10 564.36 36.88
We perform experiments on two synthetic data
sets, named synthetic 1 and synthetic 2, which
are commonly used in the literature [21, 31].
The true model is y = Xβ∗ + 0.01,  ∼
N(0, 1). For each of the data set, we fix N =
250 and select p = 20000, 50000, 100000. We
create a tree with height 4, i.e., d = 3. The
average sizes of the nodes with depth 1, 2 and
3 are 50, 10, and 1, respectively. Thus, if
p = 100000, we have roughly n1 = 2000,
n2 = 10000, and n3 = 100000. For synthet-
ic 1, the entries of the data matrix X are i.i.d.
standard Gaussian with zero pair-wise correla-
tion, i.e., corr (xi,xj) = 0 for the ith and jth
columns of X with i 6= j. For synthetic 2,
the entries of X are drawn from standard Gaus-
sian with pair-wise correlation corr (xi,xj) =
0.5|i−j|. To construct β∗, we first randomly select 50% of the nodes with depth 1, and then ran-
domly select 20% of the children nodes (with depth 2) of the remaining nodes with depth 1. The
components of β∗ corresponding to the remaining nodes are populated from a standard Gaussian,
and the remaining ones are set to zero.
7
(a) ADNI+GMV (b) ADNI+WMV (c) ADNI+WBV
Figure 2: Rejection ratios of MLFre on ADNI data set with grey matter volume (GMV), white mater
volume (WMV), and whole brain volume (WBV) as response vectors, respectively.
Fig. 1 shows the rejection ratios of all three layers of MLFre. We can see that MLFre identifies
almost all of the inactive nodes, i.e.,
∑3
i=1 ri ≥ 90%, and the first layer contributes the most.
Moreover, Fig. 1 also indicates that, as the feature dimension (and the number of nodes in each level)
increases, MLFre identifies more inactive nodes, i.e.,
∑3
i=1 ri ≈ 100%. Thus, we can expect a more
significant capability of MLFre in identifying inactive nodes on data sets with higher dimensions.
Table 1 shows the running time of the solver with and without MLFre. We can observe significant
speedups gained by MLFre, which are up to 40 times. Take synthetic 1 with p = 100000 for
example. The solver without MLFre takes about 40 minutes to solve TGL at 100 parameter values.
Combined with MLFre, the solver only needs less than one minute for the same task. Table 1 also
shows that the computational cost of MLFre is very low—that is negligible compared to that of the
solver without MLFre. Moreover, as MLFre identifies more inactive nodes with increasing feature
dimensions, Table 1 shows that the speedup gained by MLFre becomes more significant as well.
6.2 Experiments on ADNI data set
We perform experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI) data set
(http://adni.loni.usc.edu/). The data set consists of 747 patients with 406262 single
nucleotide polymorphisms (SNPs). We create the index tree such that n1 = 4567, n2 = 89332,
and n3 = 406262. Fig. 2 presents the rejection ratios of MLFre on the ADNI data set with grey
matter volume (GMV), white matter volume (WMV), and whole brain volume (WBV) as response,
respectively. We can see that MLFre identifies almost all inactive nodes, i.e.,
∑3
i=1 ri ≈ 100%. As
a result, we observe significant speedups gained by MLFre—that are about 40 times—from Table
1. Specifically, with GMV as response, the solver without MLFre takes about six hours to solve
TGL at 100 parameter values. However, combined with MLFre, the solver only needs about eight
minutes for the same task. Moreover, Table 1 also indicates that the computational cost of MLFre is
very low—that is negligible compared to that of the solver without MLFre.
7 Conclusion
In this paper, we propose a novel multi-layer feature reduction (MLFre) method for TGL. Our major
technical contributions lie in two folds. The first is the novel hierarchical projection algorithm that
is able to exactly and efficiently recover the subgradients of the tree-guided regularizer with respect
to each node from their mixture. The second is that we show a highly nontrivial nonconvex problem
admits a closed form solution. To the best of our knowledge, MLFre is the first screening method
that is applicable to TGL. An appealing feature of MLFre is that it is exact in the sense that the
identified inactive nodes are guaranteed to be absent from the sparse representations. Experiments
on both synthetic and real data sets demonstrate that MLFre is very effective in identifying inactive
nodes, leading to substantial savings in computational cost and memory usage without sacrificing
accuracy. Moreover, the capability of MLFre in identifying inactive nodes on higher dimensional
data sets is more significant. We plan to generalize MLFre to more general and complicated sparse
models, e.g., over-lapping group Lasso with logistic loss. In addition, we plan to apply MLFre to
other applications, e.g., brain image analysis [10, 18] and natural language processing [27, 28].
Acknowledgments
This work is supported in part by research grants from NIH (R01 LM010730, U54 EB020403) and
NSF (IIS- 0953662, III-1539991, III-1539722).
8
References
[1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foun-
dations and Trends in Machine Learning, 4(1):1–106, Jan. 2012.
[2] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.
Springer, 2011.
[3] M. Bazaraa, H. Sherali, and C. Shetty. Nonlinear Programming: Theory and Algorithms. Wiley-
Interscience, 2006.
[4] J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization, Second Edition. Canadian
Mathematical Society, 2006.
[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[6] X. Chen, Q. Lin, S. Kim, J. Carbonell, and E. Xing. Smoothing proximal gradient method for general
structured sparse regression. Annals of Applied Statistics, pages 719–752, 2012.
[7] W. Deng, W. Yin, and Y. Zhang. Group sparse optimization by alternating direction method. Technical
report, Rice CAAM Report TR11-06, 2011.
[8] L. El Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. Pacific
Journal of Optimization, 8:667–698, 2012.
[9] J.-B. Hiriart-Urruty. From convex optimization to nonconvex optimization. necessary and sufficient con-
ditions for global optimality. In Nonsmooth optimization and related topics. Springer, 1988.
[10] R. Jenatton, A. Gramfort, V. Michel, G. Obozinski, E. Eger, F. Bach, and B. Thirion. Multiscale mining of
fmri data with hierarchical structured sparsity. SIAM Journal on Imaging Science, pages 835–856, 2012.
[11] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for hierarchical sparse coding.
Journal of Machine Learning Research, 12:2297–2334, 2011.
[12] K. Jia, T. Chan, and Y. Ma. Robust and practical face recognition via structured sparsity. In European
Conference on Computer Vision, 2012.
[13] S. Kim and E. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In
International Conference on Machine Learning, 2010.
[14] S. Kim and E. Xing. Tree-guided group lasso for multi-response regression with structured sparsity, with
an application to eqtl mapping. The Annals of Applied Statistics, 2012.
[15] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with Efficient Projections. Arizona State University, 2009.
[16] J. Liu and J. Ye. Moreau-Yosida regularization for grouped tree structure learning. In Advances in neural
information processing systems, 2010.
[17] J. Liu, Z. Zhao, J. Wang, and J. Ye. Safe screening with variational inequalities and its application to
lasso. In International Conference on Machine Learning, 2014.
[18] M. Liu, D. Zhang, P. Yap, and D. Shen. Tree-guided sparse coding for brain disease classification. In
Medical Image Computing and Computer-Assisted Intervention, 2012.
[19] K. Ogawa, Y. Suzuki, and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computa-
tion. In ICML, 2013.
[20] A. Ruszczyński. Nonlinear Optimization. Princeton University Press, 2006.
[21] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. Tibshirani. Strong rules for
discarding predictors in lasso-type problems. Journal of the Royal Statistical Society Series B, 74:245–
266, 2012.
[22] J. Wang, W. Fan, and J. Ye. Fused lasso screening rules via the monotonicity of subdifferentials. IEEE
Transactions on Pattern Analysis and Machine Intelligence, PP(99):1–1, 2015.
[23] J. Wang, P. Wonka, and J. Ye. Scaling svm and least absolute deviations via exact data reduction. In
International Conference on Machine Learning, 2014.
[24] J. Wang, P. Wonka, and J. Ye. Lasso screening rules via dual polytope projection. Journal of Machine
Learning Research, 16:1063–1101, 2015.
[25] J. Wang and J. Ye. Two-Layer feature reduction for sparse-group lasso via decomposition of convex sets.
Advances in neural information processing systems, 2014.
[26] Z. J. Xiang, H. Xu, and P. J. Ramadge. Learning sparse representation of high dimensional data on large
scale dictionaries. In NIPS, 2011.
[27] D. Yogatama, M. Faruqui, C. Dyer, and N. Smith. Learning word representations with hierarchical sparse
coding. In International Conference on Machine Learning, 2015.
[28] D. Yogatama and N. Smith. Linguistic structured sparsity in text categorization. In Proceedings of the
Annual Meeting of the Association for Computational Linguistics, 2014.
[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the
Royal Statistical Society Series B, 68:49–67, 2006.
[30] P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical
variable selection. Annals of Statistics, 2009.
[31] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal
Statistical Society Series B, 67:301–320, 2005.
9
