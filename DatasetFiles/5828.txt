


Paper ID = 5828
Title = Private Graphon Estimation for Sparse Graphsâˆ—
Christian Borgs Jennifer T. Chayes
Microsoft Research New England
Cambridge, MA, USA.
{cborgs,jchayes}@microsoft.com
Adam Smith
Pennsylvania State University
University Park, PA, USA.
asmith@psu.edu
Abstract
We design algorithms for fitting a high-dimensional statistical model to a large,
sparse network without revealing sensitive information of individual members.
Given a sparse input graph G, our algorithms output a node-differentially private
nonparametric block model approximation. By node-differentially private, we
mean that our output hides the insertion or removal of a vertex and all its adjacent
edges. IfG is an instance of the network obtained from a generative nonparametric
model defined in terms of a graphon W , our model guarantees consistency: as the
number of vertices tends to infinity, the output of our algorithm converges toW in
an appropriate version of the L2 norm. In particular, this means we can estimate
the sizes of all multi-way cuts in G.
Our results hold as long as W is bounded, the average degree of G grows at least
like the log of the number of vertices, and the number of blocks goes to infinity
at an appropriate rate. We give explicit error bounds in terms of the parameters of
the model; in several settings, our bounds improve on or match known nonprivate
results.
1 Introduction
Differential Privacy. Social and communication networks have been the subject of intense study
over the last few years. However, while these networks comprise a rich source of information for
science, they also contain highly sensitive private information. What kinds of information can we
release about these networks while preserving the privacy of their users? Simple measures, such as
removing obvious identifiers, do not work; for example, several studies reidentified individuals in
the graph of a social network even after all vertex and edge attributes were removed. Such attacks
highlight the need for statistical and learning algorithms that provide rigorous privacy guarantees.
Differential privacy [17] provides meaningful guarantees in the presence of arbitrary side informa-
tion. In the context of traditional statistical data sets, differential privacy is now well-developed.
By contrast, differential privacy in the context of graph data is much less developed. There are two
main variants of graph differential privacy: edge and node differential privacy. Intuitively, edge
differential privacy ensures that an algorithmâ€™s output does not reveal the inclusion or removal of a
particular edge in the graph, while node differential privacy hides the inclusion or removal of a node
together with all its adjacent edges. Edge privacy is a weaker notion (hence easier to achieve) and
has been studied more extensively. Several authors designed edge-differentially private algorithms
for fitting generative graph models (e.g. [24]; see the full version for further references), but these
do not appear to generalize to node privacy with meaningful accuracy guarantees.
The stronger notion, node privacy, corresponds more closely to what was achieved in the case of
traditional data sets, and to what one would want to protect an individualâ€™s data: it ensures that no
matter what an analyst observing the released information knows ahead of time, she learns the same
âˆ—A full version of this extended abstract is available at http://arxiv.org/abs/1506.06162
1
things about an individual Alice regardless of whether Aliceâ€™s data are used or not. In particular,
no assumptions are needed on the way the individualsâ€™ data are generated (they need not even be
independent). Node privacy was studied more recently [21, 14, 6, 26], with a focus on on the release
of descriptive statistics (such as the number of triangles in a graph). Unfortunately, differential
privacyâ€™s stringency makes the design of accurate, private algorithms challenging.
In this work, we provide the first algorithms for node-private inference of a high-dimensional statis-
tical model that does not admit simple sufficient statistics.
Modeling Large Graphs via Graphons. Traditionally, large graphs have been modeled using
various parametric models, one of the most popular being the stochastic block model [20]. Here one
postulates that an observed graph was generated by first assigning vertices at random to one of k
groups, and then connecting two vertices with a probability that depends on their assigned groups.
As the number of vertices of the graph in question grows, we do not expect the graph to be well
described by a stochastic block model with a fixed number of blocks. In this paper we consider
nonparametric models (where the number of parameters need not be fixed or even finite) given in
terms of a graphon. A graphon is a measurable, bounded function W : [0, 1]2 â†’ [0,âˆ) such that
W (x, y) = W (y, x), which for convenience we take to be normalized:
âˆ«
W = 1. Given a graphon,
we generate a graph on n vertices by first assigning i.i.d. uniform labels in [0, 1] to the vertices,
and then connecting vertices with labels x, y with probability ÏnW (x, y), where Ïn is a parameter
determining the density of the generated graph Gn with Ïnâ€–Wâ€–âˆ â‰¤ 1. We call Gn a W -random
graph with target density Ïn (or simply a ÏnW -random graph).
To our knowledge, random graph models of the above form were first introduced under the name la-
tent position graphs [19], and are special cases of a more general model of â€œinhomogeneous random
graphsâ€ defined in [7], which is the first place were n-dependent target densities Ïn were considered.
For both dense graphs (whose target density does not depend on the number of vertices) and sparse
graphs (those for which Ïn â†’ 0 as nâ†’âˆ), this model is related to the theory of convergent graph
sequences, [8, 23, 9, 10] and [11, 12], respectively.
Estimation and Identifiability. Assuming that Gn is generated in this way, we are then faced with
the task of estimating W from a single observation of a graph Gn. To our knowledge, this task
was first explicitly considered in [4], which considered graphons describing stochastic block models
with a fixed number of blocks. This was generalized to models with a growing number of blocks
[27, 15], while the first estimation of the nonparametric model was proposed in [5]. Most of the
literature on estimating the nonparametric model makes additional assumptions on the function W ,
the most common one being that after a measure-preserving transformation, the integral of W over
one variable is a strictly monotone function of the other, corresponding to an asymptotically strictly
monotone degree distribution of Gn. (This assumption is quite restrictive: in particular, such results
do not apply to graphons that represent block models.) For our purposes, the most relevant works
are Wolfe and Olhede [28], Gao et al. [18], Chatterjee [13] and Abbe and Sandon [2] (as well as
recent work done concurrently with this research [22]), which provide consistent estimators without
monotonicity assumptions (see â€œComparison to nonprivate boundsâ€, below).
One issue that makes estimation of graphons challenging is identifiability: multiple graphons can
lead to the same distribution on Gn. Specifically, two graphons W and WÌƒ lead to the same distri-
bution on W -random graphs if and only if there are measure preserving maps Ï†, Ï†Ìƒ : [0, 1] â†’ [0, 1]
such that WÏ† = WÌƒ Ï†Ìƒ, where WÏ† is defined by W (x, y) = W (Ï†(x), Ï†(y)) [16]. Hence, there is
no â€œcanonical graphonâ€ that an estimation procedure can output, but rather an equivalence class of
graphons. Some of the literature circumvents identifiability by making strong additional assump-
tions, such as strict monotonicity, that imply the existence of canonical equivalent class representa-
tives. We make no such assumptions, but instead define consistency in terms of a metric on these
equivalence classes, rather than on graphons as functions. We use a variant of the L2 metric,
Î´2(W,W
â€²) = inf
Ï†:[0,1]â†’[0,1]
â€–WÏ† âˆ’W â€²â€–2, where Ï† ranges over measure-preserving bijections. (1)
Our Contributions. In this paper we construct an algorithm that produces an estimate WÌ‚ from a
single instance Gn of a W -random graph with target density Ïn (or simply Ï, when n is clear from
the context). We aim for several properties:
2
1. WÌ‚ is differentially private;
2. WÌ‚ is consistent, in the sense that Î´2(W, WÌ‚ )â†’ 0 in probability as nâ†’âˆ;
3. WÌ‚ has a compact representation (in our case, as a matrix with o(n) entries);
4. The procedure works for sparse graphs, that is, when the density Ï is small;
5. On input Gn, WÌ‚ can be calculated efficiently.
Here we give an estimation procedure that obeys the first four properties, leaving the question of
polynomial-time algorithms for future work. Given an input graph Gn, a privacy-parameter  and a
target number k of blocks, our algorithm A produces a k-block graphon WÌ‚ = A(Gn) such that
â€¢ A is -differentially node private. The privacy guarantee holds for all inputs, independent
of modeling assumptions.
â€¢ If (1) W is an arbitrary graphon, normalized so
âˆ«
W = 1, (2) the expected average degree
(nâˆ’1)Ï grows at least as fast as log n, and (3) k goes to infinity sufficiently slowly with n,
then, whenGn is ÏW -random, the estimate WÌ‚ forW is consistent (that is, Î´2(WÌ‚ ,W )â†’ 0,
both in probability and almost surely).
â€¢ We give a nonprivate variant of A that converges assuming only Ï‰(1) average degree.
Combined with the general theory of convergent graphs sequences, these results in particular give
a node-private procedure for estimating the edge density of all cuts in a ÏW -random graph,see
Section 2.2 below.
The main idea of our algorithm is to use the exponential mechanism of [25] to select a block model
which approximately minimizes the `2 distance to the observed adjacency matrix of G, under the
best possible assignment of nodes to blocks (this explicit search over assignments makes the algo-
rithm take exponential time). In order to get an algorithm that is accurate on sparse graphs, we need
several nontrivial extensions of current techniques. To achieve privacy, we use a new variation of the
Lipschitz extension technique of [21, 14] to reduce the sensitivity of the Î´2 distance. While those
works used Lipschitz extensions for noise addition, we use of Lipshitz extensions inside the â€œexpo-
nential mechanismâ€ [25] (to control the sensitivity of the score functions). To bound our algorithmâ€™s
error, we provide a new analysis of the `2-minimization algorithm; we show that approximate min-
imizers are not too far from the actual minimizer (a â€œstabilityâ€ property). Both aspects of our work
are enabled by restricting the `22-minimization to a set of block models whose density (in fact, Lâˆ
norm) is not much larger than that of the underlying graph. The algorithm is presented in Section 3.
Our most general result proves consistency for arbitrary graphons W but does not provides a con-
crete rate of convergence. However, we provide explicit rates under various assumptions on W .
Specifically, we relate the error of our estimator to two natural error terms involving the graphonW :
the error (O)k (W ) of the best k-block approximation to W in the L2 norm (see (4) below) and an
error term n(W ) measuring the L2-distance between the graphonW and the matrix of probabilities
Hn(W ) generating the graph Gn (see (5) below.) In terms of these error terms, Theorem 1 shows
Î´2
(
W, WÌ‚
)
â‰¤ (O)k (W ) + 2n(W ) +OP
(
4
âˆš
log k
Ïn
+
âˆš
k2 log n
n
+
1
Ïn
)
. (2)
provided the average degree Ïn grows at least like log n. Along the way, we provide a novel analysis
of a straightforward, nonprivate least-squares estimator that does not require an assumption on the
average degree, and leads to an error bound with a better dependence on k:
Î´2
(
W, WÌ‚nonprivate
)
â‰¤ (O)k (W ) + 2n(W ) +OP
(
4
âˆš
log k
Ïn
+
k2
Ïn2
)
. (3)
It follows from the theory of graph convergence that for all graphons W , we have (O)k (W )â†’ 0 as
k â†’ âˆ and n(W ) â†’ 0 almost surely as n â†’ âˆ. By selecting k appropriately, the nonprivate
algorithm converges for any bounded graphon as long as Ïn â†’ âˆ with n; the private algorithm
converges whenever Ïn â‰¥ 6 log n (e.g., for constant ). As proven in the full version, we also have
n(W ) = OP (
(O)
k (W ) +
4
âˆš
k/n), though this upper bound is loose in many cases.
3
As a specific instantiation of these bounds, let us consider the case that W is exactly described
by a k-block model, in which case (O)k (W ) = 0 and n(W ) = OP (
4
âˆš
k/n) (see full version
for a proof). For k â‰¤ (n/ log2 n)1/3, Ï â‰¥ log(k)/k and constant , our private estimator has an
asymptotic error that is dominated by the (unavoidable) error of n(W ) = 4
âˆš
k/n, showing that we
do not lose anything due to privacy in this special case. Another special case is when W is Î±-HoÌˆlder
continuous, in which case (O)k (W ) = O(k
âˆ’Î±) and n(W ) = OP (nâˆ’Î±/2); see Remark 2 below.
Comparison to Previous Nonprivate Bounds. We provide the first consistency bounds for estima-
tion of a nonparametric graph model subject to node differential privacy. Along the way, for sparse
graphs, we provide more general consistency results than were previously known, regardless of pri-
vacy. In particular, to the best of our knowledge, no prior results give a consistent estimator for W
that works for sparse graphs without any additional assumptions besides boundedness.
When compared to results for nonprivate algorithms applied to graphons obeying additional assump-
tions, our bounds are often incomparable, and in other cases match the existing bounds.
We start by considering graphons which are themselves step functions with a known number of steps
k. In the dense case, the nonprivate algorithms of [18] and [13], as well as our nonprivate algorithm,
give an asymptotic error that is dominated by the term n(W ) = O( 4
âˆš
k/n), which is of the same
order as our private estimator as long as k = oÌƒ(n1/3). [28] provided the first convergence results
for estimating graphons in the sparse regime. Assuming that W is bounded above and below (so it
takes values in a range [Î»1, Î»2] where Î»1 > 0), they analyze an inefficient algorithm (the MLE). The
bounds of [28] are incomparable to ours, though for the case of k-block graphons, both their bounds
and our nonprivate bound are dominated by the term 4
âˆš
k/n when Ï > (log k)/k and k â‰¤ Ïn. A
different sequence of works shows how to consistently estimate the underlying block model with a
fixed number of blocks k in polynomial time for very sparse graphs (as for our non-private algorithm,
the only thing which is needed is that nÏ â†’ âˆ) [3, 1, 2]; we are not aware of concrete bounds on
the convergence rate.
For the case of dense Î±-HoÌˆlder-continuous graphons, the results of [18] give an error which is
dominated by the term n(W ) = OP (nâˆ’Î±/2). For Î± < 1/2, our nonprivate bound matches this
bound, while for Î± > 1/2 it is worse. [28] considers the sparse case. The rate of their estimator is
incomparable to that of ours; further, their analysis requires a lower bound on the edge probabilities,
while ours does not. Very recently, after our paper was submitted, both the bounds of [28] as well as
our non-private bound (3) were substantially improved [22], leading to an error bound where the 4th
root in (3) is replaced by a square root (at the cost of an extra constant multiplying the oracle error.)
See the full version for a more detailed discussion of the previous literature.
2 Preliminaries
2.1 Notation
For a graphG on [n] = {1, . . . , n}, we useE(G) andA(G) to denote the edge set and the adjacency
matrix of G, respectively. The edge density Ï(G) is defined as the number of edges divided by
(
n
2
)
.
Finally the degree di of a vertex i inG is the number of edges containing i. We use the same notation
for a weighted graph with nonnegative edge weights Î²ij , where now Ï(G) = 2n(nâˆ’1)
âˆ‘
i<j Î²ij , and
di =
âˆ‘
j 6=i Î²ij . We use Gn to denote the set of weighted graphs on n vertices with weights in [0, 1],
and Gn,d to denote the set of all graphs in Gn that have maximal degree at most d.
From Matrices to Graphons. We define a graphon to be a bounded, measurable function W :
[0, 1]2 â†’ R+ such that W (x, y) = W (y, x) for all x, y âˆˆ [0, 1]. It will be convenient to embed
the set of a symmetric n Ã— n matrix with nonnegative entries into graphons as follows: let Pn =
(I1, . . . In) be the partition of [0, 1] into adjacent intervals of lengths 1/n. Define W [A] to be the
step function which equals Aij on Ii Ã— Ij . If A is the adjacency matrix of an unweighted graph G,
we use W [G] for W [A].
Distances. For p âˆˆ [1,âˆ) we define the Lp norm of an n Ã— n matrix A and a (Borel)-measurable
function W : [0, 1]2 â†’ R by â€–Aâ€–p =
(
1
n2
âˆ‘
i,j |Aij |p
)1/p
, and â€–fâ€–p =
(âˆ«
|f(x, y)|pdxdy
)1/p
,
4
respectively. Associated with the L2-norm is a scalar product, defined as ã€ˆA,Bã€‰ = 1n2
âˆ‘
i,j AijBij
for two n Ã— n matrices A and B, and ã€ˆU,W ã€‰ =
âˆ«
U(x, y)W (x, y)dxdy for two square integrable
functions U,W : [0, 1]2 â†’ R. Note that with this notation, the edge density and the L1 norm are
related by â€–Gâ€–1 = nâˆ’1n Ï(G).
Recalling (1), we define the Î´2 distance between two matrices A,B, or between a matrix A and a
graphon W by Î´2(A,B) = Î´2(W [A],W [B]) and Î´2(A,W ) = Î´2(W [A],W ). In addition, we will
also use the in general larger distances Î´Ì‚2(A,B) and Î´Ì‚2(A,W ), defined by taking a minimum over
matricesAâ€² which are obtained fromA by a relabelling of the indices: Î´Ì‚2(A,B) = minAâ€² â€–Aâ€²âˆ’Bâ€–2
and Î´Ì‚2(A,W ) = minAâ€² â€–W [Aâ€²]âˆ’Wâ€–2.
2.2 W -random graphs, graph convergence and multi-way cuts
W-random graphs and stochastic block models. Given a graphon W we define a random n Ã— n
matrix Hn = Hn(W ) by choosing n â€œpositionsâ€ x1, . . . , xn i.i.d. uniformly at random from [0, 1]
and then setting (Hn)ij = W (xi, xj). If â€–Wâ€–âˆ â‰¤ 1, then Hn(W ) has entries in [0, 1], and we can
form a random graph Gn = Gn(W ) on n-vertices by choosing an edge between two vertices i < j
with probability (Hn)ij , independently for all i < j. Following [23] we call Gn(W ) a W -random
graph and Hn(W ) a W -weighted random graph. We incorporate a target density Ïn (or simply Ï,
when n is clear from the context) by normalizing W so that
âˆ«
W = 1 and taking G to be a sample
from Gn(ÏW ). In other words, we set Q = Hn(ÏW ) = ÏHn(W ) and then connect i to j with
probability Qij , independently for all i < j.
Stochastic block models are specific examples of W -random graph in which W is constant on sets
of the form Ii Ã— Ij , where (I1, . . . , Ik) is a partition of [0, 1] into intervals of possibly different
lengths.
On the other hand, an arbitrary graphon W can be well approximated by a block model. Indeed, let

(O)
k (W ) = minB
â€–W âˆ’W [B]â€–2 (4)
where the minimum runs over all k Ã— k matrices B. By a straightforward argument (see, e.g., [11])

(O)
k (W ) = â€–W âˆ’WPkâ€–2 â†’ 0 as k â†’âˆ. We will take this approximation as a benchmark for our
approach, and consider it the error an â€œoracleâ€ could obtain (hence the superscript O).
Another key term in our algorithmâ€™s error guarantee is the distance between Hn(W ) and W ,
n(W ) = Î´Ì‚2(Hn(W ),W ). (5)
It goes to zero as nâ†’âˆ by the following lemma, which follows easily from the results of [11].
Lemma 1. Let W be a graphon with â€–Wâ€–âˆ<âˆ. With probability one, â€–Hn(W )â€–1 â†’ â€–Wâ€–1 and
n(W )â†’ 0.
Convergence. Given a sequence of W -random graphs with target densities Ïn, one might wonder
whether the graphs Gn = Gn(ÏnW ) converge to W in a suitable metric. The answer is yes,
and involves the so-called cut-metric Î´ first introduced in [9]. Its definition is identical to the
definition (1) of the norm Î´2, except that instead of the L2-norm â€– Â· Â· Â· â€–2, it involves the Frieze-
Kannan cut-norm â€–Wâ€– defined as the sup of
âˆ£âˆ£âˆ«
SÃ—T W
âˆ£âˆ£ over all measurable sets S, T âŠ‚ [0, 1].
In the metric Î´, the W -random graphs Gn = Gn(ÏW ) then converge to W in the sense that
Î´
(
1
Ï(Gn)
W [Gn],W
)
â†’ 0, see [11] for the proof.
Estimation of Multi-Way Cuts. Using the results of [12], the convergence of Gn in the cut-metric
Î´ implies many interesting results for estimating various quantities defined on the graph Gn. In-
deed, a consistent approximation WÌ‚ to W in the metric Î´2 is clearly consistent in the weaker metric
Î´. This distance, in turn, controls various quantities of interest to computer scientists, e.g., the size
of all multi-way cuts, implying that a consistent estimator for W also gives consistent estimators for
all multi-way cuts. See the full version for details.
5
2.3 Differential Privacy for Graphs
The goal of this paper is the development of a differentially private algorithm for graphon estimation.
The privacy guarantees are formulated for worst-case inputs â€” we do not assume thatG is generated
from a graphon when analyzing privacy. This ensures that the guarantee remains meaningful no
matter what an analyst knows ahead of time about G.
In this paper, we consider node privacy. We call two graphs G and Gâ€² node neighbors if one can be
obtained from the other by removing one node and its adjacent edges.
Definition 1 (-node-privacy). A randomized algorithmA is -node-private if for all events S in the
output space of A, and node neighbors G,Gâ€²,
Pr[A(G) âˆˆ S] â‰¤ exp()Ã— Pr[A(Gâ€²) âˆˆ S] .
We also need the notion of the node-sensitivity of a function f : Gn â†’ R, defined as maximum
maxG,Gâ€² |f(G) âˆ’ f(Gâ€²)|, where the maximum goes over node-neighbors. The node sensitivity is
the Lipshitz constant of f viewed as a map between appropriate metrics.
3 Differentially Private Graphon Estimation
3.1 Least-squares Estimation
Given a graph as input generated by an unknown graphon W , our goal is to recover a block-model
approximation to W . The basic nonprivate algorithm we emulate is least squares estimation, which
outputs the k Ã— k matrix B which is closest to the input adjacency matrix A in the distance
Î´Ì‚2(B,A) = min
Ï€
â€–BÏ€ âˆ’Aâ€–2,
where the minimum runs over all equipartitions Ï€ of [n] into k classes, i.e., over all maps Ï€ : [n]â†’
[k] such that all classes have size as close to n/k as possible, i.e., such that ||Ï€âˆ’1(i)| âˆ’ n/k| < 1
for all i, and BÏ€ is the n Ã— n block-matrix with entries (BÏ€)xy = BÏ€(x)Ï€(y). If A is the adjacency
matrix of a graph G, we write Î´Ì‚2(B,G) instead of Î´Ì‚2(B,A). In the above notation, the basic
algorithm we would want to emulate is then the algorithm which outputs the least square fit BÌ‚ =
argminB Î´Ì‚2(B,G), where the argmin runs over all symmetric k Ã— k matrices B.
3.2 Towards a Private Algorithm
Our algorithm uses a carefully chosen instantiation of the exponential mechanism of McSherry and
Talwar [25]. The most direct application of their framework would be to output a random k Ã— k
matrix BÌ‚ according to the probability distribution
Pr(BÌ‚ = B) âˆ exp
(
âˆ’CÎ´Ì‚22(B,A)
)
,
for some C > 0. The resulting algorithm is -differentially private if we set C to be  over twice the
node-sensitivity of the â€œscore functionâ€, here Î´22(B, Â·). But this value of C turns out to be too small
to produce an output that is a good approximation to the least square estimator. Indeed, for a given
matrix B and equipartition Ï€, the node-sensitivity of â€–G âˆ’ BÏ€â€–22 can be as large as 1n , leading to a
value of C which is too small to produce useful results for sparse graphs.
To address this, we first note that we can work with an equivalent score that is much less sensitive.
Given B and Ï€, we subtract off the squared norm of G to obtain the following:
score(B, Ï€;G) = â€–Gâ€–22 âˆ’ â€–Gâˆ’BÏ€â€–22 = 2ã€ˆG,BÏ€ã€‰ âˆ’ â€–BÏ€â€–2, and (6)
score(B;G) = max
Ï€
score(B, Ï€;G), (7)
where the max ranges over equipartitions Ï€ : [n] â†’ [k]. For a fixed input graph G, maximizing
the score is the same as minimizing the distance, i.e. argminB Î´Ì‚2(B,G) = argmaxB score(B;G).
6
The sensitivity of the new score is then bounded by 2n2 Â· â€–Bâ€–âˆ times the maximum degree in G
(since G only affects the score via the inner product ã€ˆG,BÏ€ã€‰). But this is still problematic since, a
priori, we have no control over either the size of â€–Bâ€–âˆ or the maximal degree of G.
To keep the sensitivity low, we make two modifications: first, we only optimize over matrices B
whose entries bounded by (roughly) Ïn (since a good estimator will have entries which are not
much larger than â€–ÏnWâ€–âˆ, which is of order Ïn); second, we restrict the score to be accurate only
on graphs whose maximum degree is at most a constant times the average degree, since this is what
one expects for graphs generated from a bounded graphon. While the first restriction can be directly
enforced by the algorithm, the second is more delicate, since we need to provide privacy for all
inputs, including graphs with very large maximum degree. We employ an idea from [6, 21]: we first
consider the restriction of score(B, Ï€; Â·) to Gn,dn where dn will be chosen to be of the order of the
average degree of G, and then extend it back to all graphs while keeping the sensitivity low.
3.3 Private Estimation Algorithm
Our final algorithm takes as input the privacy parameter , the graph G, a number k of blocks, and a
constant Î» â‰¥ 1 that will have to be chosen large enough to guarantee consistency of the algorithm.
Algorithm 1: Private Estimation Algorithm
Input:  > 0, Î» â‰¥ 1, an integer k and graph G on n vertices.
Output: k Ã— k block graphon (represented as a k Ã— k matrix BÌ‚) estimating ÏW
Compute an (/2)-node-private density approximation ÏÌ‚ = Ï(G) + Lap(4/n) ;
d = Î»ÏÌ‚n (the target maximum degree) ;
Âµ = Î»ÏÌ‚ (the target Lâˆ norm for BÌ‚) ;
For each B and Ï€, let sÌ‚core(B, Ï€; Â·) denote a nondecreasing Lipschitz extension (from [21]) of
score(B, Ï€; Â·) from Gn,d to Gn such that for all matrices A, sÌ‚core(B, Ï€;A) â‰¤ score(B, Ï€;A), and
define
sÌ‚core(B;A) = max
Ï€
sÌ‚core(B, Ï€;A)
return BÌ‚, sampled from the distribution
Pr(BÌ‚ = B) âˆ exp
( 
4âˆ†
sÌ‚core(B;A)
)
,
where âˆ† =
4dÂµ
n2
=
4Î»2ÏÌ‚2
n
and B ranges over matrices in
BÂµ = {B âˆˆ [0, Âµ]kÃ—k : all entries Bi,j are multiples of 1n};
Our main results about the private algorithm are the following lemma and theorem.
Lemma 2. Algorithm 1 is -node private.
Theorem 1 (Performance of the Private Algorithm). Let W : [0, 1]2 â†’ [0,Î›] be a normalized
graphon, let 0 < ÏÎ› â‰¤ 1, let G = Gn(ÏW ), Î» â‰¥ 1, and k be an integer. Assume that Ïn â‰¥ 6 log n
and 8Î› â‰¤ Î» â‰¤
âˆš
n, 2 â‰¤ k â‰¤ min{n
âˆš
Ï
2 , e
Ïn
2 }. Then the Algorithm 1 outputs an approximation
(ÏÌ‚, BÌ‚) such that
Î´2
(
W,
1
ÏÌ‚
W [BÌ‚]
)
â‰¤ (O)k (W ) + 2n(W ) +OP
(
4
âˆš
Î»2 log k
Ïn
+ Î»
âˆš
k2 log n
n
+
Î»
nÏ
)
.
Remark 1. While Theorem 1 is stated in term of bounds which hold in probability, our proofs yield
statements which hold almost surely as nâ†’âˆ.
Remark 2. Under additional assumptions on the graphon W , we obtain tighter bounds. For ex-
ample, if we assume that W is HoÌˆlder continuous, i.e, there exist constants Î± âˆˆ (0, 1] and C < âˆ
such that |W (x, y) âˆ’ W (xâ€², yâ€²)| â‰¤ CÎ´Î± whenever |x âˆ’ xâ€²| + |y âˆ’ yâ€²| â‰¤ Î´, then we have that

(O)
k (W ) = O(k
âˆ’Î±) and n(W ) = OP (nâˆ’Î±/2).
7
Remark 3. When considering the â€œbestâ€ block model approximation to W , one might want to
consider block models with unequal block sizes; in a similar way, one might want to construct a
private algorithm that outputs a block model with unequal size blocks, and produces a bound in
terms of this best block model approximation instead of (O)k (W ). This can be proved with our
methods, with the minimal block size taking the role of 1/k in all our statements.
3.4 Non-Private Estimation Algorithm
We also analyze a simple, non-private algorithm, which outputs the argmin of Î´Ì‚2(Â·, A) over all
k Ã— k matrices whose entries are bounded by Î»Ï(G). (Independently of our work, this non-private
algorithm was also proposed and analysed in [22].) Our bound (3) refers to this restricted least square
algorithm, and does not require any assumptions on the average degree. As in (2), we suppress the
dependence of the error on Î». To include it, one has to multiply the OP term in (3) by
âˆš
Î».
4 Analysis of the Private and Non-Private Algorithm
At a high level, our proof of Theorem 1 (as well as our new bounds on non-private estimation) fol-
low from the fact that for all B and Ï€, the expected score E[Score(B, Ï€;G)] is equal to the score
Score(B, Ï€;Q), combined with a concentration argument. As a consequence, the maximizer BÌ‚ of
Score(B;G) will approximately minimize the L2-distance Î´Ì‚2(B,Q), which in turn will approxi-
mately minimize â€– 1ÏW [B]âˆ’Wâ€–2, thus relating the L2-error of our estimator BÌ‚ to the â€œoracle errorâ€

(O)
k (W ) defined in (4).
Our main concentration statement is captured in the following proposition. To state it, we define,
for every symmetric n Ã— n matrix Q with vanishing diagonal, Bern0(Q) to be the distribution
over symmetric matrices A with zero diagonal such that the entries {Aij : i < j} are independent
Bernouilli random variables with EAij = Qij .
Proposition 1. Let Âµ > 0, Q âˆˆ [0, 1]nÃ—n be a symmetric matrix with vanishing diagonal, and
A âˆ¼ Bern0(Q). If 2 â‰¤ k â‰¤ min{n
âˆš
Ï(Q), eÏ(Q)n} and BÌ‚ âˆˆ BÂµ is such that
Score(BÌ‚;A) â‰¥ max
BâˆˆBÂµ
Score(B;A)âˆ’ Î½2
for some Î½ > 0, then with probability at least 1âˆ’ 2eâˆ’n,
Î´Ì‚2(BÌ‚,Q) â‰¤ min
BâˆˆBÂµ
Î´Ì‚2(B,Q) + Î½ +O
(
4
âˆš
Âµ2Ï(Q)
(
k2
n2
+
log k
n
))
. (8)
Morally, the proposition contains almost all that is needed to establish the bound (3) proving consis-
tency of the non-private algorithm (which, in fact, only involves the case Î½ = 0), even though there
are several additional steps needed to complete the proof.
The proposition also contains an extra ingredient which is a crucial input for the analysis of the
private algorithm: it states that if instead of an optimal, least square estimator, we output an estimator
whose score is only approximately maximal, then the excess error introduced by the approximation
is small. To apply the proposition, we then establish a lemma which gives us a lower bound on the
score of the output BÌ‚ in terms of the maximal score and an excess error Î½.
There are several steps needed to execute this strategy, the most important ones involving a rigorous
control of the error introduced by the Lipschitz extension inside the exponential algorithm. We defer
the details to the full version.
Acknowledgments. A.S. was supported by NSF award IIS-1447700 and a Google Faculty Award.
Part of this work was done while visiting Boston Universityâ€™s Hariri Institute for Computation and
Harvard Universityâ€™s Center for Research on Computation and Society.
8
References
[1] E. Abbe and C. Sandon. Recovering communities in the general stochastic block model without knowing
the parameters. arXiv:1503.00609, 2015.
[2] E. Abbe and C. Sandon. Recovering communities in the general stochastic block model without knowing
the parameters. Manuscript, 2015.
[3] E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. arXiv:1405.3267,
2014.
[4] P. J. Bickel and A. Chen. A nonparametric view of network models and newman-girvan and other mod-
ularities. Proceedings of the National Academy of Sciences of the United States of America, 106:21068â€“
21073, 2009.
[5] P. J. Bickel, A. Chen, and E. Levina. The method of moments and degree distributions for network
models. Annals of Statistics, 39(5):2280â€“2301, 2011.
[6] J. Blocki, A. Blum, A. Datta, and O. Sheffet. Differentially private data analysis of social networks via
restricted sensitivity. In Innovations in Theoretical Computer Science (ITCS), pages 87â€“96, 2013.
[7] B. Bollobas, S. Janson, and O. Riordan. The phase transition in inhomogeneous random graphs. Random
Struct. Algorithms, 31:3â€“122, 2007.
[8] C. Borgs, J. T. Chayes, L. LovaÌsz, V. SoÌs, and K. Vesztergombi. Counting graph homomorphisms. In
Topics in Discrete Mathematics (eds. M. Klazar, J. Kratochvil, M. Loebl, J. Matousek, R. Thomas, P.Valtr),
pages 315â€“371. Springer, 2006.
[9] C. Borgs, J. T. Chayes, L. LovaÌsz, V. SoÌs, and K. Vesztergombi. Convergent graph sequences I: Subgraph
frequencies, metric properties, and testing. Advances in Math., 219:1801â€“1851, 2008.
[10] C. Borgs, J. T. Chayes, L. LovaÌsz, V. SoÌs, and K. Vesztergombi. Convergent graph sequences II: Multiway
cuts and statistical physics. Ann. of Math., 176:151â€“219, 2012.
[11] C. Borgs, J. T. Chayes, H. Cohn, and Y. Zhao. An Lp theory of sparse graph convergence I: limits, sparse
random graph models, and power law distributions. arXiv:1401.2906, 2014.
[12] C. Borgs, J. T. Chayes, H. Cohn, and Y. Zhao. An Lp theory of sparse graph convergence II: LD conver-
gence, quotients, and right convergence. arXiv:1408.0744, 2014.
[13] S. Chatterjee. Matrix estimation by universal singular value thresholding. Annals of Statistics, 43(1):
177â€“214, 2015.
[14] S. Chen and S. Zhou. Recursive mechanism: towards node differential privacy and unrestricted joins. In
ACM SIGMOD International Conference on Management of Data, pages 653â€“664, 2013.
[15] D. S. Choi, P. J. Wolfe, and E. M. Airoldi. Stochastic blockmodels with a growing number of classes.
Biometrika, 99:273â€“284, 2012.
[16] P. Diaconis and S. Janson. Graph limits and exchangeable random graphs. Rendiconti di Matematica, 28:
33â€”61, 2008.
[17] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis.
In S. Halevi and T. Rabin, editors, TCC, volume 3876, pages 265â€“284, 2006.
[18] C. Gao, Y. Lu, and H. H. Zhou. Rate-optimal graphon estimation. arXiv:1410.5837, 2014.
[19] P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis.
Journal of the American Statistical Association, 97(460):1090â€“1098, 2002.
[20] P. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Soc Netw, 5:109â€“137, 1983.
[21] S. P. Kasiviswanathan, K. Nissim, S. Raskhodnikova, and A. Smith. Analyzing graphs with node-
differential privacy. In Theory of Cryptography Conference (TCC), pages 457â€“476, 2013.
[22] O. Klopp, A. Tsybakov, and N. Verzelen. Oracle inequalities for network models and sparse graphon
estimation. arXiv:1507.04118, 2015.
[23] L. LovaÌsz and B. Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory, Series B,
96:933â€“957, 2006.
[24] W. Lu and G. Miklau. Exponential random graph estimation under differential privacy. In 20th ACM
SIGKDD International Conference on Knowledge discovery and data mining, pages 921â€“930, 2014.
[25] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, pages 94â€“103. IEEE,
2007.
[26] S. Raskhodnikova and A. Smith. High-dimensional Lipschitz extensions and node-private analysis of
network data. arXiv:1504.07912, 2015.
[27] K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel.
Ann. Statist., 39(4):1878â€“1915, 08 2011.
[28] P. Wolfe and S. C. Olhede. Nonparametric graphon estimation. arXiv:1309.5936, 2013.
9
