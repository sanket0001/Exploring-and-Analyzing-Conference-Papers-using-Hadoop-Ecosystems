


Paper ID = 6006
Title = Learning Structured Densities via Infinite
Dimensional Exponential Families
Siqi Sun
TTI Chicago
siqi.sun@ttic.edu
Mladen Kolar
University of Chicago
mkolar@chicagobooth.edu
Jinbo Xu
TTI Chicago
jinbo.xu@gmail.com
Abstract
Learning the structure of a probabilistic graphical models is a well studied prob-
lem in the machine learning community due to its importance in many applica-
tions. Current approaches are mainly focused on learning the structure under re-
strictive parametric assumptions, which limits the applicability of these methods.
In this paper, we study the problem of estimating the structure of a probabilistic
graphical model without assuming a particular parametric model. We consider
probabilities that are members of an infinite dimensional exponential family [4],
which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its
kernel k. One difficulty in learning nonparametric densities is the evaluation of
the normalizing constant. In order to avoid this issue, our procedure minimizes
the penalized score matching objective [10, 11]. We show how to efficiently min-
imize the proposed objective using existing group lasso solvers. Furthermore, we
prove that our procedure recovers the graph structure with high-probability under
mild conditions. Simulation studies illustrate ability of our procedure to recover
the true graph structure without the knowledge of the data generating process.
1 Introduction
Undirected graphical models, or Markov random fields [13], have been extensively studied and ap-
plied in fields ranging from computational biology [15, 28], to natural language processing [16, 20]
and computer vision [9, 17]. In an undirected graphical model, conditional independence assump-
tions underlying a probability distribution are encoded in the graph structure. Furthermore, the joint
probability density function can be factorized according to the cliques of the graph [14]. One of the
fundamental problems in the literature is learning the structure of a graphical model given an i.i.d.
sample from an unknown distribution. A lot of work has been done under specific parametric as-
sumptions on the unknown distribution. For example, in Gaussian Graphical Models the structure of
the graph is encoded by the sparsity pattern of the precision matrix [6, 30]. Similarly, in the context
of exponential family graphical models, where the node conditional distribution given all the other
nodes is a member of an exponential family, the structure is described by the non-zero coefficients
[29]. Most existing approaches to learn the structure of a high-dimensional undirected graphical
model are based on minimizing a penalized loss objective, where the loss is usually a log-likelihood
or a composite likelihood and the penalty induces sparsity on the resulting parameter vector [see,
for example, 6, 12, 18, 22, 24, 29, 30]. In addition to sparsity inducing penalties, methods that
use other structural constraints have been proposed. For example, since many real-world networks
are scale-free [1], several algorithms are designed specifically to learn structure of such networks
1
[5, 19]. Graphs tend to have cluster structure and learning simultaneously the structure and cluster
assignment has been investigated [2, 27].
In this paper, we focus on learning the structure of a pairwise graphical models without assuming
a parametric class of models. The main challenge in estimating nonparametric graphical models
is computation of the log normalizing constant. To get around this problem, we propose to use
score matching [10, 11] as a divergence, instead of the usual KL divergence, as it does not require
evaluation of the log partition function. The probability density function is estimated by minimizing
the expected distance between the model score function and the data score function, where the score
function is defined as gradient of the corresponding probability density functions. The advantage
of this measure is that the normalization constant is canceled out when computing the distance. In
order to learn the underlying graph structure, we assume that the logarithm of the density is additive
in node-wise and edge-wise potentials and use a sparsity inducing penalty to select non-zero edge
potentials. As we will prove later, our procedure will allow us to consistently estimate the underlying
graph structure.
The rest of paper is organized as follows. We first introduce the notations, background and related
work. Then we formulate our model, establish a representer theorem and present a group lasso
algorithm to optimize the objective. Next we prove that our estimator is consistent by showing that
it can recover the true graph with high probability given sufficient number of samples. Finally the
results for simulated data are presented to demonstrate the correctness of our algorithm empirically.
1.1 Notations
Let [n] denote the set {1, 2, . . . , n}. For a vector θ = (θ1, . . . , θd)T ∈ Rd, let ‖θ‖p =
(
∑
i∈[d] |θi|p)
1
p denote its lp norm. Let column vector vec(D) denote the vectorization of ma-
trix D, cat(a, b) denote the concatenation of two vectors a and b, and mat(aT1 , . . . , a
T
d ) the
matrix with rows given by aT1 , . . . , a
T
d . For χ ⊆ Rd, let Lp(χ, p0) denote the space of func-
tion for which the p-th power of absolute value is p0 integrable; and for f ∈ Lp(χ, p0), let
‖f‖Lp(χ,p0) = ‖f‖p = (
∫
χ
|f |pdx)
1
p denote its Lp norm. Throughout the paper, we denote H
(orHi,Hij) as Hilbert space and 〈·, ·〉H, ‖ · ‖H as corresponding inner product and norm.
For any operator C : H1 → H2, we use ‖C‖ to denote the usual operator norm, which is defined as
‖C‖ = inf{a ≥ 0 : ‖Cf‖H2 ≤ a‖f‖H1 for all f ∈ H1};
and ‖C‖HS to denote its Hilbert-Schmidt norm, which is defined as
‖C‖2HS =
∑
i∈I
‖Cei‖2H2 ,
where ei is an orthonormal basis ofH for an index set I . Also, we useR(C) to denote operator C’s
range space. For any f ∈ H1 and g ∈ H2, let f ⊗ g denote their tensor product.
2 Background & Related Work
2.1 Learning graphical models in exponential families
Let x = (x1, x2, ..., xd) be a d-dimensional random vector from a multivariate Gaussian distribution.
It is well known that the conditional independency of two variables given all the others is encoded
in the zero pattern of its precision matrix Ω, that is, xi and xj are conditionally independent given
x−ij if and only if Ωij = 0, where x−ij is the vector of x without xi and xj . A sparse estimate
of Ω can be obtained by maximum-likelihood (joint selection) or pseudo-likelihood (neighborhood
selection) optimization with an added l1 penalty [6, 22, 30]. Given n independent realizations of x
(rows of X ∈ Rn×d), the penalized maximum-likelihood estimate of the precision matrix can be
obtained as
Ω̂λ = arg min
Ω0
tr(ŜΩ)− log det Ω + λ‖Ω‖1, (1)
where Ŝ = n−1XTX and λ controls the sparsity level of estimated graph.
2
The pseudo-likelihood method estimates the neighborhood of a node a by the non-zeros of the
solution to a regularized linear model
θ̂s = arg min
θ
1
n
‖Xs −X−sθ‖22 + λ‖θ‖1. (2)
The estimated neighborhood is then N̂(s) = {a : θsa 6= 0}.
Another way to specify a parametric graphical model is by assuming that each node-conditional
distributions is a part of the exponential family [29]. Specifically, the conditional distribution of xs
given x−s is assumed to be
P (xs|x−s) = exp(
∑
t∈N(s)
θstxsxt + C(xs)−D(x−s, θ)), (3)
whereC is the base measure,D is the log-normalization constant andN(s) is the neighborhood a the
node s. Similar to (2), the neighborhood of each node can be estimated by minimizing the negative
log-likelihood with l1 penalty on θ. The optimization is tractable when the normalization constant
D can be easily computed based on the model assumption. For example, under Poisson graphical
model assumptions for count data, the normalization constant is− exp(
∑
t∈N(s) θstxt). When using
the neighborhood estimation, the graph can be estimated as the union of the neighborhoods of each
node, which leads to consistent graph estimation [22, 29].
2.2 Generalized Exponential Family and RKHS
We say H is a RKHS associated with kernel k : χ × χ → R+ if and only if for each x ∈ χ, the
following two conditions are satisfied: (1) k(·, x) ∈ H and (2) it has reproducing properties such that
f(x) = 〈f, k(·, x)〉H for all f(·) ∈ H, where k is a symmetric and positive semidefinite function.
Denote the RKHSH with kernel k asH(k).
For any f ∈ H(k), there exists a set of xi and αi, such that f(·) =
∑∞
i=1 αik(·, xi). Similarly
for any g ∈ H(k), g(·) =
∑∞
j=1 βjk(·, yj), the inner product of f and g is defined as 〈f, g〉H =∑∞
i,j=1 αiβjk(xi, yj). Therefore the norm of f simply is ‖f‖H =
√∑
i,j αiαjk(xi, xj). The sum-
mation is guaranteed to be larger than or equal to zero because the kernel k is positive semidefinite.
We consider the exponential family in infinite dimensions [4], where
P = {pf (x) = ef(x)−A(f)q0(x), x ∈ χ; f ∈ F}
and the function space F is defined as
F = {f ∈ H(k) : A(f) = log
∫
χ
ef(x)q0(x)dx <∞},
where q0(x) is the base measure, A(f) is a generalized normalization constant such that pf (x) is
a valid probability density function, and H is a RKHS [3] associated with kernel k. To see it as
a generalization of the exponential family, we show some examples that can generate useful finite
dimension exponential families:
• Normal: χ = R, k(x, y) = xy + x2y2
• Poisson: χ = N ∪ {0}, k(x, y) = xy
• Exponential: χ = R+, k(x, y) = xy.
For more detailed information, please refer to [4].
When learning structure of a graphical model, we will further impose structural conditions onH(k)
in order ensure that F consists of additive functions.
2.3 Score Matching
Score matching is a convenient procedure that allows for estimating a probability density without
computing the normalizing constant [10, 11]. It is based on minimizing Fisher divergence
J(p‖p0) =
1
2
∫
p(x)
∥∥∥∥∂ log p(x)∂x − ∂ log p0(x)∂x
∥∥∥∥2
2
dx, (4)
3
where ∂ log p(x)∂x = (
∂ log p(x)
∂x1
, . . . , ∂ log p(x)∂xd ) is the score function. Observe that for p(x, θ) =
1
Z(θ)q(x, θ) the normalization constant Z(θ) cancels out in the gradient computation, which makes
the divergence independent of Z(θ). Since the score matching objective involves the unknown or-
acle probability density function p0, it is typically not computable. However, under some mild
conditions which we will discuss in METHODS section, (4) can be rewritten as
J(p‖p0) =
∫
p0(x)
∑
i∈[d]
1
2
(
∂ log p(x)
∂xi
)2 +
∂2 log p(x)
∂x2i
dx. (5)
After substituting the expectation with an empirical average, we get
Ĵ(p‖p0) =
1
n
∑
a∈[n]
∑
i∈[d]
1
2
(
∂ log p(Xa)
∂xi
)2 +
∂2 log p(Xa)
∂x2i
. (6)
Compared to maximum likelihood estimation, minimizing Ĵ(p‖p0) is computationally tractable.
While we will be able to estimate p0 only up to a scale factor, this will be sufficient for the purpose
of graph structure estimation.
3 Methods
3.1 Model Formulation and Assumptions
We assume that the true probability density function p0 is in P . Furthermore, for simplicity we
assume that
log p0(x) = f(x) =
∑
i≤j
(i,j)∈S
f0,ij(xi, xj),
where f0,ii(xi, xi) is a node potential and f0,ij(xi, xj) is an edge potential. The set S denotes the
edge set of the graph. Extensions to models where potentials are defined over larger cliques are
possible. We further assume that f0,ij ∈ Hij(kij), where Hij is a RKHS with kernel kij . To
simplify the notation, we use f0,ij(x) or kij(·, x) to denote f0,ij(xi, xj) and kij(·, (xi, xj)). If the
context is clear, we drop the subscript for norm or inner product. Define
H(S) = {f =
∑
(i,j)∈S
fij |fij ∈ Hij} (7)
as a set of functions that decompose as sum of bivariate functions on edge set S. Note that
H(S) is also (a subset of) a RKHS with the norm ‖f‖2H(S) =
∑
(i,j)∈S ‖fij‖2Hij and kernel
k =
∑
(i,j)∈S kij .
Let Ω(f) = ‖f‖H,1 =
∑
i≤j ‖fij‖Hij . For any edge set S (not necessarily the true edge set), we
denote ΩS(fS) =
∑
s∈S ‖fs‖Hs as the norm Ω reduced to S. Similarly, denote its dual norm as
Ω∗S [fS ] = maxΩS(gS)≤1〈fS , gS〉 [25].
Under the assumption that the unknown f0 is additive, the loss function becomes
J(f) =
1
2
∫
p0(x)
∑
i∈[d]
(∂f(x)
∂xi
− ∂f0(x)
∂xi
)2
dx
=
1
2
∑
i∈[d]
∑
j,j′∈[d]
〈fij − f0,ij ,
∫
p0(x)
∂kij(·, (xi, xj))
∂xi
⊗ ∂kij
′(·, (xi, xj′))
∂xi
dx(fij′ − f0,ij′)〉
=
1
2
∑
i∈[d]
∑
j,j′∈[d]
〈fij − f0,ij , Cijij′(fij′ − f0,ij′)〉.
Intuitively, C can be viewed as a d2 matrix, and the operator at position (ij, ij′) is Cij,ij′ . For
general (ij, i′j′), i 6= i′ the corresponding operator simply is 0. Define CSS′ as∫
p0(x)
∑
(i,j)∈S,(i′,j′)∈S′
∂kij(·, (xi, xj))
∂xi
⊗ ∂ki
′j′(·, (xi′ , xj′))
∂xi
dx,
4
which intuitively can be treated as a sub matrix of C with rows S and columns S′. We will use this
notation intensively in the main theorem and its proof.
Following [26], we make the following assumptions.
A1. Each kij is twice differentiable on χ× χ.
A2. For any i and x̃j ∈ χj = [aj , bj ], we assume that
lim
xi→a+i or b
−
i
∂2kij(x, y)
∂xi∂yi
|y=x p20(x) = 0,
where x = (xi, x̃j) and ai, bi could be −∞ or∞.
A3. This condition ensures that J(p‖p0) <∞ for any p ∈ P [for more details see 26]:
‖∂kij(·, x)
∂xi
‖Hij ∈ L2(χ, p0), ‖
∂2kij(·, x)
∂x2i
‖Hij ∈ L2(χ, p0).
A4. The operator CSS , is compact and the smallest eigenvalue ωmin = λmin(CSS) > 0.
A5. Ω∗Sc [CScSC
−1
SS ] ≤ 1− η, where η > 0.
A6. f0 ∈ R(C), which means there exists γ ∈ H, such that f0 = Cγ. f0 is the oracle function.
We will discuss the definition of operator C and Ω∗ in section 4. Compared with [29], A4 can be
interpreted as the dependency condition and the A5 is the incoherence condition, which is a standard
condition for structure learning in high dimensional statistical estimators.
3.2 Estimation Procedure
We estimate f by minimizing the following penalized score matching objective
min
f
L̂µ(f) = Ĵ(f) +
µ
2
‖f‖H,1
s.t. fij ∈ Hij , (8)
where Ĵ(f) is given in (6). The norm ‖f‖H,1 =
∑
i≤j ‖fij‖Hij is used as a sparsity inducing
penalty. A simplified form of Ĵ(f) is given below that will lead to efficient algorithm for solving
(8).
The following theorem states that the score matching objective can be written as a penalized
quadratic function on f .
Theorem 3.1 (i) The score matching objective can be represented as
Lµ(f) =
1
2
〈f − f0, C(f − f0)〉+
µ
2
‖f‖H,1 (9)
where C =
∫
p0(x)
∑
i∈[d]
∂k(·,x)
∂xi
⊗ ∂k(·,x)∂xi dx is a trace operator.
(ii) Given observed data Xn×d, the empirical estimation of Lµ is
L̂µ(f) =
1
2
〈f, Ĉf〉+
∑
i≤j
〈fij ,−ξ̂ij〉+
µ
2
‖f‖H,1 + const (10)
where Ĉ = 1n
∑
a∈[n]
∑
i∈[d]
∂k(·,Xa)
∂xi
⊗ ∂k(·,Xa)∂xi and ξ̂ij =
1
n
∑
a∈[n]
∂2kij(·,(Xai,Xaj))
∂x2i
+
∂2kij(·,(Xai,Xaj))
∂x2j
if i 6= j, or ξ̂ij = 1n
∑
a∈n
∂2kij(·,(Xai,Xaj))
∂x2i
otherwise.
Please refer to our supplementary material for detailed proof 1.
The above theorem still requires us to minimize over F . Our next results shows that the solution is
finite dimensional. That is, we establish a representer theorem for our problem.
1Please visit ttic.uchicago.edu/∼siqi for supplementary material and code.
5
Theorem 3.2 (i) The solution to (10) can be represented as
f̂ij(·) =
∑
b∈[n]
βbij
∂kij(·, (Xbi, Xbj))
∂xi
+ βbji
∂kij(·, (Xbi, Xbj))
∂xj
+ αij ξ̂ij , (11)
where i ≤ j.
(ii) Minimizing (10) is equivalent to minimizing the following quadratic function:
1
2n
∑
ai
(∑
bj
(βbijG
ab
ij11 + βbjiG
ab
ij12) +
∑
j
αijh
1a
ij
)2
+
∑
i≤j
∑
b
(βbijh
1b
ij + βbjih
2b
ij ) +
∑
i≤j
αij‖ξ̂ij‖2 +
µ
2
‖f‖H,1
=
1
2n
∑
ai
(DTai · θ)2 + Etθ +
µ
2
∑
i≤j
√
θtijFijθij (12)
where Gabijrs =
∂2kij(Xa,Xb)
∂xr∂ys
, hrbij = 〈
∂kij(·,Xb)
∂xr
, ξ̂ij〉 are constant that only depends on X , θ =
cat(vec(α), vec(β)) is the vector parameter and θij = cat(αij , vec(β·ij)) is a group of parameters.
Dai, E and F are corresponding constant vectors and matrices based on G, h and the order of
parameters. Then the above problem can be solved by group lasso [7, 21].
The first part of theorem states our representer theorem, and the second part is obtained by plugging
in (11) to (10). See supplementary material for a detailed proof. Theorem 3.2 provides us with an
efficient way to minimize (8), as it reduced the optimization to a group lasso problem for which
many efficient solvers exist.
Let f̂µ = arg minf∈H L̂µ(f) denote the solution to (12). We can estimate the graph as follows:
Ŝµ = {(i, j) : ‖f̂µij‖ 6= 0}, (13)
That is, the graph is encoded in the sparsity pattern of f̂µ.
4 Statistical Guarantees
In this section we study statistical properties of the proposed estimator (13). Let S denote the true
edge set and Sc its complement. We prove that Ŝµ recovers S with high probability when the sample
size n is sufficiently large.
Denote D = mat(DT11, . . . , D
T
ai, . . . , D
T
nd). We will need the following result on the estimated
operator Ĉ,
Proposition 4.1 (Lemma 5 in [8] or Theorem 5 in [26] ) (Properties of C)
1. ‖Ĉ − C‖HS = Op0(n−
1
2 )
2. ‖(C +µL)−1‖ ≤ 1µmin diag(L) , ‖C(C +µL)
−1‖ ≤ 1, where µ > 0 and L is diagonal with
positive constants.
The following result gives first order optimality conditions for the optimization problem (8).
Proposition 4.2 (Optimality Condition)
Ĵ(f) + µ2 Ω(f)
2 achieves optimality when the following two conditions are satisfied:
(1) ∇fs Ĵ(f) + µΩ(f)
fs
‖fs‖Hs
= 0 ∀s ∈ S
(2) Ω∗Sc [∇fSc Ĵ(f)] ≤ µΩ(f).
6
With these preliminary results, we have the following main results.
Theorem 4.3 Assume that conditions A1-A7 are satisfied. The regularization parameter µ is se-
lected at the order of n−
1
4 and satisfies µ ≤ ηκminωmin
4(1−η)κmax
√
|S|+ η5
, where κmin = mins∈S ‖f∗s ‖ > 0
and κmax = maxs∈S ‖f∗s ‖ > 0. Then P (Ŝµ = S)→ 1.
Proof Idea: The theorem above is the main theoretical guarantee for our score matching estimator.
We use the “witness” proof framework inspired by [23, 29]. Let f∗ denote the true density function
and p∗ the probability density function. We first construct a solution f̂S on true edge set S as
f̂S = min
fSc=0
Ĵ(f) +
µ
2
(
∑
(i,j)∈S
‖fij‖)2 (14)
and set f̂Sc as zero. Using Proposition 4.1, we prove that ‖f̂S − f∗S‖ = Op(n−
1
4 ). Then we
compute the subgradient on Sc and prove that its dual norm is upper bounded by µΩ(f) by using
assumptions A4, A5 and A6. Therefore we construct a solution that satisfied the optimality condition
and converges in probability to the true graph. Refer to supplementary material for detailed proof.
5 Experiments
We illustrate performance of our method on two simulations. In our experiments, we use the same
kernel defined as follows:
k(x, y) = exp(−‖x− y‖
2
2
2σ2
) + r(xT y + c)2, (15)
that is, the summation of a Gaussian kernel and a polynomial kernel. We set σ2 = 1.5, r = 0.1 and
c = 0.5 for all the simulations.
We report the true positive rate vs false positive rate (ROC) curve to measure the performance of
different procedures. Let S be the true edge set, and let Ŝµ be the estimated graph. The true positive
rate is defined as TPRµ =
|S=1 and Ŝµ=1|
|S=1| , and false positive rate is FPRµ =
|Ŝµ=1 and S=0|
|S=0| , where | · |
is the cardinality of the set. The curve is then plotted based on 100 uniformly-sampled regularization
parameters and based on 20 independent runs.
In the first simulation, we apply our algorithm to data sampled from a simple chain graph-based
Gaussian model (see Figure 1 for detail), and compare its performance with glasso [6]. We use the
same sampling method as in [31] to generate the data: we set Ωs = 0.4 for s ∈ S and its diagonal
to a constant such that Ω is positive definite. We set the dimension d to 25 and change the sample
size n ∈ {20, 40, 60, 80, 100} data points.
Except for the low sample size case (n = 20), the performance of our method is comparable with
glasso, without utilizing the fact that the underlying distribution is of a particular parametric form.
Intuitively, to capture the graph structure, the proposed nonparametric method requires more data
because of much weaker assumptions.
To further show the strength of our algorithm, we test it on a nonparanormal (NPN) distribution
([18]). A random vector x = (x1, . . . , xp) has a nonparanormal distribution if there exist functions
(f1, . . . , fp) such that (f1(x1), . . . , fd(xd)) ∼ N(µ,Σ). When f is monotone and differentiable,
the probability density function is given by
P (x) =
1
(2π)
p
2 |Σ| 12
exp{−1
2
(f(x)− µ)TΣ−1(f(x)− µ)}
∏
j
|f ′j |.
Here the graph structure is still encoded in the sparsity pattern of Ω = Σ−1, that is, xi⊥xj |x−i,j if
and only if Ωij = 0 [18].
In our experiments we use the “Symmetric Power Transformation” [18], that is,
fj(zj) = σj(
g0(zj − µj)√∫
g20(t− µj)φ(
t−µj
σj
)dt
) + µj ,
7
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Adjacent Matrix
●
●
●●
●●●●●●
●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●●●
●●
●
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Glasso
FalsePositiveRate
Tr
ue
P
os
iti
ve
R
at
e
● 20
40
60
80
100 ●●
●●
●●
●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●
●
●
●
●
● ●
●
● ●
●
● ●
●
●
●
● ●
●
●
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
SME
FalsePositiveRate
Tr
ue
P
os
iti
ve
R
at
e
● 20
40
60
80
100
Figure 1: The estimation results for Gaussian graphical models. left: The adjacent matrix of true
graph. center: the ROC curve of glasso. right: the ROC curve of score matching estimator (SME).
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●●
●
●
●●
●
●●
●
●●●●
●
●●●
●●●●
●●●●●●
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Glasso
FalsePositiveRate
Tr
ue
P
os
iti
ve
R
at
e
● 20
40
60
80
100
●
●●●●●●
●●●●●●●●●●●●●●●
●●●●
●
●●●
●
●●
●
●●●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●●●
●●●
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
NonParaNormal
FalsePositiveRate
Tr
ue
P
os
iti
ve
R
at
e
● 20
40
60
80
100 ●●●●
●●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
●
● ●
●
●
● ●
●
●
●
●
● ●
●●
●
●
●
●
● ● ●
● ●
● ●
●
● ● ●
●
●●●
●●
●
●
●●●
●
●●
●
●●
●●●●
●●●●●●●
●
●
●
●●●
●●●
●
●●●●●
●●●●
●●●
●●●●
●●●●●
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
SME
FalsePositiveRate
Tr
ue
P
os
iti
ve
R
at
e
● 20
40
60
80
100
Figure 2: The estimated ROC curves of nonparanormal graphical models for glasso (left), NPN
(center) and SME (right).
where g0(t) = sign(t)|t|α, to transform data. For comparison with graph lasso, we first use a
truncation method to Gaussianize the data, and then apply graphical lasso to the transformed data.
See [18, 31] for details. From figure 2, without knowing the underlying data distribution, the score
matching estimator outperforms glasso, and show similar results to nonparanormal when the sample
size is large.
6 Discussion
In this paper, we have proposed a new procedure for learning the structure of a nonparametric graph-
ical model. Our procedure is based on minimizing a penalized score matching objective, which can
be performed efficiently using existing group lasso solvers. Particularly appealing aspect of our
approach is that it does not require computing the normalization constant. Therefore, our proce-
dure can be applied to a very broad family of infinite dimensional exponential families. We have
established that the procedure provably recovers the true underlying graphical structure with high-
probability under mild conditions. In the future, we plan to investigate more efficient algorithms for
solving (10), since it is often the case that Ĉ is well structured and can be efficiently approximated.
Acknowledgments
The authors are grateful to the financial support from National Institutes of Health R01GM0897532,
National Science Foundation CAREER award CCF-1149811 and IBM Corporation Faculty Re-
search Fund at the University of Chicago Booth School of Business. This work was completed in
part with resources provided by the University of Chicago Research Computing Center.
8
References
[1] R. Albert. Scale-free networks in cell biology. Journal of cell science, 118(21):4947–4957, 2005.
[2] C. Ambroise, J. Chiquet, C. Matias, et al. Inferring sparse gaussian graphical models with latent structure.
Electronic Journal of Statistics, 3:205–238, 2009.
[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, pages
337–404, 1950.
[4] S. Canu and A. Smola. Kernel methods and the exponential family. Neurocomputing, 69(7):714–720,
2006.
[5] A. Defazio and T. S. Caetano. A convex formulation for learning scale-free networks via submodular
relaxation. In Advances in Neural Information Processing Systems, pages 1250–1258, 2012.
[6] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 9(3):432–441, 2008.
[7] J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group lasso. arXiv
preprint arXiv:1001.0736, 2010.
[8] K. Fukumizu, F. R. Bach, and A. Gretton. Statistical consistency of kernel canonical correlation analysis.
The Journal of Machine Learning Research, 8:361–383, 2007.
[9] S. Geman and C. Graffigne. Markov random field image models and their applications to computer vision.
In Proceedings of the International Congress of Mathematicians, volume 1, page 2, 1986.
[10] A. Hyvärinen. Estimation of non-normalized statistical models by score matching. In Journal of Machine
Learning Research, pages 695–709, 2005.
[11] A. Hyvärinen. Some extensions of score matching. Computational statistics & data analysis, 51(5):2499–
2512, 2007.
[12] Y. Jeon and Y. Lin. An effective method for high-dimensional log-density anova estimation, with appli-
cation to nonparametric graphical model building. Statistica Sinica, 16(2):353, 2006.
[13] R. Kindermann, J. L. Snell, et al. Markov random fields and their applications, volume 1. American
Mathematical Society Providence, RI, 1980.
[14] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
[15] Y. A. Kourmpetis, A. D. Van Dijk, M. C. Bink, R. C. van Ham, and C. J. ter Braak. Bayesian markov
random field analysis for protein function prediction based on network data. PloS one, 5(2):e9293, 2010.
[16] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segment-
ing and labeling sequence data. 2001.
[17] S. Z. Li. Markov random field modeling in Image Analysis. 2011.
[18] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimen-
sional undirected graphs. The Journal of Machine Learning Research, 10:2295–2328, 2009.
[19] Q. Liu and A. T. Ihler. Learning scale free networks by reweighted l1 regularization. In International
Conference on Artificial Intelligence and Statistics, pages 40–48, 2011.
[20] C. D. Manning and H. Schütze. Foundations of statistical natural language processing. MIT press, 1999.
[21] L. Meier, S. Van De Geer, and P. Bühlmann. The group lasso for logistic regression. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 70(1):53–71, 2008.
[22] N. Meinshausen and P. Bühlmann. High-dimensional graphs and variable selection with the lasso. The
Annals of Statistics, pages 1436–1462, 2006.
[23] P. Ravikumar, M. J. Wainwright, and J. Lafferty. High-dimensional graphical model selection using l1-
regularized logistic regression. 2008.
[24] P. Ravikumar, M. J. Wainwright, J. D. Lafferty, et al. High-dimensional ising model selection using
1-regularized logistic regression. The Annals of Statistics, 38(3):1287–1319, 2010.
[25] R. T. Rockafellar. Convex analysis. Number 28. Princeton university press, 1970.
[26] B. Sriperumbudur, K. Fukumizu, R. Kumar, A. Gretton, and A. Hyvärinen. Density estimation in infinite
dimensional exponential families. arXiv preprint arXiv:1312.3516, 2013.
[27] S. Sun, H. Wang, and J. Xu. Inferring block structure of graphical models in exponential families. In
Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages
939–947, 2015.
[28] Z. Wei and H. Li. A markov random field model for network-based analysis of genomic data. Bioinfor-
matics, 23(12):1537–1544, 2007.
[29] E. Yang, G. Allen, Z. Liu, and P. K. Ravikumar. Graphical models via generalized linear models. In
Advances in Neural Information Processing Systems, pages 1358–1366, 2012.
[30] M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika,
94(1):19–35, 2007.
[31] T. Zhao, H. Liu, K. Roeder, J. Lafferty, and L. Wasserman. The huge package for high-dimensional
undirected graph estimation in r. The Journal of Machine Learning Research, 13(1):1059–1062, 2012.
9
