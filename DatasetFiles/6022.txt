


Paper ID = 6022
Title = Unified View of Matrix Completion under General
Structural Constraints
Suriya Gunasekar
UT at Austin, USA
suriya@utexas.edu
Arindam Banerjee
UMN Twin Cities, USA
banerjee@cs.umn.edu
Joydeep Ghosh
UT at Austin, USA
ghosh@ece.utexas.edu
Abstract
In this paper, we present a unified analysis of matrix completion under general
low-dimensional structural constraints induced by any norm regularization. We
consider two estimators for the general problem of structured matrix completion,
and provide unified upper bounds on the sample complexity and the estimation
error. Our analysis relies on results from generic chaining, and we establish
two intermediate results of independent interest: (a) in characterizing the size
or complexity of low dimensional subsets in high dimensional ambient space, a
certain partial complexity measure encountered in the analysis of matrix comple-
tion problems is characterized in terms of a well understood complexity measure
of Gaussian widths, and (b) it is shown that a form of restricted strong convexity
holds for matrix completion problems under general norm regularization. Further,
we provide several non-trivial examples of structures included in our framework,
notably the recently proposed spectral k-support norm.
1 Introduction
The task of completing the missing entries of a matrix from an incomplete subset of (potentially
noisy) entries is encountered in many applications including recommendation systems, data impu-
tation, covariance matrix estimation, and sensor localization among others. Traditionally illâ€“posed
high dimensional estimation problems, where the number of parameters to be estimated is much
higher than the number of observations, has been extensively studied in the recent literature. How-
ever, matrix completion problems are particularly illâ€“posed as the observations are both limited
(high dimensional), and the measurements are extremely localized, i.e., the observations consist of
individual matrix entries. The localized measurement model, in contrast to random Gaussian or
subâ€“Gaussian measurements, poses additional complications in high dimensional estimation.
For wellâ€“posed estimation in high dimensional problems including matrix completion, it is imper-
ative that low dimensional structural constraints are imposed on the target. For matrix completion,
the special case of lowâ€“rank constraint has been widely studied. Several existing work propose
tractable estimators with nearâ€“optimal recovery guarantees for (approximate) lowâ€“rank matrix com-
pletion [8, 7, 28, 26, 18, 19, 22, 11, 20, 21]. A recent work [16] addresses the extension to structures
with decomposable norm regularization. However, the scope of matrix completion extends for low
dimensional structures far beyond simple lowâ€“rankness or decomposable norm structures.
In this paper, we present a unified statistical analysis of matrix completion under a general set of low
dimensional structures that are induced by any suitable norm regularization. We provide statistical
analysis of two generalized matrix completion estimators, the constrained norm minimizer, and the
generalized matrix Dantzig selector (Section 2.2). The main results in the paper (Theorem 1aâ€“1b)
provide unified upper bounds on the sample complexity and estimation error of these estimators for
matrix completion under any norm regularization. Existing results on matrix completion with low
rank or other decomposable structures can be obtained as special cases of our general results.
1
Our unified analysis of sample complexity is motivated by recent work on high dimensional estima-
tion using global (sub) Gaussian measurements [10, 1, 35, 3, 37, 5]. A key ingredient in the recovery
analysis of high dimensional estimation involves establishing a certain variation of Restricted Isom-
etry Property (RIP) [9] of the measurement operator. It has been shown that such properties are sat-
isfied by Gaussian and subâ€“Gaussian measurement operators with high probability. Unfortunately,
as has been noted before by Candes et al. [8], owing to highly localized measurements, such con-
ditions are not satisfied in the matrix completion problem, and the existing results based on global
(sub) Gaussian measurements are not directly applicable. In fact, a key question we consider is:
given the radically limited measurement model in matrix completion, by how much would the sam-
ple complexity of estimation increase beyond the known sample complexity bounds for global (sub)
Gaussian measurements. Our results upper bounds the sample complexity for matrix completion to
within a log d factor over that for estimation under global (sub) Gaussian measurements [10, 3, 5].
While the result was previously known for low rank matrix completion using nuclear norm min-
imization [26, 20], with a careful use of generic chaining, we show that the log d factor suffices
for structures induced by any norm! As a key intermediate result, we show that a useful form of
restricted strong convexity (RSC) [27] holds for the localized measurements encountered in matrix
completion under general norm regularized structures. The result substantially generalizes existing
RSC results for matrix completion under the special cases of nuclear norm and decomposable norm
regularization [26, 16].
For our analysis, we use tools from generic chaining [33] to characterize the main results (Theo-
rem 1aâ€“1b) in terms of the Gaussian width (Definition 1) of certain error sets. Gaussian widths
provide a powerful geometric characterization for quantifying the complexity of a structured low di-
mensional subset in a high dimensional ambient space. Numerous tools have been developed in the
literature for bounding the Gaussian width of structured sets. A unified characterization of results in
terms of Gaussian width has the advantage that this literature can be readily leveraged to derive new
recovery guarantees for matrix completion under suitable structural constraints (Appendix D.2).
In addition to the theoretical elegance of such a unified framework, identifying useful but potentially
nonâ€“decomposable low dimensional structures is of significant practical interest. The broad class
of structures enforced through symmetric convex bodies and symmetric atomic sets [10] can be
analyzed under this paradigm (Section 2.1). Such specialized structures can capture the constraints
in certain applications better than simple lowâ€“rankness. In particular, we discuss in detail, a nonâ€“
trivial example of the spectral kâ€“support norm introduced by McDonald et al. [25].
To summarize the key contributions of the paper:
â€¢ Theorem 1aâ€“1b provide unified upper bounds on sample complexity and estimation error for
matrix completion estimators using general norm regularization: a substantial generalization of the
existing results on matrix completion under structural constraints.
â€¢ Theorem 1a is applied to derive statistical results for the special case of matrix completion under
spectral kâ€“support norm regularization.
â€¢ An intermediate result, Theorem 5 shows that under any norm regularization, a variant of Re-
stricted Strong Convexity (RSC) holds in the matrix completion setting with extremely localized
measurements. Further, a certain partial measure of complexity of a set is encountered in matrix
completion analysis (12). Another intermediate result, Theorem 2 provides bounds on the par-
tial complexity measures in terms of a better understood complexity measure of Gaussian width.
These intermediate results are of independent interest beyond the scope of the paper.
Notations and Preliminaries
Indexes i, j are typically used to index rows and columns respectively of matrices, and index k is
used to index the observations. ei, ej , ek, etc. denote the standard basis in appropriate dimensionsâˆ—.
Notation G and g are used to denote a matrix and vector respectively, with independent standard
Gaussian random variables. P(.) and E(.) denote the probability of an event and the expectation of
a random variable, respectively. Given an integer N , let [N ] = {1, 2, . . . , N}. Euclidean norm in a
vector space is denoted as â€–xâ€–2 =
âˆš
ã€ˆx, xã€‰. For a matrix X with singular values Ïƒ1 â‰¥ Ïƒ2 â‰¥ . . .,
common norms include the Frobenius norm â€–Xâ€–F =
âˆšâˆ‘
i Ïƒ
2
i , the nuclear norm â€–Xâ€–âˆ— =
âˆ‘
i Ïƒi,
the spectral norm â€–Xâ€–op = Ïƒ1, and the maximum norm â€–Xâ€–âˆ = maxij |Xij |. Also let, Sd1d2âˆ’1 =
âˆ—for brevity we omit the explicit dependence of dimension unless necessary
2
{X âˆˆ Rd1Ã—d2 : â€–Xâ€–F = 1} and Bd1d2 = {X âˆˆ Rd1Ã—d2 : â€–Xâ€–F â‰¤ 1}. Finally, given a norm â€–.â€–
defined on a vectorspace V , its dual norm is given by â€–Xâ€–âˆ— = supâ€–Y â€–â‰¤1ã€ˆX,Y ã€‰.
Definition 1 (Gaussian Width). Gaussian width of a set S âŠ‚ Rd1Ã—d2 is a widely studied measure of
complexity of a subset in high dimensional ambient space and is given by:
wG(S) = EG sup
XâˆˆS
ã€ˆX,Gã€‰, (1)
where recall thatG is a matrix of independent standard Gaussian random variables. Some key results
on Gaussian width are discussed in Appendix D.2.
Definition 2 (Subâ€“Gaussian Random Variable [36]). The subâ€“Gaussian norm of a random variable
X is given by: â€–Xâ€–Î¨2 = suppâ‰¥1 pâˆ’1/2(E|X|p)1/p. X is bâ€“subâ€“Gaussian if â€–Xâ€–Î¨2 â‰¤ b <âˆ.
Equivalently, X is subâ€“Gaussian if one of the following conditions are satisfied for some constants
k1, k2, and k3 [Lemma 5.5 of [36]].
(1) âˆ€p â‰¥ 1, (E|X|p)1/p â‰¤ bâˆšp, (2) âˆ€t > 0, P(|X| > t) â‰¤ e1âˆ’t2/k21b2 ,
(3) E[ek2X2/b2 ] â‰¤ e, or (4) if EX = 0, then âˆ€s > 0, E[esX ] â‰¤ ek3s2b2/2.
Definition 3 (Restricted Strong Convexity (RSC)). A function L is said to satisfy Restricted Strong
Convexity (RSC) at Î˜ with respect to a subset S, if for some RSC parameter ÎºL > 0,
âˆ€âˆ† âˆˆ S,L(Î˜ + âˆ†)âˆ’ L(Î˜)âˆ’ ã€ˆâˆ‡L(Î˜),âˆ†ã€‰ â‰¥ ÎºLâ€–âˆ†â€–2F . (2)
Definition 4 (Spikiness Ratio [26]). For XâˆˆRd1Ã—d2 , a measure of its â€œspikinessâ€ is given by:
Î±sp(X) =
âˆš
d1d2â€–Xâ€–âˆ
â€–Xâ€–F
. (3)
Definition 5 (Norm Compatibility Constant [27]). The compatibility constant of a normR : V â†’ R
under a closed convex cone C âŠ‚ V is defined as follows:
Î¨R(C) = sup
XâˆˆC\{0}
R(X)
â€–Xâ€–F
. (4)
2 Structured Matrix Completion
Denote the ground truth target matrix as Î˜âˆ— âˆˆ Rd1Ã—d2 ; let d=d1+ d2. In the noisy matrix comple-
tion, observations consists of individual entries of Î˜âˆ— observed through an additive noise channel.
Subâ€“Gaussian Noise: Given, a list of independently sampled standard basis â„¦ = {Ek = eike>jk :
ik âˆˆ [d1], jk âˆˆ [d2]} with potential duplicates, observations (yk)k âˆˆ R|â„¦| are given by:
yk = ã€ˆÎ˜âˆ—, Ekã€‰+ Î¾Î·k, for k = 1, 2, . . . , |â„¦|, (5)
where Î· âˆˆ R|â„¦| is the noise vector of independent subâ€“Gaussian random variables with E[Î·k] = 0
and Var(Î·k) = 1, and Î¾2 is scaled variance of noise per observation. Further let â€–Î·kâ€–Î¨2 â‰¤ b for a
constant b (recall â€–.â€–Î¨2 from Definition 2). Also, without loss of generality, assume normalization
â€–Î˜âˆ—â€–F = 1.
Uniform Sampling: Assume that the entries in â„¦ are drawn independently and uniformly:
Ek âˆ¼ uniform{eie>j : i âˆˆ [d1], j âˆˆ [d2]}, for Ek âˆˆ â„¦. (6)
Let {ek} be the standard basis of R|â„¦|. Given â„¦, define Pâ„¦ : Rd1Ã—d2 â†’ R|â„¦| as:
Pâ„¦(X) =
âˆ‘|â„¦|
k=1ã€ˆX,Ekã€‰ek (7)
Structural Constraints For matrix completion with |â„¦| < d1d2, low dimensional structural con-
straints on Î˜âˆ— are necessary for wellâ€“posedness. We consider a generalized constraint setting
wherein for some lowâ€“dimensional model space M, Î˜âˆ— âˆˆ M is enforced through a surrogate
norm regularizer R(.). We make no further assumptions onR other than it being a norm in Rd1Ã—d2 .
Low Spikiness In matrix completion under uniform sampling model, further restrictions on Î˜âˆ— (be-
yond low dimensional structure) are required to ensure that the most informative entries of the matrix
are observed with high probability [8]. Early work assumed stringent matrix incoherence conditions
for lowâ€“rank completion to preclude such matrices [7, 18, 19], while more recent work [11, 26], re-
lax these assumptions to a more intuitive restriction of the spikiness ratio, defined in (3). However,
under this relaxation only an approximate recovery is typically guaranteed in lowâ€“noise regime, as
opposed to near exact recovery under incoherence assumptions [26, 11].
Assumption 1 (Spikiness Ratio). There exists Î±âˆ— > 0, such that
â€–Î˜âˆ—â€–âˆ = Î±sp(Î˜âˆ—)â€–Î˜
âˆ—â€–Fâˆš
d1d2
â‰¤ Î±
âˆ—
âˆš
d1d2
. 
3
2.1 Special Cases and Applications
We briefly introduce some interesting examples of structural constraints with practical applications.
Example 1 (Low Rank and Decomposable Norms). Lowâ€“rankness is the most common structure
used in many matrix estimation problems including collaborative filtering, PCA, spectral clustering,
etc. Convex estimators using nuclear norm â€–Î˜â€–âˆ— regularization has been widely studied statistically
[8, 7, 28, 26, 18, 19, 22, 11, 20, 21]. A recent work [16] extends the analysis of low rank matrix
completion to general decomposable norms, i.e. R :âˆ€X,Y âˆˆ(M,MâŠ¥),R(X+Y )=R(X)+R(Y ).
Example 2 (Spectral kâ€“support Norm). A nonâ€“trivial and significant example of norm regular-
ization that is not decomposable is the spectral kâ€“support norm recently introduced by McDon-
ald et al. [25]. Spectral kâ€“support norm is essentially the vector kâ€“support norm [2] applied on the
singular values Ïƒ(Î˜) of a matrix Î˜ âˆˆ Rd1Ã—d2 . Without loss of generality, let dÌ„ = d1 = d2.
Let Gk = {g âŠ† [dÌ„] : |g| â‰¤ k} be the set of all subsets [dÌ„] of cardinality at most k, and let
V(Gk) = {(vg)gâˆˆGk : vg âˆˆ RdÌ„, supp(vg) âŠ† g}. The spectral kâ€“support norm is given by:
â€–Î˜â€–kâ€“sp = inf
vâˆˆV(Gk)
{ âˆ‘
gâˆˆGk
â€–vgâ€–2 :
âˆ‘
gâˆˆGk
vg = Ïƒ(Î˜)
}
, (8)
McDonald et al. [25] showed that spectral kâ€“support norm is a special case of cluster norm [17]. It
was further shown that in multiâ€“task learning, wherein the tasks (columns of Î˜âˆ—) are assumed to be
clustered into dense groups, the cluster norm provides a tradeâ€“off between intraâ€“cluster variance,
(inverse) interâ€“cluster variance, and the norm of the task vectors. Both [17] and [25] demonstrate
superior empirical performance of cluster norms (and kâ€“support norm) over traditional trace norm
and spectral elastic net minimization on bench marked matrix completion and multiâ€“task learning
datasets. However, statistical analysis of consistent matrix completion using spectral kâ€“support
norm regularization has not been previously studied. In Section 3.2, we discuss the consequence of
our main theorem for this nonâ€“trivial special case.
Example 3 (Additive Decomposition). Elementwise sparsity is a common structure often assumed
in highâ€“dimensional estimation problems. However, in matrix completion, elementwise sparsity
conflicts with Assumption 1 (and more traditional incoherence assumptions). Indeed, it is easy to
see that with high probability most of the |â„¦|  d1d2 uniformly sampled observations will be
zero, and an informed prediction is infeasible. However, elementwise sparse structures can often
be modelled within an additive decomposition framework, wherein Î˜âˆ— =
âˆ‘
k Î˜
(k), such that each
component matrix Î˜(k) is in turn structured (e.g. low rank+sparse used for robust PCA [6]). In
such structures, there is no scope for recovering sparse components outside the observed indices,
and it is assumed that: Î˜(k) is sparseâ‡’ supp(Î˜(k)) âŠ† â„¦. In such cases, our results are applicable
under additional regularity assumptions that enforces nonâ€“spikiness on the superposed matrix. A
candidate norm regularizer for such structures is the weighted infimum convolution of individual
structure inducing norms [6, 39],
Rw(Î˜) = inf
{âˆ‘
k
wkRk(Î˜(k)) :
âˆ‘
k
Î˜(k) = Î˜
}
.
Example 4 (Other Applications). Other potential applications including cut matrices [30, 10], struc-
tures induced by compact convex sets, norms inducing structured sparsity assumptions on the spec-
trum of Î˜âˆ—, etc. can also be handled under the paradigm of this paper.
2.2 Structured Matrix Estimator
Let R be the norm surrogate for the structural constraints on Î˜âˆ—, and Râˆ— denote its dual norm. We
propose and analyze two convex estimators for the task of structured matrix completion:
Constrained Norm Minimizer
Î˜Ì‚cn = argmin
â€–Î˜â€–âˆâ‰¤ Î±
âˆ—âˆš
d1d2
R(Î˜) s.t. â€–Pâ„¦(Î˜)âˆ’ yâ€–2 â‰¤ Î»cn. (9)
Generalized Matrix Dantzig Selector
Î˜Ì‚ds = argmin
â€–Î˜â€–âˆâ‰¤ Î±
âˆ—âˆš
d1d2
R(Î˜) s.t.
âˆš
d1d2
|â„¦|
Râˆ—P âˆ—â„¦(Pâ„¦(Î˜)âˆ’ y) â‰¤ Î»ds, (10)
4
where recall that P âˆ—â„¦ : Râ„¦ â†’ Rd1Ã—d2 is the linear adjoint of Pâ„¦, i.e. ã€ˆPâ„¦(X), yã€‰ = ã€ˆX,P âˆ—â„¦(y)ã€‰.
Note: Theorem 1aâ€“1b gives consistency results for (9) and (10), respectively, under certain con-
ditions on the parameters Î»cn > 0, Î»ds > 0, and Î±âˆ— > 1. In particular, these conditions assume
knowledge of the noise variance Î¾2 and spikiness ratio Î±sp(Î˜âˆ—). In practice, typically Î¾ and Î±sp(Î˜âˆ—)
are unknown and the parameters are tuned by validating on held out data.
3 Main Results
We define the following â€œrestrictedâ€ error cone and its subset:
TR = TR(Î˜âˆ—) = cone{âˆ† : R(Î˜âˆ— + âˆ†) â‰¤ R(Î˜âˆ—)}, and ER = TR âˆ© Sd1d2âˆ’1. (11)
Let Î˜Ì‚cn and Î˜Ì‚ds be the estimates from (9) and (10), respectively. If Î»cn and Î»ds are chosen such that
Î˜âˆ— belongs to the feasible sets in (9) and (10), respectively, then the error matrices âˆ†Ì‚cn = Î˜Ì‚cnâˆ’Î˜âˆ—
and âˆ†Ì‚ds = Î˜Ì‚ds âˆ’Î˜âˆ— are contained in TR.
Theorem 1a (Constrained Norm Minimizer). Under the problem setup in Section 2, let Î˜Ì‚cn = Î˜âˆ—+
âˆ†Ì‚cn be the estimate from (9) with Î»cn = 2Î¾
âˆš
|â„¦|. For large enough c0, if |â„¦| > c20w2G(ER) log d,
then there exists an RSC parameter Îºc0 > 0 with Îºc0 â‰ˆ 1 âˆ’ o
(
1âˆš
log d
)
, and constants c1 and c2
such that, with probability greater than 1âˆ’exp(âˆ’c1w2G(ER))âˆ’2 exp(âˆ’c2w2G(ER) log d),
1
d1d2
â€–âˆ†Ì‚cnâ€–2F â‰¤4 max
{
Î¾2
Îºc0
,
Î±âˆ—2
d1d2
âˆš
c20w
2
G(ER) log d
|â„¦|
}
.
Theorem 1b (Matrix Dantzig Selector). Under the problem setup in Section 2, let Î˜Ì‚ds =
Î˜âˆ— + âˆ†Ì‚ds be the estimate from (10) with Î»ds â‰¥ 2Î¾
âˆš
d1d2
|â„¦| R
âˆ—P âˆ—â„¦(Î·). For large enough c0, if
|â„¦| > c20w2G(ER) log d, then there exists an RSC parameter Îºc0 > 0 with Îºc0 â‰ˆ 1 âˆ’ o
(
1âˆš
log d
)
,
and a constant c1 such that, with probability greater than 1âˆ’exp(âˆ’c1w2G(ER)),
1
d1d2
â€–âˆ†Ì‚dsâ€–2F â‰¤16 max
{
Î»2dsÎ¨
2
R(TR)
Îº2c0
,
Î±âˆ—2
d1d2
âˆš
c20w
2
G(ER) log d
|â„¦|
}
.
Recall Gaussian width wG and subspace compatibility constant Î¨R from (1) and (4), respectively.
Remarks:
1. If R(Î˜) = â€–Î˜â€–âˆ— and rank(Î˜âˆ—) = r, then w2G(ER) â‰¤ 3dr, Î¨R(TR) â‰¤ 2r andâˆš
d1d2
|â„¦| â€–P
âˆ—
â„¦(Î·)â€–2 â‰¤ 2
âˆš
d log d
|â„¦| w.h.p [10, 14, 26]. Using these bounds in Theorem 1b recovers
nearâ€“optimal results for low rank matrix completion under spikiness assumption [26].
2. For both estimators, upper bound on sample complexity is dominated by the square of Gaussian
width which is often considered the effective dimension of a subset in high dimensional space
and plays a key role in high dimensional estimation under Gaussian measurement ensembles.
The results show that, independent ofR(.), the upper bound on sample complexity for consistent
matrix completion with highly localized measurements is within a log d factor of the known
sample complexity of âˆ¼ w2G(ER) for estimation from Gaussian measurements [3, 10, 37, 5].
3. First term in estimation error bounds in Theorem 1aâ€“1b scales with Î¾2 which is the per observa-
tion noise variance (upto constant). The second term is an upper bound on error that arises due
to unidentifiability of Î˜âˆ— within a certain radius under the spikiness constraints [26]; in contrast
[7] show exact recovery when Î¾ = 0 using more stringent matrix incoherence conditions.
4. Bound on âˆ†Ì‚cn from Theorem 1a is comparable to the result by CandeÌs et al. [7] for low rank
matrix completion under nonâ€“lowâ€“noise regime, where the first term dominates, and those of [10,
35] for high dimensional estimation under Gaussian measurements. With a bound on w2G(ER), it
is easy to specialize this result for new structural constraints. However, this bound is potentially
loose and asymptotically converges to a constant error proportional to the noise variance Î¾2.
5. The estimation error bound in Theorem 1b is typically sharper than that in Theorem 1a. However,
for specific structures, using application of Theorem 1b requires additional bounds on Râˆ—P âˆ—â„¦(Î·)
and Î¨R(TR) besides w2G(ER).
5
3.1 Partial Complexity Measures
Recall that for wG(S) = E supXâˆˆSã€ˆX,Gã€‰ and R|â„¦| 3 g âˆ¼ N (0, I|â„¦|) is a standard normal vector.
Definition 6 (Partial Complexity Measures). Given a randomly sampled â„¦ = {Ek âˆˆ Rd1Ã—d2}, and
a centered random vector Î· âˆˆ R|â„¦|, the partial Î·â€“complexity measure of S is given by:
wâ„¦,Î·(S) = Eâ„¦,Î· sup
XâˆˆSâˆ’S
ã€ˆX,P âˆ—â„¦(Î·)ã€‰. (12)
Special cases of Î· being a vector of standard Gaussian g, or standard Rademacher  (i.e. k âˆˆ
{âˆ’1, 1} w.p. 1/2) variables, are of particular interest.
Note: In the case of symmetric Î·, like g and , wâ„¦,Î·(S) = 2Eâ„¦,Î· supXâˆˆSã€ˆX,P âˆ—â„¦(Î·)ã€‰, and the later
expression will be used interchangeably ignoring the constant term. 
Theorem 2 (Partial Gaussian Complexity). Let S âŠ† Bd1d2 with nonâ€“empty interior, and let â„¦ be
sampled according to (6). âˆƒ universal constants k1, k2, K1 and K2 such that:
wâ„¦,g(S) â‰¤ k1
âˆš
|â„¦|
d1d2
wG(S) + k2
âˆš
Eâ„¦ sup
X,Y âˆˆS
â€–Pâ„¦(X âˆ’ Y )â€–22
wâ„¦,g(S) â‰¤ K1
âˆš
|â„¦|
d1d2
wG(S) +K2 sup
X,Y âˆˆS
â€–X âˆ’ Y â€–âˆ.
(13)
Also, for centered i.i.d. subâ€“Gaussian vector Î· âˆˆ R|â„¦|, âˆƒ constant K3 s.t. wâ„¦,Î·(S) â‰¤ K3wâ„¦,g(S).
Note: For â„¦ ( [d1]Ã— [d2], the second term in (13) is a consequence of the localized measurements.
3.2 Spectral kâ€“Support Norm
We introduced spectral kâ€“support norm in Section 2.1. The estimators from (9) and (10) for spectral
kâ€“support norm can be efficiently solved via proximal methods using the proximal operators derived
in [25]. We are interested in the statistical guarantees for matrix completion using spectral kâ€“support
norm regularization. We extend the analysis for upper bounding the Gaussian width of the descent
cone for the vector kâ€“support norm by [29] to the case of spectral kâ€“support norm. WLOG let
d1 = d2 = dÌ„. Let Ïƒâˆ— âˆˆ RdÌ„ be the vector of singular values of Î˜âˆ— sorted in nonâ€“ascending order.
Let r âˆˆ {0, 1, 2, . . . , k âˆ’ 1} be the unique integer satisfying: Ïƒâˆ—kâˆ’râˆ’1 > 1r+1
âˆ‘p
i=kâˆ’r Ïƒ
âˆ—
i â‰¥ Ïƒâˆ—kâˆ’r.
Denote I2 = {1, 2, . . . , k âˆ’ r âˆ’ 1} and I1 = {k âˆ’ r, k âˆ’ r + 1, . . . , s}. Finally, for I âŠ† [dÌ„],
(Ïƒâˆ—I )i = 0 âˆ€i âˆˆ Ic, and (Ïƒâˆ—I )i = Ïƒâˆ—i âˆ€i âˆˆ I .
Lemma 3. If rank of Î˜âˆ— is s and ER is the error set forR(Î˜) = â€–Î˜â€–kâ€“sp, then
w2G(ER) â‰¤ s(2dÌ„âˆ’ s) +
( (r + 1)2â€–Ïƒâˆ—I2â€–22
â€–Ïƒâˆ—I1â€–
2
1
+ |I1|
)
(2dÌ„âˆ’ s).
Proof of the above lemma is provided in the appendix. Lemma 3 can be combined with Theorem 1a
to obtain recovery guarantees for matrix completion under spectral kâ€“support norm.
4 Discussions and Related Work
Sample Complexity: For consistent recovery in high dimensional convex estimation, it is desirable
that the descent cone at the target parameter Î˜âˆ— is â€œsmallâ€ relative to the feasible set (enforced by the
observations) of the estimator. Thus, it is not surprising that the sample complexity and estimation
error bounds of an estimator depends on some measure of complexity/size of the error cone at
Î˜âˆ—. Results in this paper are largely characterized in terms of a widely used complexity measure
of Gaussian width wG(.), and can be compared with the literature on estimation from Gaussian
measurements.
Error Bounds: Theorem 1a provides estimation error bounds that depends only on the Gaussian
width of the descent cone. In nonâ€“lowâ€“noise regime, this result is comparable to analogous results
of constrained norm minimization [6, 10, 35]. However, this bound is potentially loose owing to
mismatched dataâ€“fit term using squared loss, and asymptotically converges to a constant error pro-
portional to the noise variance Î¾2.
6
A tighter analysis on the estimation error can be obtained for the matrix Dantzig selector (10) from
Theorem 1b. However, application of Theorem 1b requires computing high probability upper bound
onRâˆ—P âˆ—â„¦(Î·). The literature on norms of random matrices [13, 24, 36, 34] can be exploited in com-
puting such bounds. Beside, in special cases: if R(.) â‰¥ Kâ€–.â€–âˆ—, then KRâˆ—(.) â‰¤ â€–.â€–op can be used
to obtain asymptotically consistent results.
Finally, under near zeroâ€“noise, the second term in the results of Theorem 1 dominates, and bounds
are weaker than that of [6, 19] owing to the relaxation of stronger incoherence assumption.
Related Work and Future Directions: The closest related work is the result on consistency of
matrix completion under decomposable norm regularization by [16]. Results in this paper are a strict
generalization to general norm regularized (not necessarily decomposable) matrix completion. We
provide nonâ€“trivial examples of application where structures enforced by such nonâ€“decomposable
norms are of interest. Further, in contrast to our results that are based on Gaussian width, the RSC
parameter in [16] depends on a modified complexity measure ÎºR(d, |â„¦|) (see definition in [16]). An
advantage of results based on Gaussian width is that, application of Theorem 1 for special cases can
greatly benefit from the numerous tools in the literature for the computation of wG(.).
Another closely related line of work is the nonâ€“asymptotic analysis of high dimensional estimation
under random Gaussian or subâ€“Gaussian measurements [10, 1, 35, 3, 37, 5]. However, the analysis
from this literature rely on variants of RIP of the measurement ensemble [9], which is not satisfied by
the the extremely localized measurements encountered in matrix completion[8]. In an intermediate
result, we establish a form of RSC for matrix completion under general norm regularization: a result
that was previously known only for nuclear norm and decomposable norm regularization.
In future work, it is of interest to derive matching lower bounds on estimation error for matrix
completion under general low dimensional structures, along the lines of [22, 5] and explore special
case applications of the results in the paper. We also plan to derive explicit characterization of Î»ds
in terms of Gaussian width of unit balls by exploiting generic chaining results for general Banach
spaces [33].
5 Proof Sketch
Proofs of the lemmas are provided in the Appendix.
5.1 Proof of Theorem 1
Define the following set of Î²â€“nonâ€“spiky matrices in Rd1Ã—d2 for constant c0 from Theorem 1:
A(Î²)=
{
X : Î±sp(X) =
âˆš
d1d2â€–Xâ€–âˆ
â€–Xâ€–F
< Î²
}
. (14)
Define, Î²2c0 =
âˆš
|â„¦|
c20w
2
G(ER) log d
(15)
Case 1: Spiky Error Matrix When the error matrix from (9) or (10) has large spikiness ratio,
following bound on error is immediate using â€–âˆ†Ì‚â€–âˆâ‰¤â€–Î˜Ì‚â€–âˆ+â€–Î˜âˆ—â€–âˆâ‰¤2Î±âˆ—/
âˆš
d1d2 in (3).
Proposition 4 (Spiky Error Matrix). For the constant c0 in Theorem 1a, if Î±sp(âˆ†Ì‚cn) /âˆˆ A(Î²c0), then
â€–âˆ†Ì‚cnâ€–2F â‰¤ 4Î±
âˆ—2
Î²2c0
= 4Î±âˆ—2
âˆš
c20w
2
G(ER) log d
|â„¦| . An analogous result also holds for âˆ†Ì‚ds. 
Case 2: Nonâ€“Spiky Error Matrix Let âˆ†Ì‚ds, âˆ†Ì‚cn âˆˆ A(Î²). Recall from (5), that y âˆ’ Pâ„¦(Î˜âˆ—) = Î¾Î·,
where Î· âˆˆ R|â„¦| consists of independent subâ€“Gaussian random variables with E[Î·k] = 0, Var(Î·k) =
1, and â€–Î·kâ€–Î¨2 â‰¤ b for a constant b.
5.1.1 Restricted Strong Convexity (RSC)
Recall TR and ER from (11). The most significant step in the proof of Theorem 1 involves showing
that over a useful subset of TR, a form of RSC (2) is satisfied by a squared loss penalty.
Theorem 5 (Restricted Strong Convexity). Let |â„¦| > c20w2G(ER) log d, for large enough constant
c0. There exists a RSC parameter Îºc0 > 0 with Îºc0 â‰ˆ 1 âˆ’ o
(
1âˆš
log d
)
, and a constant c1 such that,
the following holds w.p. greater that 1âˆ’ exp(âˆ’c1w2G(ER)),
7
âˆ€X âˆˆ TR âˆ© A(Î²c0),
d1d2
|â„¦|
â€–Pâ„¦(X)â€–22 â‰¥ Îºc0â€–Xâ€–2F .
Proof in Appendix A combines empirical process tools along with Theorem 2. 
5.1.2 Constrained Norm Minimizer
Lemma 6. Under the conditions of Theorem 1, let b be a constant such that âˆ€k, â€–Î·kâ€–Î¨2 â‰¤ b. There
exists a universal constant c2 such that, if Î»cnâ‰¥2Î¾
âˆš
|â„¦|, then w.p. greater than 1âˆ’ 2 exp (âˆ’c2|â„¦|),
(a) âˆ†Ì‚ds âˆˆ TR, and (b) â€–Pâ„¦(âˆ†Ì‚cn)â€–2â‰¤2Î»cn. 
Using Î»cn =2Î¾
âˆš
|â„¦| in (9), if âˆ†Ì‚cnâˆˆA(Î²c0), then using Theorem 5 and Lemma 6, w.h.p.
â€–âˆ†Ì‚cnâ€–2F
d1d2
â‰¤ 1
Îºc0
â€–Pâ„¦(âˆ†Ì‚cn)â€–22
|â„¦|
â‰¤4Î¾
2
Îºc0
. (16)
5.1.3 Matrix Dantzig Selector
Proposition 7. Î»dsâ‰¥Î¾
âˆš
d1d2
|â„¦| R
âˆ—P âˆ—â„¦(Î·)â‡’ w.h.p. (a) âˆ†Ì‚dsâˆˆTR; (b)
âˆš
d1d2
|â„¦| R
âˆ—P âˆ—â„¦(Pâ„¦(âˆ†Ì‚ds))â‰¤2Î»ds.
Above result follows from optimality of Î˜Ì‚ds and triangle inequality. Also,
âˆš
d1d2
|â„¦|
â€–Pâ„¦(âˆ†Ì‚ds)â€–22 â‰¤
âˆš
d1d2
|â„¦|
Râˆ—P âˆ—â„¦(Pâ„¦(âˆ†Ì‚ds))R(âˆ†Ì‚ds) â‰¤ 2Î»dsÎ¨R(TR)â€–âˆ†Ì‚dsâ€–F ,
where recall norm compatibility constant Î¨R(TR) from (4). Finally, using Theorem 5, w.h.p.
â€–âˆ†Ì‚dsâ€–2F
d1d2
â‰¤ 1
|â„¦|
â€–Pâ„¦(âˆ†Ì‚ds)â€–22
Îºc0
â‰¤4Î»dsÎ¨R(TR)
Îºc0
â€–âˆ†Ì‚dsâ€–Fâˆš
d1d2
. (17)
5.2 Proof of Theorem 2
Let the entries of â„¦ = {Ek = eike>jk : k = 1, 2, . . . , |â„¦|} be sampled as in (6). Recall that g âˆˆ R
|â„¦|
is a standard normal vector. For a compact S âŠ† Rd1Ã—d2 , it suffices to prove Theorem 2 for a dense
countable subset of S. Overloading S to such a countable subset, define following random process:
(Xâ„¦,g(X))XâˆˆS ,where Xâ„¦,g(X) = ã€ˆX,P âˆ—â„¦(g)ã€‰ =
âˆ‘
kã€ˆX,Ekã€‰gk. (18)
We start with a key lemma in the proof of Theorem 2. Proof of this lemma, provided in AppendixB,
uses tools from the broad topic of generic chaining developed in recent works [31, 33].
Lemma 8. For a compact subset S âŠ† Rd1Ã—d2 with nonâ€“empty interior, âˆƒ constants k1, k2 such that:
wâ„¦,g(S) = E sup
XâˆˆS
Xâ„¦,g(X) â‰¤ k1
âˆš
|â„¦|
d1d2
wG(S) + k2
âˆš
E sup
X,Y âˆˆS
â€–Pâ„¦(X âˆ’ Y )â€–22. 
Lemma 9. There exists constants k3, k4, such that for compact S âŠ† Bd1d2 with nonâ€“empty interior
E sup
X,Y âˆˆS
â€–Pâ„¦(X âˆ’ Y )â€–22 â‰¤ k3
|â„¦|
d1d2
w2G(S) + k4( sup
X,Y âˆˆS
â€–X âˆ’ Y â€–âˆ)wâ„¦,g(S)
Theorem 2 follows by combining Lemma 8 and Lemma 9, and simple algebraic manipulations usingâˆš
ab â‰¤ a/2 + b/2 and triangle inequality (See Appendix B.4).
The statement in Theorem 2 about partial subâ€“Gaussian complexity follows from a standard result
in empirical process given in Lemma 11 in the appendix. 
Acknowledgments We thank the anonymous reviewers for helpful comments and suggestions. S.
Gunasekar and J. Ghosh acknowledge funding from NSF grants IIS-1421729, IIS-1417697, and
IIS1116656. A. Banerjee acknowledges NSF grants IIS-1447566, IIS-1422557, CCF-1451986,
CNS-1314560, IIS-0953274, IIS-1029711, and NASA grant NNX12AQ39A.
8
References
[1] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: A geometric theory of phase
transitions in convex optimization. Inform. Inference, 2014.
[2] A. Argyriou, R. Foygel, and N. Srebro. Sparse prediction with the k-support norm. In NIPS, 2012.
[3] A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In NIPS, 2014.
[4] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. Clustering with bregman divergences. JMLR, 2005.
[5] T. Cai, T. Liang, and A. Rakhlin. Geometrizing local rates of convergence for linear inverse problems.
arXiv preprint, 2014.
[6] E. J. CandeÌs, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? ACM, 2011.
[7] E. J. CandeÌs and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 2010.
[8] E. J. CandeÌs and B. Recht. Exact matrix completion via convex optimization. FoCM, 2009.
[9] Emmanuel J Candes and Terence Tao. Decoding by linear programming. Information Theory, IEEE
Transactions on, 2005.
[10] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse
problems. Foundations of Computational Mathematics, 2012.
[11] M. A. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion. Inform. Inference, 2014.
[12] R. M. Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian processes. Journal
of Functional Analysis, 1967.
[13] A. Edelman. Eigenvalues and condition numbers of random matrices. Journal on Matrix Analysis and
Applications, 1988.
[14] M. Fazel, H Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order
system approximation. In American Control Conference, 2001.
[15] J. Forster and M. Warmuth. Relative expected instantaneous loss bounds. Journal of Computer and
System Sciences, 2002.
[16] S. Gunasekar, P. Ravikumar, and J. Ghosh. Exponential family matrix completion under structural con-
straints. In ICML, 2014.
[17] L. Jacob, J. P. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In NIPS, 2009.
[18] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. IT, 2010.
[19] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. JMLR, 2010.
[20] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 2014.
[21] O. Klopp. Matrix completion by singular value thresholding: sharp bounds. arXiv preprint arXiv, 2015.
[22] Vladimir Koltchinskii, Karim Lounici, Alexandre B Tsybakov, et al. Nuclear-norm penalization and
optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 2011.
[23] M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer, 1991.
[24] A. E. Litvak, A. Pajor, M. Rudelson, and N. Tomczak-Jaegermann. Smallest singular value of random
matrices and geometry of random polytopes. Advances in Mathematics, 2005.
[25] A. M. McDonald, M. Pontil, and D. Stamos. New perspectives on k-support and cluster norms. arXiv
preprint, 2014.
[26] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal
bounds with noise. JMLR, 2012.
[27] S. Negahban, B. Yu, M. J. Wainwright, and P. Ravikumar. A unified framework for high-dimensional
analysis of m-estimators with decomposable regularizers. In NIPS, 2009.
[28] B. Recht. A simpler approach to matrix completion. JMLR, 2011.
[29] E. Richard, G. Obozinski, and J.-P. Vert. Tight convex relaxations for sparse matrix factorization. In
ArXiv e-prints, 2014.
[30] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Learning Theory. Springer, 2005.
[31] M. Talagrand. Majorizing measures: the generic chaining. The Annals of Probability, 1996.
[32] M. Talagrand. Majorizing measures without measures. Annals of probability, 2001.
[33] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer, 2014.
[34] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational
Mathematics, 2012.
[35] J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. arXiv
preprint, 2014.
[36] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed sensing,
pages 210â€“268, 2012.
[37] R. Vershynin. Estimation in high dimensions: a geometric perspective. ArXiv e-prints, 2014.
[38] A. G. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and its
Applications, 1992.
[39] E. Yang and P. Ravikumar. Dirty statistical models. In NIPS, 2013.
9
