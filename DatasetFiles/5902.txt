


Paper ID = 5902
Title = Weighted Theta Functions and Embeddings with
Applications to Max-Cut, Clustering and
Summarization
Fredrik D. Johansson
Computer Science & Engineering
Chalmers University of Technology
GoÃàteborg, SE-412 96, Sweden
frejohk@chalmers.se
Ankani Chattoraj‚àó
Brain & Cognitive Sciences
University of Rochester
Rochester, NY 14627-0268, USA
achattor@ur.rochester.edu
Chiranjib Bhattacharyya
Computer Science and Automation
Indian Institute of Science
Bangalore 560012, Karnataka, India
chiru@csa.iisc.ernet.in
Devdatt Dubhashi
Computer Science & Engineering
Chalmers University of Technology
GoÃàteborg, SE-412 96, Sweden
dubhashi@chalmers.se
Abstract
We introduce a unifying generalization of the LovaÃÅsz theta function, and the as-
sociated geometric embedding, for graphs with weights on both nodes and edges.
We show how it can be computed exactly by semidefinite programming, and how
to approximate it using SVM computations. We show how the theta function can
be interpreted as a measure of diversity in graphs and use this idea, and the graph
embedding in algorithms for Max-Cut, correlation clustering and document sum-
marization, all of which are well represented as problems on weighted graphs.
1 Introduction
Embedding structured data, such as graphs, in geometric spaces, is a central problem in machine
learning. In many applications, graphs are attributed with weights on the nodes and edges ‚Äì infor-
mation that needs to be well represented by the embedding. LovaÃÅsz introduced a graph embedding
together with the famous theta function in the seminal paper [19], giving his celebrated solution to
the problem of computing the Shannon capacity of the pentagon. Indeed, LovaÃÅsz‚Äôs embedding is a
very elegant and powerful representation of unweighted graphs, that has come to play a central role
in information theory, graph theory and combinatorial optimization [10, 8]. However, despite there
being at least eight different formulations of œë(G) for unweighted graphs, see for example [20], there
does not appear to be a version that applies to graphs with weights on the edges. This is surprising,
as it has a natural interpretation in the information theoretic problem of the original definition [19].
A version of the LovaÃÅsz number for edge-weighted graphs, and a corresponding geometrical rep-
resentation, could open the way to new approaches to learning problems on data represented as
similarity matrices. Here we propose such a generalization for graphs with weights on both nodes
and edges, by combining a few key observations. Recently, Jethava et al. [14] discovered an inter-
esting connection between the original theta function and a central problem in machine learning,
namely the one class Support Vector Machine (SVM) formulation [14]. This kernel based method
gives yet another equivalent characterization of the LovaÃÅsz number. Crucially, it is easily modified
to yield an equivalent characterization of the closely related Delsarte version of the LovaÃÅsz number
‚àóThis work was performed when the author was affiliated with CSE at Chalmers University of Technology.
1
introduced by Schrijver [24] which is more flexible and often more convenient to work with. Using
this kernel characterization of the Delsarte version of LovaÃÅsz number, we define a theta function and
embedding of weighted graphs, suitable for learning with data represented as similarity matrices.
The original theta function is limited to applications on small graphs, because of its formulation as a
semidefinite program (SDP). In [14], Jethava et al. showed that their kernel characterization can be
used to compute a number and an embedding of a graph that are often good approximations to the
theta function and embedding, and that can be computed fast, scaling to very large graphs. Here we
give the analogous approximate method for weighted graphs. We use this approximation to solve
the weighted maximum cut problem faster than the classical SDP relaxation.
Finally, we show that our edge-weighted theta function has a natural interpretation as a measure of
diversity in graphs. We use this intuition to define a centroid-based correlation clustering algorithm
that automatically chooses the number of clusters and initializes the centroids. We also show how to
use the support vectors, computed in the kernel characterization with both node and edge weights,
to perform extractive document summarization.
To summarize our main contributions:
‚Ä¢ We introduce a unifying generalization of the famous LovaÃÅsz number applicable to graphs
with weights on both nodes and edges.
‚Ä¢ We show that via our characterization, we can compute a good approximation to our
weighted theta function and the corresponding embeddings using SVM computations.
‚Ä¢ We show that the weighted version of the LovaÃÅsz number can be interpreted as a measure
of diversity in graphs, and we use this to define a correlation clustering algorithm dubbed
œë-means that automatically a) chooses the number of clusters, and b) initializes centroids.
‚Ä¢ We apply the embeddings corresponding to the weighted LovaÃÅsz numbers to solve weighted
maximum cut problems faster than the classical SDP methods, with similar accuracy.
‚Ä¢ We apply the weighted kernel characterization of the theta function to document summa-
rization, exploiting both node and edge weights.
2 Extensions of LovaÃÅsz and Delsarte numbers for weighted graphs
Background Consider embeddings of undirected graphs G = (V,E). LovaÃÅsz introduced an el-
egant embedding, implicit in the definition of his celebrated theta function œë(G) [19], famously an
upper bound on the Shannon capacity and sandwiched between the independence number and the
chromatic number of the complement graph.
œë(G) = min
{ui},c
max
i
1
(c>ui)2
, u>i uj = 0, ‚àÄ(i, j) 6‚àà E, ‚Äñui‚Äñ = ‚Äñc‚Äñ = 1 . (1)
The vectors {ui}, c are so-called orthonormal representations or labellings, the dimension of which
is determined by the optimization. We refer to both {ui}, and the matrix U = [u1, . . . ,un] as an
embeddingG, and use the two notations interchangeably. Jethava et al. [14] introduced a characteri-
zation of the LovaÃÅsz œë function that established a close connection with the one-class support vector
machine [23]. They showed that, for an unweighted graph G = (V,E),
œë(G) = min
K‚ààK(G)
œâ(K), where (2)
K(G) := {K  0 | Kii = 1,Kij = 0,‚àÄ(i, j) 6‚àà E}, (3)
œâ(K) := max
Œ±‚â•0
f(Œ±;K), f(Œ±;K) := 2
‚àë
i
Œ±i ‚àí
‚àë
i,j
KijŒ±iŒ±j (4)
is the dual formulation of the one-class SVM problem, see [16]. Note that the conditions on K only
refer to the non-edges of G. In the sequel, œâ(K) and f(Œ±;K) always refer to the definitions in (4).
2.1 New weighted versions of œë(G)
A key observation in proving (2), is that the set of valid orthonormal representations is equivalent
to the set of kernels K. This equivalence can be preserved in a natural way when generalizing the
2
definition to weighted graphs: any constraint on the inner product uTi uj may be represented as
constraints on the elements Kij of the kernel matrix.
To define weighted extensions of the theta function, we need to first pass to the closely related
Delsarte version of the LovaÃÅsz number introduced by Schrijver [24]. In the Delsarte version, the
orthogonality constraint for non-edges is relaxed to uTi uj ‚â§ 0, (i, j) 6‚àà E. With reference to the
formulation (2) it is easy to observe that the Delsarte version is given by
œë1(G) = min
K‚ààK1(G)
œâ(K), where K1(G) := {K  0 | Kii = 1,Kij ‚â§ 0,‚àÄ(i, j) 6‚àà E} (5)
In other words, the LovaÃÅsz number corresponds to orthogonal labellings of G with orthogonal vec-
tors on the unit sphere assigned to non‚Äìadjacent nodes whereas the Delsarte version corresponds to
obtuse labellings, i.e. the vectors corresponding to non‚Äìadjacent nodes are vectors on the unit sphere
meeting at obtuse angles. In both cases, the corresponding number is essentially the half-angle of
the smallest spherical cap containing all the vectors assigned to the nodes. Comparing (2) and (5) it
follows that œë1(G) ‚â§ œë(G). In the sequel, we will use the Delsarte version and obtuse labellings to
define weighted generalizations of the theta function.
We observe in passing, that for any K ‚àà K1, and for any independent set I in the graph, taking
Œ±i = 1 if i ‚àà I and 0 otherwise,
œâ(K) ‚â• 2
‚àë
i
Œ±i ‚àí
‚àë
i,j
Œ±iŒ±jKij =
‚àë
i
Œ±i ‚àí
‚àë
i6=j
Œ±iŒ±jKij ‚â•
‚àë
i
Œ±i = |I| (6)
since for each term in the second sum, either (i, j) is an edge, in which case either Œ±i or Œ±j is zero,
or (i, j) is a non‚Äìedge in which case Kij ‚â§ 0. Thus, like œë(G), the Delsarte version œë1(G) is also
an upper bound on the stability or independence number Œ±(G).
Kernel characterization of theta functions on node-weighted graphs LovaÃÅsz number has a
classical extension to graphs with node weights œÉ = [œÉ1, . . . , œÉn]>, see for example [17]. The
generalization, in the Delsarte version (note the inequality constraint), is the following
œë(G,œÉ) = min
{ui},c
max
i
œÉi
(c>ui)2
, u>i uj ‚â§ 0, ‚àÄ(i, j) 6‚àà E, ‚Äñui‚Äñ = ‚Äñc‚Äñ = 1 . (7)
By passing to the dual of (7), see section 2.1 and [16], we may, as for unweighted graphs, character-
ize œë(G,œÉ) by a minimization over the set of kernels,
K(G,œÉ) := {K  0 | Kii = 1/œÉi,Kij ‚â§ 0,‚àÄ(i, j) 6‚àà E} (8)
and, just like in the unweighted case, œë1(G,œÉ) = minK‚ààK(G,œÉ) œâ(K). When œÉi = 1,‚àÄi ‚àà V , this
reduces to the unweighted case. We also note that for any K ‚àà K(G,œÉ) and for any independent
set I in the graph, taking Œ±i = œÉi if i ‚àà I and 0 otherwise,
œâ(K) ‚â• 2
‚àë
i
Œ±i ‚àí
‚àë
i,j
Œ±iŒ±jKij = 2
‚àë
i‚ààI
œÉi ‚àí
‚àë
i‚ààI
œÉ2i
œÉi
‚àí
‚àë
i 6=j
Œ±iŒ±jKij ‚â•
‚àë
i‚ààI
œÉi , (9)
since Kij ‚â§ 0 ‚àÄ(i, j) 6‚àà E. Thus, œë1(G,œÉ) ‚â• œâ(K) is an upper bound on the weight of the
maximum-weight independent set.
Extension to edge-weighted graphs The kernel characterization of œë1(G) allows one to define a
natural extension to data given as similarity matrices represented in the form of a weighted graph
G = (V, S). Here, S is a similarity function on (unordered) node pairs, and S(i, j) ‚àà [0, 1] with +1
representing complete similarity and 0 complete dissimilarity. The obtuse labellings corresponding
to the Delsarte version are somewhat more flexible even for unweighted graphs, but is particularly
well suited for weighted graphs. We define
œë1(G,S) := min
K‚ààK(G,S)
œâ(K) where K(G,S) := {K  0 | Kii = 1,Kij ‚â§ Sij} (10)
In the case of an unweighted graph, where Sij ‚àà {0, 1}, this reduces exactly to (5).
3
Table 1: Characterizations of weighted theta functions. In the first row are characterizations follow-
ing the original definition. In the second are kernel characterizations. The bottom row are versions
of the LS-labelling [14]. In all cases, ‚Äñui‚Äñ = ‚Äñc‚Äñ = 1. A refers to the adjacency matrix of G.
Unweighted Node-weighted Edge-weighted
min
{ui}
min
c
max
i
1
(c>ui)2
u>i uj ‚â§ 0, ‚àÄ(i, j) 6‚àà E
min
{ui}
min
c
max
i
œÉi
(c>ui)2
u>i uj = 0, ‚àÄ(i, j) 6‚àà E
min
{ui}
min
c
max
i
1
(c>ui)2
u>i uj ‚â§ Sij , i 6= j
KG = {K  0 | Kii = 1,
Kij = 0, ‚àÄ(i, j) 6‚àà E}
KG,œÉ = {K  0 | Kii = 1/œÉi,
Kij = 0, ‚àÄ(i, j) 6‚àà E}
KG,S = {K  0 | Kii = 1,
Kij ‚â§ Sij , i 6= j}
KLS =
A
|Œªn(A)|
+ I KœÉLS =
A
œÉmax|Œªn(A)|
+diag(œÉ)‚àí1 K
S
LS =
S
|Œªn(S)|
+ I
Unifying weighted generalization We may now combine both node and edge weights to form a
fully general extension to the Delsarte version of the LovaÃÅsz number,
œë1(G,œÉ, S) = min
K‚ààK(G,œÉ,S)
œâ(K), K(G,œÉ, S) :=
{
K  0 | Kii =
1
œÉi
,Kij ‚â§
Sij‚àö
œÉiœÉj
}
(11)
It is easy to see that for unweighted graphs, Sij ‚àà {0, 1}, œÉi = 1, the definition reduces to the
Delsarte version of the theta function in (5). œë1(G,œÉ, S) is hence a strict generalization of œë1(G).
All the proposed weighted extensions are defined by the same objective, œâ(K). The only difference
is the set K, specialized in various ways, over which the minimum, minK‚ààK œâ(K), is computed.
It also is important to note, that with the generalization of the theta function comes an implicit
generalization of the geometric representation of G. Specifically, for any feasible K in (11), there
is an embedding U = [u1, . . . ,un] such that K = U>U with the properties u>i uj
‚àö
œÉiœÉj ‚â§ Sij ,
‚Äñui‚Äñ2 = 1/
‚àö
œÉi, which can be retrieved using matrix decomposition. Note that u>i uj
‚àö
œÉiœÉj is
exactly the cosine similarity between ui and uj , which is a very natural choice when Sij ‚àà [0, 1].
The original definition of the (Delsarte) theta function and its extensions, as well as their kernel
characterizations, can be seen in table 1. We can prove the equivalence of the embedding (top) and
kernel characterizations (middle) using the following result.
Proposition 2.1. For any embedding U ‚àà Rd√ón with K = U>U , and f in (4), the following holds
min
c‚ààSd‚àí1
max
i
1
(c>ui)2
= max
Œ±i‚â•0
f(Œ±;K) . (12)
Proof. The result is given as part of the proof of Theorem 3 in Jethava et al. [14]. See also [16].
As we have already established in section 2 that any set of geometric embeddings have a characteri-
zation as a set of kernel matrices, it follows that the minimizing the LHS in (12) over a (constrained)
set of orthogonal representations, {ui}, is equivalent to minimizing the RHS over a kernel set K.
3 Computation and fixed-kernel approximation
The weighted generalization of the theta function, œë1(G,œÉ, S), defined in the previous section, may
be computed as a semidefinite program. In fact œë1(G,œÉ, S) = 1/(t‚àó)2 for t‚àó, the solution to the
following problem. For details, see [16]. With Sk+ the set of k √ó k symmetric p.s.d. matrices,
maximize
X
t subject to X ‚àà Sn+1+
Xi,n+1 ‚â• t, Xii = 1/œÉi, i ‚àà [n]
Xij ‚â§ Sij/
‚àö
œÉiœÉj , i 6= j, i, j ‚àà [n] .
(13)
4
While polynomial in time complexity [13], solving the SDP is too slow in many cases. To address
this, Jethava et al. [14] introduced a fast approximation to (the unweighted) œë(G), dubbed SVM-
theta. They showed that in some cases, the minimization over K in (2) can be replaced by a fixed
choice of K, while causing just a constant-factor error. Specifically, for unweighted graphs with
adjacency matrix A, Jethava et al. [14] defined the so called LS-labelling, KLS(G) = A/|Œªn(A)|+
I , and showed that for large families of graphs œë(G) ‚â§ œâ(KLS(G)) ‚â§ Œ≥œë(G), for a constant Œ≥.
We extend the LS-labelling to weighted graphs. For graphs with edge weights, represented by
a similarity matrix S, the original definition may be used, with S substituted for A. For node
weighted graphs we also must satisfy the constraint Kii = 1/œÉi, see (8). A natural choice, still
ensuring positive semidefiniteness is,
KLS(G,œÉ) =
A
œÉmax|Œªn(A)|
+ diag(œÉ)‚àí1 (14)
where diag(œÉ)‚àí1 is the diagonal matrix Œ£ with elements Œ£ii = 1/œÉi, and œÉmax = maxni=1 œÉi. Both
weighted versions of the LS-labelling are presented in table 1. The fully generalized labelling, for
graphs with weights on both nodes and edges, KLS(G,œÉ, S) can be obtained by substituting S for
A in (14). As with the exact characterization, we note that KLS(G,œÉ, S) reduces to KLS(G) for
the uniform case, Sij ‚àà {0, 1}, œÉi = 1. For all versions of the LS-labelling of G, as with the exact
characterization, a geometric embedding U may be obtained from KLS using matrix decompotion.
3.1 Computational complexity
Solving the full problem in the kernel characterization (11), is not faster than the computing the
SDP characterization (13). However, for a fixed K, the one-class SVM can be solved in O(n2)
time [12]. Retrieving the embedding U : K = UTU may be done using Cholesky or singular value
decomposition (SVD). In general, algorithms for these problems have complexity O(n3). However,
in many cases, a rank d approximation to the decomposition is sufficient, see for example [9]. A thin
(or truncated) SVD corresponding to the top d singular values may be computed in O(n2d) time [5]
for d = O(
‚àö
n). The remaining issue is the computation of K. The complexity of computing the
LS-labelling discussed in the previous section is dominated by the computation of the minimum
eigenvalue Œªn(A). This can be done approximately in OÃÉ(m) time, where m is the number of edges
of the graph [1]. Overall, the complexity of computing both the embedding U and œâ(K) is O(dn2).
4 The theta function as diversity in graphs: œë-means clustering
In section 2, we defined extensions of the Delsarte version of the LovaÃÅsz number, œë1(G) and the
associated geometric embedding, for weighted graphs. Now we wish to show how both œë(G) and
the geometric embedding are useful for solving common machine learning tasks. We build on an
intuition of œë(G) as a measure of diversity in graphs, illustrated here by a few simple examples. For
complete graphs Kn, it is well known that œë(Kn) = 1, and for empty graphs Kn, œë(Kn) = n.
We may interpret these graphs as having 1 and n clusters respectively. Graphs with several disjoint
clusters make a natural middle-ground. For a graphG that is a union of k disjoint cliques, œë(G) = k.
Now, consider the analogue of (6) for graphs with edge weights Sij . For any K ‚àà K(G,S) and for
any subset H of nodes, let Œ±i = 1 if i ‚àà H and 0 otherwise. Then, since Kij ‚â§ Sij ,
2
‚àë
i
Œ±i ‚àí
‚àë
ij
Œ±iŒ±jKij =
‚àë
i
Œ±i ‚àí
‚àë
i 6=j
Œ±iŒ±jKij ‚â• |H| ‚àí
‚àë
i 6=j,i,j‚ààH
Sij .
Maximizing this expression may be viewed as the trade-off of finding a subset of nodes that is both
large and diverse; the objective function is the size of the set subjected to a penalty for non‚Äìdiversity.
In general support vector machines, non-zero support values Œ±i correspond to support vectors, defin-
ing the decision boundary. As a result, nodes i ‚àà V with high values Œ±i may be interpreted as an
important and diverse set of nodes.
4.1 œë-means clustering
A common problem related to diversity in graphs is correlation clustering [3]. In correlation clus-
tering, the task is to cluster a set of items V = {1, . . . , n}, based on their similarity, or correlation,
5
Algorithm 1 œë-means clustering
1: Input: Graph G, with weight matrix S and node weights œÉ.
2: Compute kernel K ‚àà K(G,œÉ, S)
3: Œ±‚àói ‚Üê arg maxŒ±i f(Œ±;K), as in (4)
4: Sort alphas according to ji such that Œ±j1 ‚â• Œ±j2 ‚â• ... ‚â• Œ±jn
5: Let k = dœëÃÇe where œëÃÇ‚Üê œâ(K) = f(Œ±‚àó;K)
6: either a)
7: Initialize labels Zi = arg maxj‚àà{j1,...,jk}Kij
8: Output: result of kernel k-means with kernel K, k = dœëÃÇe and Z as initial labels
9: or b)
10: Compute U : K = UTU , with columns Ui, and let C ‚Üê {Uji : i ‚â§ k}
11: Output: result of k-means with k = dœëÃÇe and C as initial cluster centroids
S : V √ó V ‚Üí Rn√ón, without specifying the number of clusters beforehand. This is naturally posed
as a problem of clustering the nodes of an edge-weighted graph. In a variant called overlapping
correlation clustering [4], items may belong to several, overlapping, clusters. The usual formulation
of correlation clustering is an integer linear program [3]. Making use of geometric embeddings, we
may convert the graph clustering problem to the more standard problem of clustering a set of points
{ui}ni=1 ‚àà Rd√ón, allowing the use of an arsenal of established techniques, such as k-means cluster-
ing. However, we remind ourselves of two common problems with existing clustering algorithms.
Problem 1: Number of clusters Many clustering algorithms relies on the user making a good
choice of k, the number of clusters. As this choice can have dramatic effect on both the accuracy
and speed of the algorithm, heuristics for choosing k, such as Pham et al. [22], have been proposed.
Problem 2: Initialization Popular clustering algorithms such as Lloyd‚Äôs k-means, or expectation-
maximization for Gaussian mixture models require an initial guess of the parameters. As a result,
these algorithms are often run repeatedly with different random initializations.
We propose solutions to both problems based on œë1(G). To solve Problem 1, we choose k =
dœë1(G)e. This is motivated by œë1(G) being a measure of diversity. For Problem 2, we propose
initializing parameters based on the observation that the non-zero Œ±i are support vectors. Specifi-
cally, we let the initial clusters by represented by the set of k nodes, I ‚äÇ V , with the largest Œ±i. In
k-means clustering, this corresponds to letting the initial centroids be {ui}i‚ààI . We summarize these
ideas in algorithm 1, comprising both œë-means and kernel œë-means clustering.
In section 3.1, we showed that computating the approximate weighted theta function and embedding,
can be done in O(dn2) time for a rank d = O(
‚àö
n) approximation to the SVD. As is well-known,
Lloyd‚Äôs algorithm has a very high worst-case complexity and will dominate the overall complexity.
5 Experiments
5.1 Weighted Maximum Cut
The maximum cut problem (Max-Cut), a fundamental problem in graph algorithms, with applica-
tions in machine learning [25], has famously been solved using geometric embeddings defined by
semidefinite programs [9]. Here, given a graph G, we compute an embedding U ‚àà Rd√ón, the
SVM-theta labelling in [15], using the LS-labelling, KLS . To reduce complexity, while preserving
accuracy [9], we use a rank d =
‚àö
2n truncated SVD, see section 3.1. We apply the Goemans-
Williamson random hyperplane rounding [9] to partition the embedding into two sets of points,
representing the cut. The rounding was repeated 5000 times, and the maximum cut is reported.
Helmberg & Rendl [11] constructed a set of 54 graphs, 24 of which are weighted, that has since
often been used as benchmarks for Max-Cut. We use the six of the weighted graphs for which there
are multiple published results [6, 21]. Our approach is closest to that of the SDP-relaxation, which
6
Table 2: Weighted maximum cut. c is the weight of the produced cut.
SDP [6] SVM-œë Best known [21]
Graph c Time c Time c Time
G11 528 165s 522 3.13s 564 171.8s
G12 522 145s 518 2.94s 556 241.5s
G13 542 145s 540 2.97s 580 227.5s
G32 1280 1318s 1286 35.5s 1398 900.6s
G33 1248 1417s 1260 36.4s 1376 925.6s
G34 1264 1295s 1268 37.9s 1372 925.6s
Table 3: Clustering of the (mini) newsgroup dataset. Average (and std. deviation) over 5 splits. kÃÇ is
the average number of clusters predicted. The true number is k = 16.
F1 kÃÇ Time
VOTE/BOEM 31.29¬± 4.0 124 8.7m
PIVOT/BOEM 30.07¬± 3.4 120 14m
BEST/BOEM 29.67¬± 3.4 112 13m
FIRST/BOEM 26.76¬± 3.8 109 14m
k-MEANS+RAND 17.31¬± 1.3 2 15m
k-MEANS+INIT 20.06¬± 6.8 3 5.2m
œë-MEANS+RAND 35.60¬± 4.3 25 45s
œë-MEANS 36.20¬± 4.9 25 11s
has time complexity O(mn log2 n/3) [2]. In comparison, our method takes O(n2.5) time, see sec-
tion 3.1. The results are presented in table 2. For all graphs, the SVM approximation is comparable
to or better than the SDP solution, and considerably faster than the best known method [21].1
5.2 Correlation clustering
We evaluate several different versions of algorithm 1 in the task of correlation clustering, see sec-
tion 4.1. We consider a) the full version (œë-MEANS), b) one with k = dœëÃÇe but random initialization
of centroids (œë-MEANS+RAND), c) one with Œ±-based initialization but choosing k according to Pham
et al. [22] (k-MEANS+INIT) and d) k according to [22] and random initialization (k-MEANS+RAND).
For the randomly initialized versions, we use 5 restarts of k-means++. In all versions, we cluster the
points of the embedding defined by the fixed kernel (LS-labelling) K = KLS(G,S).
Elsner & Schudy [7] constructed five affinity matrices for a subset of the classical 20-newsgroups
dataset. Each matrix, corresponding to a different split of the data, represents the similarity between
messages in 16 different newsgroups. The task is to cluster the messages by their respective news-
group. We run algorithm 1 on every split, and compute the F1-score [7], reporting the average and
standard deviation over all splits, as well as the predicted number of clusters, kÃÇ. We compare our
results to several greedy methods described by Elsner & Schudy [7], see table 3. We only compare
to their logarithmic weighting schema, as the difference to using additive weights was negligible [7].
The results are presented in table 3. We observe that the full œë-means method achieves the highest
F1-score, followed by the version with random initialization (instead of using embeddings of nodes
with highest Œ±i, see algorithm 1). We note also that choosing k by the method of Pham et al. [22]
consistently results in too few clusters, and with the greedy search methods, far too many.
5.3 Overlapping Correlation Clustering
Bonchi et al. [4] constructed a benchmark for overlapping correlation clustering based on two
datasets for multi-label classification, Yeast and Emotion. The datasets consist of 2417 and 593
items belonging to one or more of 14 and 6 overlapping clusters respectively. Each set can be repre-
sented as an n√ó k binary matrix L, where k is the number of clusters and n is the number of items,
1Note that the timing results for the SDP method are from the original paper, published in 2001.
7
Table 4: Clustering of the Yeast and Emotion datasets. ‚Ä†The total time for finding the best solution.
The times for OCC-ISECT for a single k was 2.21s and 80.4s respectively.
Emotion Yeast
Prec. Rec. F1 Time Prec. Rec. F1 Time
OCC-ISECT [4] 0.98 1 0.99 12.1‚Ä† 0.99 1.00 1.00 716s‚Ä†
œë-means (no k-means) 1 1 1 0.34s 0.94 1 0.97 6.67s
such that Lic = 1 iff item i belongs to cluster c. From L, a weight matrix S is defined such that Sij
is the Jaccard coefficient between rows i and j of L. S is often sparse, as many of the pairs do not
share a single cluster. The correlation clustering task is to reconstruct L from S.
Here, we use only the centroids C = {uj1 , ...,ujk} produced by algorithm 1, without running k-
means. We let each centroid c = 1, ..., k represent a cluster, and assign a node i ‚àà V to that cluster,
i.e. LÃÇic = 1, iff uTi ujc > 0. We compute the precision and recall following Bonchi et al. [4]. For
comparison with Bonchi et al. [4], we run their algorithm called OCC-ISECT with the parameter kÃÑ,
bounding the number of clusters, in the interval 1, ..., 16 and select the one resulting in lowest cost.
The results are presented in table 4. For Emotion and Yeast, œë-means estimated the number of
clusters, k to be 6 (the correct number) and 8 respectively. For OCC-Isect, the k with the lowest
cost were 10 and 13. We note that while very similar in performance, the œë-means algorithms is
considerably faster than OCC-ISECT, especially when k is unknown.
5.4 Document summarization
Finally, we briefly examine the idea of using Œ±i to select a both relevant and diverse set of items, in a
very natural application of the weighted theta function ‚Äì extractive summarization [18]. In extractive
summarization, the goal is to automatically summarize a text by picking out a small set of sentences
that best represents the whole text. We may view the sentences of a text as the nodes of a graph, with
edge weights Sij , the similarity between sentences, and node weights œÉi representing the relevance
of the sentence to the text as a whole. The trade-off between brevity and relevance described above
can then be viewed as finding a set of nodes that has both high total weight and high diversity. This is
naturally accomplished using our framework by computing [Œ±‚àó1, . . . , Œ±
‚àó
n]
> = arg maxŒ±i>0 f(Œ±;K)
for fixed K = KLS(G,œÉ, S) and picking the sentences with the highest Œ±‚àói .
We apply this method to the multi-document summarization task of DUC-042. We let Sij be the
TF-IDF sentence similarity described by Lin & Bilmes [18], and let œÉi = (
‚àë
j Sij)
2. State-of-the-
art systems, purpose-built for summarization, achieve around 0.39 in recall and F1 score [18]. Our
method achieves a score of 0.33 on both measures which is about the same as the basic version
of [18]. This is likely possible to improve by tuning the trade-off between relevance and diversity,
such as a making a more sophisticated choice of S and œÉ. However, we leave this to future work.
6 Conclusions
We have introduced a unifying generalization of LovaÃÅsz‚Äôs theta function and the corresponding
geometric embedding to graphs with node and edge weights, characterized as a minimization over
a constrained set of kernel matrices. This allows an extension of a fast approximation of the LovaÃÅsz
number to weighted graphs, defined by an SVM problem for a fixed kernel matrix. We have shown
that the theta function has a natural interpretation as a measure of diversity in graphs, a useful
function in several machine learning problems. Exploiting these results, we have defined algorithms
for weighted maximum cut, correlation clustering and document summarization.
Acknowledgments
This work is supported in part by the Swedish Foundation for Strategic Research (SSF).
2http://duc.nist.gov/duc2004/
8
References
[1] S. Arora, E. Hazan, and S. Kale. Fast algorithms for approximate semidefinite programming using the
multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th
Annual IEEE Symposium on, pages 339‚Äì348. IEEE, 2005.
[2] S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta-algorithm and
applications. Theory of Computing, 8(1):121‚Äì164, 2012.
[3] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. Machine Learning, 56(1-3):89‚Äì113, 2004.
[4] F. Bonchi, A. Gionis, and A. Ukkonen. Overlapping correlation clustering. Knowledge and information
systems, 35(1):1‚Äì32, 2013.
[5] M. Brand. Fast low-rank modifications of the thin singular value decomposition. Linear algebra and its
applications, 415(1):20‚Äì30, 2006.
[6] S. Burer and R. D. Monteiro. A projected gradient algorithm for solving the maxcut sdp relaxation.
Optimization methods and Software, 15(3-4):175‚Äì200, 2001.
[7] M. Elsner and W. Schudy. Bounding and comparing methods for correlation clustering beyond ilp. In
Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages
19‚Äì27. Association for Computational Linguistics, 2009.
[8] M. X. Goemans. Semidefinite programming in combinatorial optimization. Math. Program., 79:143‚Äì161,
1997.
[9] M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut and sat-
isfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6):1115‚Äì1145,
1995.
[10] M. GroÃàtschel, L. LovaÃÅsz, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization,
volume 2 of Algorithms and Combinatorics. Springer, 1988.
[11] C. Helmberg and F. Rendl. A spectral bundle method for semidefinite programming. SIAM Journal on
Optimization, 10(3):673‚Äì696, 2000.
[12] D. Hush, P. Kelly, C. Scovel, and I. Steinwart. Qp algorithms with guaranteed accuracy and run time for
support vector machines. Journal of Machine Learning Research, 7:733‚Äì769, 2006.
[13] G. Iyengar, D. J. Phillips, and C. Stein. Approximating semidefinite packing programs. SIAM Journal on
Optimization, 21(1):231‚Äì268, 2011.
[14] V. Jethava, A. Martinsson, C. Bhattacharyya, and D. Dubhashi. LovaÃÅsz œë function, svms and finding
dense subgraphs. The Journal of Machine Learning Research, 14(1):3495‚Äì3536, 2013.
[15] V. Jethava, J. Sznajdman, C. Bhattacharyya, and D. Dubhashi. Lovasz œë, svms and applications. In
Information Theory Workshop (ITW), 2013 IEEE, pages 1‚Äì5. IEEE, 2013.
[16] F. D. Johanson, A. Chattoraj, C. Bhattacharyya, and D. Dubhashi. Supplementary material, 2015.
[17] D. E. Knuth. The sandwich theorem. Electr. J. Comb., 1, 1994.
[18] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In Proc. of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-
Volume 1, pages 510‚Äì520. Association for Computational Linguistics, 2011.
[19] L. LovaÃÅsz. On the shannon capacity of a graph. IEEE Transactions on Information Theory, 25(1):1‚Äì7,
1979.
[20] L. LovaÃÅsz and K. Vesztergombi. Geometric representations of graphs. Paul Erdos and his Mathematics,
1999.
[21] R. Martƒ±ÃÅ, A. Duarte, and M. Laguna. Advanced scatter search for the max-cut problem. INFORMS
Journal on Computing, 21(1):26‚Äì38, 2009.
[22] D. T. Pham, S. S. Dimov, and C. Nguyen. Selection of k in k-means clustering. Proceedings of the
Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science, 219(1):103‚Äì
119, 2005.
[23] B. SchoÃàlkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of
a high-dimensional distribution. Neural computation, 13(7):1443‚Äì1471, 2001.
[24] A. Schrijver. A comparison of the delsarte and lovaÃÅsz bounds. Information Theory, IEEE Transactions
on, 25(4):425‚Äì429, 1979.
[25] J. Wang, T. Jebara, and S.-F. Chang. Semi-supervised learning using greedy max-cut. The Journal of
Machine Learning Research, 14(1):771‚Äì800, 2013.
9
