


Paper ID = 5726
Title = Regularization-Free Estimation in Trace Regression with
Symmetric Positive Semidefinite Matrices
Martin Slawski Ping Li
Department of Statistics & Biostatistics
Department of Computer Science
Rutgers University
Piscataway, NJ 08854, USA
{martin.slawski@rutgers.edu,
pingli@stat.rutgers.edu}
Matthias Hein
Department of Computer Science
Department of Mathematics
Saarland University
Saarbrücken, Germany
hein@cs.uni-saarland.de
Abstract
Trace regression models have received considerable attention in the context of
matrix completion, quantum state tomography, and compressed sensing. Esti-
mation of the underlying matrix from regularization-based approaches promoting
low-rankedness, notably nuclear norm regularization, have enjoyed great popular-
ity. In this paper, we argue that such regularization may no longer be necessary
if the underlying matrix is symmetric positive semidefinite (spd) and the design
satisfies certain conditions. In this situation, simple least squares estimation sub-
ject to an spd constraint may perform as well as regularization-based approaches
with a proper choice of regularization parameter, which entails knowledge of the
noise level and/or tuning. By contrast, constrained least squares estimation comes
without any tuning parameter and may hence be preferred due to its simplicity.
1 Introduction
Trace regression models of the form
yi = tr(X
⊤
i Σ
∗) + εi, i = 1, . . . , n, (1)
where Σ∗ ∈ Rm1×m2 is the parameter of interest to be estimated given measurement matricesXi ∈
R
m1×m2 and observations yi contaminated by errors εi, i = 1, . . . , n, have attracted considerable
interest in high-dimensional statistical inference, machine learning and signal processing over the
past few years. Research in these areas has focused on a setting with fewmeasurementsn ≪ m1 ·m2
and Σ∗ being (approximately) of low rank r ≪ min{m1,m2}. Such setting is relevant to problems
such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and
phase retrieval [7]. A common thread in these works is the use of the nuclear norm of a matrix
as a convex surrogate for its rank [18] in regularized estimation amenable to modern optimization
techniques. This approach can be seen as natural generalization of ℓ1-norm (aka lasso) regularization
for the linear regression model [24] that arises as a special case of model (1) in which both Σ∗ and
{Xi}ni=1 are diagonal. It is inarguable that in general regularization is essential if n < m1 ·m2.
The situation is less clear if Σ∗ is known to satisfy additional constraints that can be incorporated in
estimation. Specifically, in the present paper we consider the case in which m1 = m2 = m and Σ
∗
is known to be symmetric positive semidefinite (spd), i.e. Σ∗ ∈ Sm+ with Sm+ denoting the positive
semidefinite cone in the space of symmetric realm×m matrices Sm. The set Sm+ deserves specific
interest as it includes covariance matrices and Gram matrices in kernel-based learning [20]. It is
rather common for these matrices to be of low rank (at least approximately), given the widespread
use of principal components analysis and low-rank kernel approximations [28]. In the present paper,
we focus on the usefulness of the spd constraint for estimation. We argue that if Σ∗ is spd and the
measurement matrices {Xi}ni=1 obey certain conditions, constrained least squares estimation
min
Σ∈Sm
+
1
2n
n∑
i=1
(yi − tr(X⊤i Σ))2 (2)
may perform similarly well in prediction and parameter estimation as approaches employing nuclear
norm regularization with proper choice of the regularization parameter, including the interesting
1
regime n < δm, where δm = dim(S
m) = m(m+ 1)/2. Note that the objective in (2) only consists
of a data fitting term and is hence convenient to work with in practice since there is no free parameter.
Our findings can be seen as a non-commutative extension of recent results on non-negative least
squares estimation for linear regression [16, 21].
Related work. Model (1) with Σ∗ ∈ Sm+ has been studied in several recent papers. A good deal
of these papers consider the setup of compressed sensing in which the {Xi}ni=1 can be chosen by
the user, with the goal to minimize the number of observations required to (approximately) recover
Σ∗. For example, in [27], recovery of Σ∗ being low-rank from noiseless observations (εi = 0,
i = 1, . . . , n) by solving a feasibility problem over Sm+ is considered, which is equivalent to the
constrained least squares problem (1) in a noiseless setting.
In [3, 8], recovery from rank-one measurements is considered, i.e., for {xi}ni=1 ⊂ Rm
yi = x
⊤
i Σ
∗xi + εi = tr(X
⊤
i Σ
∗) + εi, with Xi = xix
⊤
i , i = 1, . . . , n. (3)
As opposed to [3, 8], where estimation based on nuclear norm regularization is proposed, the present
work is devoted to regularization-free estimation. While rank-onemeasurements as in (3) are also in
the center of interest herein, our framework is not limited to this case. In [3] an application of (3) to
covariance matrix estimation given only one-dimensional projections {x⊤i zi}ni=1 of the data points
is discussed, where the {zi}ni=1 are i.i.d. from a distribution with zero mean and covariance matrix
Σ∗. In fact, this fits the model under study with observations
yi = (x
⊤
i zi)
2 = x⊤i ziz
⊤
i xi = x
⊤
i Σ
∗xi + εi, εi = x
⊤
i {ziz⊤i − Σ∗}xi, i = 1, . . . , n. (4)
Specializing (3) to the case in which Σ∗ = σ∗(σ∗)⊤, one obtains the quadratic model
yi = |x⊤i σ∗|2 + εi (5)
which (with complex-valued σ∗) is relevant to the problem of phase retrieval [14]. The approach
of [7] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one
solutions. In follow-up work [4], the authors show a refined recovery result stating that imposing an
spd constraint − without regularization− suffices. A similar result has been proven independently
by [10]. However, the results in [4] and [10] only concern model (5). After posting an extended
version [22] of the present paper, a generalization of [4, 10] to general low-rank spd matrices has
been achieved in [13]. Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes
Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].
Notation. Md denotes the space of real d × d matrices with inner product 〈M,M ′〉 :=
tr(M⊤M ′). The subspace of symmetric matrices Sd has dimension δd := d(d + 1)/2. M ∈ Sd
has an eigen-decomposition M = UΛU⊤ =
∑d
j=1 λj(M)uju
⊤
j , where λ1(M) = λmax(M) ≥
λ2(M) ≥ . . . ≥ λd(M) = λmin(M), Λ = diag(λ1(M), . . . , λd(M)), and U = [u1 . . . ud]. For
q ∈ [1,∞) and M ∈ Sd, ‖M‖q := (
∑d
j=1 |λj(M)|q)1/q denotes the Schatten-q-norm (q = 1: nu-
clear norm; q = 2 Frobenius norm ‖M‖F , q = ∞: spectral norm ‖M‖∞ := max1≤j≤d |λj(M)|).
Let S1(d) = {M ∈ Sd : ‖M‖1 = 1} and S+1 (d) = S1(d) ∩ Sd+. The symbols ,,≻,≺ refer to
the semidefinite ordering. For a set A and α ∈ R, αA := {αa, a ∈ A}.
It is convenient to re-write model (1) as y = X (Σ∗) + ε, where y = (yi)ni=1, ε = (εi)ni=1 and
X : Mm → Rn is a linear map defined by (X (M))i = tr(X⊤i M), i = 1, . . . , n, referred to as
sampling operator. Its adjoint X ∗ : Rn → Mm is given by the map v 7→ ∑ni=1 viXi.
Supplement. The appendix contains all proofs, additional experiments and figures.
2 Analysis
Preliminaries. Throughout this section, we consider a special instance of model (1) in which
yi = tr(XiΣ
∗) + εi, where Σ
∗ ∈ Sm+ , Xi ∈ Sm, and εi
i.i.d.∼ N(0, σ2), i = 1, . . . , n. (6)
The assumption that the errors {εi}ni=1 are Gaussian is made for convenience as it simplifies the
stochastic part of our analysis, which could be extended to sub-Gaussian errors.
Note that w.l.o.g., we may assume that {Xi}ni=1 ⊂ Sm. In fact, since Σ∗ ∈ Sm, for any M ∈ Mm
we have that tr(MΣ∗) = tr(M symΣ∗), whereM sym = (M +M⊤)/2.
2
In the sequel, we study the statistical performance of the constrained least squares estimator
Σ̂ ∈ argmin
Σ∈Sm
+
1
2n
‖y −X (Σ)‖22 (7)
under model (6). More specifically, under certain conditions on X , we shall derive bounds on
(a)
1
n
‖X (Σ∗)−X (Σ̂)‖22, and (b) ‖Σ̂− Σ∗‖1, (8)
where (a) will be referred to as “prediction error” below. The most basic method for estimating Σ∗
is ordinary least squares (ols) estimation
Σ̂ols ∈ argmin
Σ∈Sm
1
2n
‖y −X (Σ)‖22, (9)
which is computationally simpler than (7). While (7) requires convex programming, (9) boils down
to solving a linear system of equations in δm = m(m + 1)/2 variables. On the other hand, the
prediction error of ols scales as OP(dim(range(X ))/n), where dim(range(X )) can be as large as
min{n, δm}, in which case the prediction error vanishes only if δm/n → 0 as n → ∞. Moreover,
the estimation error ‖Σ̂ols − Σ∗‖1 is unbounded unless n ≥ δm. Research conducted over the past
few years has thus focused on methods dealing successfully with the case n < δm as long as the
target Σ∗ has additional structure, notably low-rankedness. Indeed, if Σ∗ has rank r ≪ m, the
intrinsic dimension of the problem becomes (roughly)mr ≪ δm. In a large body of work, nuclear
norm regularization, which serves as a convex surrogate of rank regularization, is considered as a
computationally convenient alternative for which a series of adaptivity properties to underlying low-
rankedness has been established, e.g. [5, 15, 17, 18, 19]. Complementing (9) with nuclear norm
regularization yields the estimator
Σ̂1 ∈ argmin
Σ∈Sm
1
2n
‖y −X (Σ)‖22 + λ‖Σ‖1, (10)
where λ > 0 is a regularization parameter. In case an spd constraint is imposed (10) becomes
Σ̂1+ ∈ argmin
Σ∈Sm
+
1
2n
‖y −X (Σ)‖22 + λ tr(Σ). (11)
Our analysis aims at elucidating potential advantages of the spd constraint in the constrained least
squares problem (7) from a statistical point of view. It turns out that depending on properties of
X , the behaviour of Σ̂ can range from a performance similar to the least squares estimator Σ̂ols on
the one hand to a performance similar to the nuclear norm regularized estimator Σ̂1+ with properly
chosen/tuned λ on the other hand. The latter case appears to be remarkable: Σ̂ may enjoy similar
adaptivity properties as nuclear norm regularized estimators even though Σ̂ is obtained from a pure
data fitting problem without explicit regularization.
2.1 Negative result
We first discuss a negative example of X for which the spd-constrained estimator Σ̂ does not im-
prove (substantially) over the unconstrained estimator Σ̂ols. At the same time, this example provides
clues on conditions to be imposed on X to achieve substantially better performance.
Random Gaussian design. Consider the Gaussian orthogonal ensemble (GOE)
GOE(m) = {X = (xjk), {xjj}mj=1
i.i.d.∼ N(0, 1), {xjk = xkj}1≤j<k≤m i.i.d.∼ N(0, 1/2)}.
Gaussian measurements are common in compressed sensing. It is hence of interest to study mea-
surements {Xi}ni=1
i.i.d.∼ GOE(m) in the context of the constrained least squares problem (7). The
following statement points to a serious limitation associated with such measurements.
Proposition 1. Consider Xi
i.i.d.∼ GOE(m), i = 1, . . . , n. For any ε > 0, if n ≤ (1 − ε)δm/2, with
probability at least 1− 32 exp(−ε2δm), there exists∆ ∈ Sm+ ,∆ 6= 0 such that X (∆) = 0.
Proposition 1 implies that if the number of measurements drops below 1/2 of the ambient dimension
δm, estimating Σ
∗ based on (7) becomes ill-posed; the estimation error ‖Σ̂ − Σ∗‖1 is unbounded,
irrespective of the rank of Σ∗. Geometrically, the consequence of Proposition 1 is that the convex
cone CX = {z ∈ Rn : z = X (∆), ∆ ∈ Sm+} contains 0. Unless 0 is contained in the boundary
of CX (we conjecture that this event has measure zero), this means that CX = Rn, i.e. the spd
constraint becomes vacuous.
3
2.2 Slow Rate Bound on the Prediction Error
We present a positive result on the spd-constrained least squares estimator Σ̂ under an additional
condition on the sampling operator X . Specifically, the prediction error will be bounded as
1
n
‖X (Σ∗)−X (Σ̂)‖22 = O(λ0‖Σ∗‖1 + λ20), where λ0 =
1
n
‖X ∗(ε)‖∞, (12)
with λ0 typically being of the order O(
√
m/n) (up to log factors). The rate in (12) can be a sig-
nificant improvement of what is achieved by Σ̂ols if ‖Σ∗‖1 = tr(Σ∗) is small. If λ0 = o(‖Σ∗‖1)
that rate coincides with those of the nuclear norm regularized estimators (10), (11) with regulariza-
tion parameter λ ≥ λ0, cf. Theorem 1 in [19]. For nuclear norm regularized estimators, the rate
O(λ0‖Σ∗‖1) is achieved for any choice of X and is slow in the sense that the squared prediction
error only decays at the rate n−1/2 instead of n−1.
Condition on X . In order to arrive at a suitable condition to be imposed on X so that (12) can
be achieved, it makes sense to re-consider the negative example of Proposition 1, which states that
as long as n is bounded away from δm/2 from above, there is a non-trivial ∆ ∈ Sm+ such that
X (∆) = 0. Equivalently, dist(PX , 0) = min∆∈S+
1
(m)‖X (∆)‖2 = 0, where
PX := {z ∈ Rn : z = X (∆), ∆ ∈ S+1 (m)}, and S+1 (m) := {∆ ∈ Sm+ : tr(∆) = 1}.
In this situation, it is impossible to derive a non-trivial bound on the prediction error as dist(PX , 0) =
0 may imply CX = Rn so that ‖X (Σ∗) − X (Σ̂)‖22 = ‖ε‖22. To rule this out, the condition
dist(PX , 0) > 0 is natural. More strongly, one may ask for the following:
There exists a constant τ > 0 such that τ20 (X ) := min
∆∈S+
1
(m)
1
n
‖X (∆)‖22 ≥ τ2. (13)
An analogous condition is sufficient for a slow rate bound in the vector case, cf. [21]. However, the
condition for the slow rate bound in Theorem 1 below is somewhat stronger than (13).
Condition 1. There exist constants R∗ > 1, τ∗ > 0 s.t. τ
2(X , R∗) ≥ τ2∗ , where for R ∈ R
τ2(X , R) = dist2(RPX ,PX )/n = min
A∈RS+
1
(m)
B∈S+
1
(m)
1
n
‖X (A)−X (B)‖22.
The following condition is sufficient for Condition 1 and in some cases much easier to check.
Proposition 2. Suppose there exists a ∈ Rn, ‖a‖2 ≤ 1, and constants 0 < φmin ≤ φmax s.t.
λmin(n
−1/2X ∗(a)) ≥ φmin, and λmax(n−1/2X ∗(a)) ≤ φmax.
Then for any ζ > 1, X satisfies Condition 1 with R∗ = ζ(φmax/φmin) and τ2∗ = (ζ − 1)2φ2max.
The condition of Proposition 2 can be phrased as having a positive definite matrix in the image of
the unit ball under X ∗, which, after scaling by 1/√n, has its smallest eigenvalue bounded away
from zero and a bounded condition number. As a simple example, suppose that X1 =
√
nI . In-
voking Proposition 2 with a = (1, 0, . . . , 0)⊤ and ζ = 2, we find that Condition 1 is satisfied with
R∗ = 2 and τ
2
∗ = 1. A more interesting example is random design where the {Xi}ni=1 are (sam-
ple) covariance matrices, where the underlying random vectors satisfy appropriate tail or moment
conditions.
Corollary 1. Let πm be a probability distribution on R
m with second moment matrix Γ :=
Ez∼πm [zz
⊤] satisfying λmin(Γ) > 0. Consider the random matrix ensemble
M(πm, q) =
{
1
q
∑q
k=1 zkz
⊤
k , {zk}
q
k=1
i.i.d.∼ πm
}
. (14)
Suppose that {Xi}ni=1
i.i.d.∼ M(πm, q) and let Γ̂n := 1n
∑n
i=1 Xi and 0 < ǫn < λmin(Γ). Under the
event {‖Γ− Γ̂n‖∞ ≤ ǫn}, X satisfies Condition 1 with
R∗ =
2(λmax(Γ) + ǫn)
λmin(Γ)− ǫn
and τ2∗ = (λmax(Γ) + ǫn)
2.
4
It is instructive to spell out Corollary 1 with πm as the standard Gaussian distribution on R
m. The
matrix Γ̂n equals the sample covariance matrix computed fromN = n · q samples. It is well-known
[9] that form,N large, λmax(Γ̂n) and λmin(Γ̂n) concentrate sharply around (1+ηn)
2 and (1−ηn)2,
respectively, where ηn =
√
m/N . Hence, for any γ > 0, there exists Cγ > 1 so that if N ≥ Cγm,
it holds that R∗ ≤ 2 + γ. Similar though weaker concentration results for ‖Γ− Γ̂n‖∞ exist for the
broader class of distributions πm with finite fourth moments [26]. Specialized to q = 1, Corollary 1
yields a statement aboutX made up from random rank-onemeasurementsXi = ziz⊤i , i = 1, . . . , n,
cf. (3). The preceding discussion indicates that Condition 1 tends to be satisfied in this case.
Theorem 1. Suppose that model (6) holds with X satisfying Condition 1 with constantsR∗ and τ2∗ .
We then have
1
n
‖X (Σ∗)−X (Σ̂)‖22 ≤ max
{
2(1 +R∗)λ0‖Σ∗‖1, 2λ0‖Σ∗‖1 + 8
(
λ0
R∗
τ∗
)2}
where, for any µ ≥ 0, with probability at least 1− (2m)−µ
λ0 ≤ σ
√
(1 + µ)2 log(2m)
V 2
n
n , where V
2
n =
∥∥ 1
n
∑n
i=1 X
2
i
∥∥
∞
.
Remark: Under the scalings R∗ = O(1) and τ
2
∗ = Ω(1), the bound of Theorem 1 is of the
order O(λ0‖Σ∗‖1 + λ20) as announced at the beginning of this section. For given X , the quantity
τ2(X , R) can be evaluated by solving a least squares problem with spd constraints. Hence it is
feasible to check in practice whether Condition 1 holds. For later reference, we evaluate the term
V 2n for M(πm, q) with πm as standard Gaussian distribution. As shown in the supplement, with
high probability, V 2n = O(m logn) holds as long asm = O(nq).
2.3 Bound on the Estimation Error
In the previous subsection, we did not make any assumptions aboutΣ∗ apart fromΣ∗ ∈ Sm+ . Hence-
forth, we suppose that Σ∗ is of low rank 1 ≤ r ≪ m and study the performance of the constrained
least squares estimator (7) for prediction and estimation in such setting.
Preliminaries. Let Σ∗ = UΛU⊤ be the eigenvalue decomposition of Σ∗, where
U =
[
U‖ U⊥
m× r m× (m− r)
] [
Λr 0r×(m−r)
0(m−r)×r 0(m−r)×(m−r)
]
where Λr is diagonal with positive diagonal entries. Consider the linear subspace
T
⊥ = {M ∈ Sm : M = U⊥AU⊤⊥ , A ∈ Sm−r}.
From U⊤⊥Σ
∗U⊥ = 0, it follows that Σ
∗ is contained in the orthogonal complement
T = {M ∈ Sm : M = U‖B +B⊤U⊤‖ , B ∈ Rr×m},
of dimensionmr − r(r − 1)/2 ≪ δm if r ≪ m. The image of T under X is denoted by T .
Conditions on X . We introduce the key quantities the bound in this subsection depends on.
Separability constant.
τ2(T) =
1
n
dist2 (T ,PX ) , PX := {z ∈ Rn : z = X (∆), ∆ ∈ T⊥ ∩ S+1 (m)}
= min
Θ∈T, Λ∈S+
1
(m)∩T⊥
1
n
‖X (Θ)−X (Λ)‖22
Restricted eigenvalue.
φ2(T) = min
06=∆∈T
‖X (∆)‖22/n
‖∆‖21
.
As indicated by the following statement concerning the noiseless case, for bounding ‖Σ̂−Σ∗‖, it is
inevitable to have lower bounds on the above two quantities.
5
Proposition 3. Consider the trace regression model (1) with εi = 0, i = 1, . . . , n. Then
argmin
Σ∈Sm
+
1
2n
‖X (Σ∗)−X (Σ)‖22 = {Σ∗} for all Σ∗ ∈ T ∩ Sm+
if and only if it holds that τ2(T) > 0 and φ2(T) > 0.
Correlation constant. Moreover, we use of the following the quantity. It is not clear to us if it is
intrinsically required, or if its appearance in our bound is for merely technical reasons.
µ(T) = max
{
1
n 〈X (∆),X (∆′)〉 : ‖∆‖1 ≤ 1,∆ ∈ T, ∆′ ∈ S
+
1 (m) ∩ T⊥
}
.
We are now in position to provide a bound on ‖Σ̂− Σ∗‖1.
Theorem 2. Suppose that model (6) holds with Σ∗ as considered throughout this subsection and let
λ0 be defined as in Theorem 1. We then have
‖Σ̂− Σ∗‖1 ≤ max
{
8λ0
µ(T)
τ2(T)φ2(T)
(
3
2
+
µ(T)
φ2(T)
)
+ 4λ0
(
1
φ2(T)
+
1
τ2(T)
)
,
8λ0
φ2(T)
(
1 +
µ(T)
φ2(T)
)
,
8λ0
τ2(T)
}
.
Remark. Given Theorem 2 an improved bound on the prediction error scaling with λ20 in place
of λ0 can be derived, cf. (26) in Appendix D.
The quality of the bound of Theorem 2 depends on how the quantities τ2(T), φ2(T) and µ(T) scale
with n, m and r, which is design-dependent. Accordingly, the estimation error in nuclear norm
can be non-finite in the worst case and O(λ0r) in the best case, which matches existing bounds for
nuclear norm regularization (cf. Theorem 2 in [19]).
• The quantity τ2(T) is specific to the geometry of the constrained least squares problem
(7) and hence of critical importance. For instance, it follows from Proposition 1 that for
standard Gaussian measurements, τ2(T) = 0 with high probability once n < δm/2. The
situation can be much better for random spd measurements (14) as exemplified for mea-
surements Xi = ziz
⊤
i with zi
i.i.d.∼ N(0, I) in Appendix H. Specifically, it turns out that
τ2(T) = Ω(1/r) as long as n = Ω(m · r).
• It is not restrictive to assume φ2(T) is positive. Indeed, without that assumption, even an
oracle estimator based on knowledge of the subspace T would fail. Reasonable sampling
operatorsX have rankmin{n, δm} so that the nullspace of X only has a trivial intersection
with the subspace T as long as n ≥ dim(T) = mr − r(r − 1)/2.
• For fixed T, computing µ(T) entails solving a biconvex (albeit non-convex) optimization
problem in∆ ∈ T and∆′ ∈ S+1 (m)∩T⊥. Block coordinate descent is a practical approach
to such optimization problems for which a globally optimal solution is out of reach. In
this manner we explore the scaling of µ(T) numerically as done for τ2(T). We find that
µ(T) = O(δm/n) so that µ(T) = O(1) apart from the regime n/δm → 0, without ruling
out the possibility of undersampling, i.e. n < δm.
3 Numerical Results
In this section, we empirically study properties of the estimator Σ̂. In particular, its performance
relative to regularization-based methods is explored. We also present an application to spiked co-
variance estimation for the CBCL face image data set and stock prices from NASDAQ.
Comparison with regularization-based approaches. We here empirically evaluate ‖Σ̂ − Σ∗‖1
relative to well-known regularization-based methods.
Setup. We consider rank-one Wishart measurement matrices Xi = ziz
⊤
i , zi
i.i.d.∼ N(0, I), i =
1, . . . , n, fix m = 50 and let n ∈ {0.24, 0.26, . . . , 0.36, 0.4, . . . , 0.56} ·m2 and r ∈ {1, 2, . . . , 10}
vary. Each configuration of (n, r) is run with 50 replications. In each of these, we generate data
yi = tr(XiΣ
∗) + σεi, σ = 0.1, i = 1, . . . , n, (15)
where Σ∗ is generated randomly as rank r Wishart matrices and the {εi}ni=1 are i.i.d.N(0, 1).
6
600 700 800 900 1000 1100 1200 1300 1400
0.03
0.04
0.05
0.06
0.07
0.08
0.09
n
   r: 1   
|S
ig
m
a
 −
 S
ig
m
a
*|
1
 
 
constrained LS
regularized LS #
regularized LS
Chen et al. #
Chen et al.
oracle
600 700 800 900 1000 1100 1200 1300 1400
0.06
0.08
0.1
0.12
0.14
0.16
n
   r: 2   
|S
ig
m
a
 −
 S
ig
m
a
*|
1
 
 
constrained LS
regularized LS #
regularized LS
Chen et al. #
Chen et al.
oracle
600 700 800 900 1000 1100 1200 1300 1400
0.15
0.2
0.25
0.3
0.35
n
   r: 4   
|S
ig
m
a
 −
 S
ig
m
a
*|
1
 
 
constrained LS
regularized LS #
regularized LS
Chen et al. #
Chen et al.
oracle
600 700 800 900 1000 1100 1200 1300 1400
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
n
   r: 6   
|S
ig
m
a
 −
 S
ig
m
a
*|
1
 
 
constrained LS
regularized LS #
regularized LS
Chen et al. #
Chen et al.
oracle
700 800 900 1000 1100 1200 1300 1400
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
n
   r: 8   
|S
ig
m
a
 −
 S
ig
m
a
*|
1
 
 
constrained LS
regularized LS #
regularized LS
Chen et al. #
Chen et al.
oracle
800 900 1000 1100 1200 1300 1400
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
n
   r: 10   
|S
ig
m
a
 −
 S
ig
m
a
*|
1
 
 
constrained LS
regularized LS #
regularized LS
Chen et al. #
Chen et al.
oracle
Figure 1: Average estimation error (over 50 replications) in nuclear norm for fixed m = 50 and
certain choices of n and r. In the legend, “LS” is used as a shortcut for “least squares”. Chen et
al. refers to (16). “#”indicates an oracular choice of the tuning parameter. “oracle” refers to the ideal
error σr
√
m/n. Best seen in color.
Regularization-based approaches. We compare Σ̂ to the corresponding nuclear norm regularized
estimator in (11). Regarding the choice of the regularization parameter λ, we consider the grid
λ∗ · {0.01, 0.05, 0.1, 0.3, 0.5, 1, 2, 4, 8, 16}, where λ∗ = σ
√
m/n as recommended in [17] and pick
λ so that the prediction error on a separate validation data set of size n generated from (15) is
minimized. Note that in general, neither σ is known nor an extra validation data set is available. Our
goal here is to ensure that the regularization parameter is properly tuned. In addition, we consider
an oracular choice of λ where λ is picked from the above grid such that the performance measure
of interest (the distance to the target in the nuclear norm) is minimized. We also compare to the
constrained nuclear norm minimization approach of [8]:
min
Σ
tr(Σ) subject to Σ  0, and ‖y −X (Σ)‖1 ≤ λ. (16)
For λ, we consider the grid nσ
√
2/π · {0.2, 0.3, . . . , 1, 1.25}. This specific choice is motivated by
the fact that E[‖y − X (Σ∗)‖1] = E[‖ε‖1] = nσ
√
2/π. Apart from that, tuning of λ is performed
as for the nuclear norm regularized estimator. In addition, we have assessed the performance of the
approach in [3], which does not impose an spd constraint but adds another constraint to (16). That
additional constraint significantly complicates optimization and yields a second tuning parameter.
Thus, instead of doing a 2D-grid search, we use fixed values given in [3] for known σ. The results
are similar or worse than those of (16) (note in particular that positive semidefiniteness is not taken
advantage of in [3]) and are hence not reported.
Discussion of the results. We conclude from Figure 1 that in most cases, the performance of
the constrained least squares estimator does not differ much from that of the regularization-based
methods with careful tuning. For larger values of r, the constrained least squares estimator seems to
require slightly more measurements to achieve competitive performance.
Real Data Examples. We now present an application to recovery of spiked covariance matrices
which are of the form Σ∗ =
∑r
j=1 λjuju
⊤
j + σ
2I , where r ≪ m and λj ≫ σ2 > 0, j = 1, . . . , r.
This model appears frequently in connection with principal components analysis (PCA).
Extension to the spiked case. So far, we have assumed that Σ∗ is of low rank, but it is straight-
forward to extend the proposed approach to the case in which Σ∗ is spiked as long as σ2 is known or
an estimate is available. A constrained least squares estimator of Σ∗ takes the form Σ̂ + σ2I , where
Σ̂ ∈ argmin
Σ∈Sm
+
1
2n
‖y −X (Σ + σ2I)‖22. (17)
The case of σ2 unknown or general (unknown) diagonal perturbation is left for future research.
7
0 2 4 6 8 10 12
−1.4
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
n / (m * r)
lo
g
1
0
(|
S
ig
m
a
 −
 S
ig
m
a
*|
F
)
β = 1 (all samples)
β = 0.4
β = 0.08
β = 0.008
β = 1/N (1 sample)
oracle
CBCL
0 1 2 3 4 5 6
−0.5
0
0.5
1
1.5
2
n / (m * r)
lo
g
1
0
(|
S
ig
m
a
 −
 S
ig
m
a
*|
F
)
β = 1 (all samples)
β = 0.4
β = 0.08
β = 0.008
β = 1/N (1 sample)
oracle
NASDAQ
Figure 2: Average reconstruction errors log10‖Σ̂− Σ∗‖F in dependence of n/(mr) and the param-
eter β. “oracle” refers to the best rank r-approximation Σr.
Data sets. (i) The CBCL facial image data set [1] consist of N = 2429 images of 19× 19 pixels
(i.e., m = 361). We take Σ∗ as the sample covariance matrix of this data set. It turns out that
Σ∗ can be well approximated by Σr, r = 50, where Σr is the best rank r approximation to Σ
∗
obtained from computing its eigendecomposition and setting to zero all but the top r eigenvalues.
(ii) We construct a second data set from the daily end prices ofm = 252 stocks from the technology
sector in NASDAQ, starting from the beginning of the year 2000 to the end of the year 2014 (in
totalN = 3773 days, retrieved from finance.yahoo.com). We take Σ∗ as the resulting sample
correlation matrix and choose r = 100.
Experimental setup. As in preceding measurements, we consider n random Wishart mea-
surements for the operator X , where n = C(mr), where C ranges from 0.25 to 12. Since
‖Σr − Σ∗‖F/‖Σ∗‖F ≈ 10−3 for both data sets, we work with σ2 = 0 in (17) for simplicity.
To make recovery of Σ∗ more difficult, we make the problem noisy by using observations
yi = tr(XiSi), i = 1, . . . , n, (18)
where Si is an approximation to Σ
∗ obtained from the sample covariance respectively sample cor-
relation matrix of βN data points randomly sampled with replacement from the entire data set,
i = 1, . . . , n, where β ranges from 0.4 to 1/N (Si is computed from a single data point). For each
choice of n and β, the reported results are averages over 20 replications.
Results. For the CBCL data set, as shown in Figure 2, Σ̂ accurately approximates Σ∗ once the
number of measurements crosses 2mr. Performance degrades once additional noise is introduced to
the problem by using measurements (18). Even under significant perturbations (β = 0.08), reason-
able reconstruction of Σ∗ remains possible, albeit the number of required measurements increases
accordingly. In the extreme case β = 1/N , the error is still decreasing with n, but millions of
samples seems to be required to achieve reasonable reconstruction error.
The general picture is similar for the NASDAQ data set, but the difference between using measure-
ments based on the full sample correlation matrix on the one hand and approximations based on
random subsampling (18) on the other hand are more pronounced.
4 Conclusion
We have investigated trace regression in the situation that the underlying matrix is symmetric posi-
tive semidefinite. Under restrictions on the design, constrained least squares enjoys similar statistical
properties as methods employing nuclear norm regularization. This may come as a surprise, as reg-
ularization is widely regarded as necessary in small sample settings.
Acknowledgments
The work of Martin Slawski and Ping Li is partially supported by NSF-DMS-1444124, NSF-III-
1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137.
8
References
[1] CBCL face dataset. http://cbcl.mit.edu/software-datasets/FaceData2.html.
[2] D. Amelunxen, M. Lotz, M. McCoy, and J. Tropp. Living on the edge: phase transitions in convex
programs with random data. Information and Inference, 3:224–294, 2014.
[3] T. Cai and A. Zhang. ROP: Matrix recovery via rank-one projections. The Annals of Statistics, 43:102–
138, 2015.
[4] E. Candes and X. Li. Solving quadratic equations via PhaseLift when there are about as many equations
as unknowns. Foundation of Computational Mathematics, 14:1017–1026, 2014.
[5] E. Candes and Y. Plan. Tight oracle bounds for low-rank matrix recovery from a minimal number of noisy
measurements. IEEE Transactions on Information Theory, 57:2342–2359, 2011.
[6] E. Candes and B. Recht. Exact matrix completion via convex optimization. Foundation of Computational
Mathematics, 9:2053–2080, 2009.
[7] E. Candes, T. Strohmer, and V. Voroninski. PhaseLift: exact and stable signal recovery from magnitude
measurements via convex programming. Communications on Pure and Applied Mathematics, 66:1241–
1274, 2012.
[8] Y. Chen, Y. Chi, and A. Goldsmith. Exact and Stable Covariance Estimation from Quadratic Sampling
via Convex Programming. IEEE Transactions on Information Theory, 61:4034–4059, 2015.
[9] K. Davidson and S. Szarek. Handbook of the Geometry of Banach Spaces, volume 1, chapter Local
operator theory, random matrices and Banach spaces, pages 317–366. 2001.
[10] L. Demanet and P. Hand. Stable optimizationless recovery from phaseless measurements. Journal of
Fourier Analysis and its Applications, 20:199–221, 2014.
[11] D. Gross, Y.-K. Liu, S. Flammia, S. Becker, and J. Eisert. Quantum State Tomography via Compressed
Sensing. Physical Review Letters, 105:150401–15404, 2010.
[12] R. Horn and C. Johnson. Matrix Analysis. Cambridge University Press, 1985.
[13] M. Kabanva, R. Kueng, and H. Rauhut und U. Terstiege. Stable low rank matrix recovery via null space
properties. arXiv:1507.07184, 2015.
[14] M. Klibanov, P. Sacks, and A. Tikhonarov. The phase retrieval problem. Inverse Problems, 11:1–28,
1995.
[15] V. Koltchinskii, K. Lounici, and A. Tsybakov. Nuclear-norm penalization and optimal rates for noisy
low-rank matrix completion. The Annals of Statistics, 39:2302–2329, 2011.
[16] N. Meinshausen. Sign-constrained least squares estimation for high-dimensional regression. The Elec-
tronic Journal of Statistics, 7:1607–1631, 2013.
[17] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional
scaling. The Annals of Statistics, 39:1069–1097, 2011.
[18] B. Recht, M. Fazel, and P. Parillo. Guaranteed minimum-rank solutions of linear matrix equations via
nuclear norm minimization. SIAM Review, 52:471–501, 2010.
[19] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. The Annals of Statistics,
39:887–930, 2011.
[20] B. Schölkopf and A. Smola. Learning with kernels. MIT Press, Cambridge, Massachussets, 2002.
[21] M. Slawski and M. Hein. Non-negative least squares for high-dimensional linear models: consistency
and sparse recovery without regularization. The Electronic Journal of Statistics, 7:3004–3056, 2013.
[22] M. Slawski, P. Li, and M. Hein. Regularization-free estimation in trace regression with positive semidef-
inite matrices. arXiv:1504.06305, 2015.
[23] N. Srebro, J. Rennie, and T. Jaakola. Maximum margin matrix factorization. In Advances in Neural
Information Processing Systems 17, pages 1329–1336, 2005.
[24] R. Tibshirani. Regression shrinkage and variable selection via the lasso. Journal of the Royal Statistical
Society Series B, 58:671–686, 1996.
[25] J. Tropp. User-friendly tools for random matrices: An introduction. 2014. http://users.cms.
caltech.edu/˜jtropp/.
[26] R. Vershynin. How close is the sample covariance matrix to the actual covariance matrix ? Journal of
Theoretical Probability, 153:405–419, 2012.
[27] M. Wang, W. Xu, and A. Tang. A unique ’nonnegative’ solution to an underdetermined system: from
vectors to matrices. IEEE Transactions on Signal Processing, 59:1007–1016, 2011.
[28] C. Williams and M. Seeger. Using the Nyström method to speed up kernel machines. In Advances in
Neural Information Processing Systems 14, pages 682–688, 2001.
9
