


Paper ID = 5882
Title = Inverse Reinforcement Learning with Locally
Consistent Reward Functions
Quoc Phong Nguyen†, Kian Hsiang Low†, and Patrick Jaillet§
Dept. of Computer Science, National University of Singapore, Republic of Singapore†
Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA§
{qphong,lowkh}@comp.nus.edu.sg†, jaillet@mit.edu§
Abstract
Existing inverse reinforcement learning (IRL) algorithms have assumed each ex-
pert’s demonstrated trajectory to be produced by only a single reward function.
This paper presents a novel generalization of the IRL problem that allows each
trajectory to be generated by multiple locally consistent reward functions, hence
catering to more realistic and complex experts’ behaviors. Solving our gener-
alized IRL problem thus involves not only learning these reward functions but
also the stochastic transitions between them at any state (including unvisited
states). By representing our IRL problem with a probabilistic graphical model,
an expectation-maximization (EM) algorithm can be devised to iteratively learn
the different reward functions and the stochastic transitions between them in order
to jointly improve the likelihood of the expert’s demonstrated trajectories. As a
result, the most likely partition of a trajectory into segments that are generated
from different locally consistent reward functions selected by EM can be derived.
Empirical evaluation on synthetic and real-world datasets shows that our IRL al-
gorithm outperforms the state-of-the-art EM clustering with maximum likelihood
IRL, which is, interestingly, a reduced variant of our approach.
1 Introduction
The reinforcement learning problem in Markov decision processes (MDPs) involves an agent using
its observed rewards to learn an optimal policy that maximizes its expected total reward for a given
task. However, such observed rewards or the reward function defining them are often not available
nor known in many real-world tasks. The agent can therefore learn its reward function from an
expert associated with the given task by observing the expert’s behavior or demonstration, and this
approach constitutes the inverse reinforcement learning (IRL) problem.
Unfortunately, the IRL problem is ill-posed because infinitely many reward functions are consistent
with the expert’s observed behavior. To resolve this issue, existing IRL algorithms have proposed
alternative choices of the agent’s reward function that minimize different dissimilarity measures de-
fined using various forms of abstractions of the agent’s generated optimal behavior vs. the expert’s
observed behavior, as briefly discussed below (see [17] for a detailed review): (a) The projection
algorithm [1] selects a reward function that minimizes the squared Euclidean distance between the
feature expectations obtained by following the agent’s generated optimal policy and the empirical
feature expectations observed from the expert’s demonstrated state-action trajectories; (b) the multi-
plicative weights algorithm for apprentice learning [24] adopts a robust minimax approach to deriv-
ing the agent’s behavior, which is guaranteed to perform no worse than the expert and is equivalent
to choosing a reward function that minimizes the difference between the expected average reward
under the agent’s generated optimal policy and the expert’s empirical average reward approximated
using the agent’s reward weights; (c) the linear programming apprentice learning algorithm [23]
picks its reward function by minimizing the same dissimilarity measure but incurs much less time
empirically; (d) the policy matching algorithm [16] aims to match the agent’s generated optimal
behavior to the expert’s observed behavior by choosing a reward function that minimizes the sum of
1
squared Euclidean distances between the agent’s generated optimal policy and the expert’s estimated
policy (i.e., from its demonstrated trajectories) over every possible state weighted by its empirical
state visitation frequency; (e) the maximum entropy IRL [27] and maximum likelihood IRL (MLIRL)
[2] algorithms select reward functions that minimize an empirical approximation of the Kullback-
Leibler divergence between the distributions of the agent’s and expert’s generated state-action tra-
jectories, which is equivalent to maximizing the average log-likelihood of the expert’s demonstrated
trajectories. The log-likelihood formulations of the maximum entropy IRL and MLIRL algorithms
differ in the use of smoothing at the trajectory and action levels, respectively. As a result, the for-
mer’s log-likelihood or dissimilarity measure does not utilize the agent’s generated optimal policy,
which is consequently questioned by [17] as to whether it is considered an IRL algorithm. Bayesian
IRL [21] extends IRL to the Bayesian setting by maintaining a distribution over all possible reward
functions and updating it using Bayes rule given the expert’s demonstrated trajectories. The work
of [5] extends the projection algorithm [1] to handle partially observable environments given the
expert’s policy (i.e., represented as a finite state controller) or observation-action trajectories.
All the IRL algorithms described above have assumed that the expert’s demonstrated trajectories
are only generated by a single reward function. To relax this restrictive assumption, the recent
works of [2, 6] have, respectively, generalized MLIRL (combining it with expectation-maximization
(EM) clustering) and Bayesian IRL (integrating it with a Dirichlet process mixture model) to handle
trajectories generated by multiple reward functions (e.g., due to many intentions) in observable
environments. But, each trajectory is assumed to be produced by a single reward function.
In this paper, we propose a new generalization of the IRL problem in observable environments,
which is inspired by an open question posed in the seminal works of IRL [19, 22]: If behavior
is strongly inconsistent with optimality, can we identify “locally consistent” reward functions for
specific regions in state space? Such a question implies that no single reward function is globally
consistent with the expert’s behavior, hence invalidating the use of all the above-mentioned IRL
algorithms. More importantly, multiple reward functions may be locally consistent with the expert’s
behavior in different segments along its state-action trajectory and the expert has to switch/transition
between these locally consistent reward functions during its demonstration. This can be observed
in the following real-world example [26] where every possible intention of the expert is uniquely
represented by a different reward function: A driver intends to take the highway to a food center for
lunch. An electronic toll coming into effect on the highway may change his intention to switch to
another route. Learning of the driver’s intentions to use different routes and his transitions between
them allows the transport authority to analyze, understand, and predict the traffic route patterns and
behavior for regulating the toll collection. This example, among others (e.g., commuters’ intentions
to use different transport modes, tourists’ intentions to visit different attractions, Section 4), motivate
the practical need to formalize and solve our proposed generalized IRL problem.
This paper presents a novel generalization of the IRL problem that, in particular, allows each ex-
pert’s state-action trajectory to be generated by multiple locally consistent reward functions, hence
catering to more realistic and complex experts’ behaviors than that afforded by existing variants of
the IRL problem (which all assume that each trajectory is produced by a single reward function)
discussed earlier. At first glance, one may straightaway perceive our generalization as an IRL prob-
lem in a partially observable environment by representing the choice of locally consistent reward
function in a segment as a latent state component. However, the observation model cannot be easily
specified nor learned from the expert’s state-action trajectories, which invalidates the use of IRL
for POMDP [5]. Instead, we develop a probabilistic graphical model for representing our gener-
alized IRL problem (Section 2), from which an EM algorithm can be devised to iteratively select
the locally consistent reward functions as well as learn the stochastic transitions between them in
order to jointly improve the likelihood of the expert’s demonstrated trajectories (Section 3). As a
result, the most likely partition of an expert’s demonstrated trajectory into segments that are gener-
ated from different locally consistent reward functions selected by EM can be derived (Section 3),
thus enabling practitioners to identify states in which the expert transitions between locally consis-
tent reward functions and investigate the resulting causes. To extend such a partitioning to work
for trajectories traversing through any (possibly unvisited) region of the state space, we propose
using a generalized linear model to represent and predict the stochastic transitions between reward
functions at any state (i.e., including states not visited in the expert’s demonstrated trajectories) by
exploiting features that influence these transitions (Section 2). Finally, our proposed IRL algorithm
is empirically evaluated using both synthetic and real-world datasets (Section 4).
2
2 Problem Formulation
A Markov decision process (MDP) for an agent is defined as a tuple (S,A, t, r✓,  ) consisting of a
finite set S of its possible states such that each state s 2 S is associated with a column vector  s
of realized feature measurements, a finite set A of its possible actions, a state transition function
t : S ⇥ A ⇥ S ! [0, 1] denoting the probability t(s, a, s0) , P (s0|s, a) of moving to state s0
by performing action a in state s, a reward function r✓ : S ! R mapping each state s 2 S
to its reward r✓(s) , ✓> s where ✓ is a column vector of reward weights, and constant factor
  2 (0, 1) discounting its future rewards. When ✓ is known, the agent can compute its policy
⇡✓ : S⇥A! [0, 1] specifying the probability ⇡✓(s, a) , P (a|s, r✓) of performing action a in state
s. However, ✓ is not known in IRL and to be learned from an expert (Section 3).
Let R denote a finite set of locally consistent reward functions of the agent and re✓ be a reward func-
tion chosen arbitrarily from R prior to learning. Define a transition function ⌧! : R⇥S⇥R! [0, 1]
for switching between these reward functions as the probability ⌧!(r✓, s, r✓0) , P (r✓0 |s, r✓,!)
of switching from reward function r✓ to reward function r✓0 in state s where the set ! ,
{!r✓r✓0 }r✓2R,r✓02R\{re✓} contains column vectors of transition weights !r✓r✓0 for all r✓ 2 R and
r✓0 2 R \ {re✓} if the features influencing the stochastic transitions between reward functions can
be additionally observed by the agent during the expert’s demonstration, and ! , ; otherwise.
In our generalized IRL problem, ⌧! is not known and to be learned from the expert (Section 3).
Specifically, in the former case, we propose using a generalized linear model to represent ⌧!:
⌧!(r✓, s, r✓0) ,
⇢
exp(!>r✓r✓0's)/(1 +
P
r✓̄2R\{re✓}
exp(!>r✓r✓̄'s)) if r✓0 6= re✓,
1/(1 +
P
r✓̄2R\{re✓}
exp(!>r✓r✓̄'s)) otherwise;
(1)
where 's is a column vector of random feature measurements influencing the stochastic transitions
between reward functions (i.e., ⌧!) in state s.
Remark 1. Different from  s whose feature measurements are typically assumed in IRL algorithms
to be realized/known to the agent for all s 2 S and remain static over time, the feature measurements
of 's are, in practice, often not known to the agent a priori and can only be observed when the ex-
pert (agent) visits the corresponding state s 2 S during its demonstration (execution), and may vary
over time according to some unknown distribution, as motivated by the real-world examples given
in Section 1. Without prior observation of the feature measurements of 's for all s 2 S (or knowl-
edge of their distributions) necessary for computing ⌧! (1), the agent cannot consider exploiting ⌧!
for switching between reward functions within MDP or POMDP planning, even after learning its
weights !; this eliminates the possibility of reducing our generalized IRL problem to an equivalent
conventional IRL problem (Section 1) with only a single reward function (i.e., comprising a mixture
of locally consistent reward functions). Furthermore, the observation model cannot be easily speci-
fied nor learned from the expert’s trajectories of states, actions, and 's, which invalidates the use of
IRL for POMDP [5]. Instead of exploiting ⌧! within planning, during the agent’s execution, when
it visits some state s and observes the feature measurements of 's, it can then use and compute ⌧!
for state s to switch between reward functions, each of which has generated a separate MDP policy
prior to execution, as illustrated in a simple example in Fig. 1 below.
r✓ r✓0
⌧!(r✓, s, r✓)
⌧!(r✓, s, r✓0)
⌧!(r✓0 , s, r✓)
⌧!(r✓0 , s, r✓0)
Figure 1: Transition func-
tion ⌧! of an agent in state s
for switching between two
reward functions r✓ and r✓0
with their respective poli-
cies ⇡✓ and ⇡✓0 generated
prior to execution.
Remark 2. Using a generalized linear model to represent ⌧! (1) al-
lows learning of the stochastic transitions between reward functions
(specifically, by learning ! (Section 3)) to be generalized across dif-
ferent states. After learning, (1) can then be exploited for predicting
the stochastic transitions between reward functions at any state (i.e.,
including states not visited in the expert’s demonstrated state-action
trajectories). Consequently, the agent can choose to traverse a trajec-
tory through any region (i.e., possibly not visited by the expert) of the
state space during its execution and the most likely partition of its tra-
jectory into segments that are generated from different locally consis-
tent reward functions selected by EM can still be derived (Section 3).
In contrast, if the feature measurements of 's cannot be observed by
the agent during the expert’s demonstration (i.e., ! = ;, as defined above), then such a generaliza-
tion is not possible; only the transition probabilities of switching between reward functions at states
visited in the expert’s demonstrated trajectories can be estimated (Section 3). In practice, since the
number |S| of visited states is expected to be much larger than the length L of any feature vector 's,
3
the number O(|S||R|2) of transition probabilities to be estimated is bigger than |!| = O(L|R|2) in
(1). So, observing 's offers a further advantage of reducing the number of parameters to be learned.
R✓n0 R✓n1
An
1
Sn
1
R✓n2
An
2
Sn
2
· · ·
· · ·
· · ·
R✓nTn
AnTn
SnTn
Figure 2: Probabilistic graphical model
of the expert’s n-th demonstrated tra-
jectory encoding its stochastic transi-
tions between reward functions with
solid edges (i.e., ⌧!(r✓nt 1 , s
n
t , r✓nt ) =
P (r✓nt |snt , r✓nt 1 ,!) for t = 1, . . . , Tn),
state transitions with dashed edges
(i.e., t(snt , ant , snt+1) = P (snt+1|snt , ant )
for t = 1, . . . , Tn   1), and policy
with dotted edges (i.e., ⇡✓nt (s
n
t , a
n
t ) =
P (ant |snt , r✓nt ) for t = 1, . . . , Tn).
Fig. 2 shows the probabilistic graphical model for rep-
resenting our generalized IRL problem. To describe our
model, some notations are necessary: Let N be the num-
ber of the expert’s demonstrated trajectories and Tn be the
length (i.e., number of time steps) of its n-th trajectory for
n = 1, . . . , N . Let r✓nt 2 R, ant 2 A, and snt 2 S de-
note its reward function, action, and state at time step t
in its n-th trajectory, respectively. Let R✓nt , A
n
t , and Snt
be random variables corresponding to their respective re-
alizations r✓nt , a
n
t , and snt where R✓nt is a latent variable,
and Ant and Snt are observable variables. Define r✓n ,
(r✓nt )
Tn
t=0, an , (ant )Tnt=1, and sn , (snt )Tnt=1 as sequences
of all its reward functions, actions, and states in its n-th
trajectory, respectively. Finally, define r✓1:N , (r✓n)Nn=1,
a1:N , (an)Nn=1, and s1:N , (sn)Nn=1 as tuples of all
its reward function sequences, action sequences, and state
sequences in its N trajectories, respectively.
It can be observed from Fig. 2 that our probabilistic graph-
ical model of the expert’s n-th demonstrated trajectory en-
codes its stochastic transitions between reward functions, state transitions, and policy. Through our
model, the Viterbi algorithm [20] can be applied to derive the most likely partition of the expert’s
trajectory into segments that are generated from different locally consistent reward functions se-
lected by EM, as shown in Section 3. Given the state transition function t(·, ·, ·) and the number
|R| of reward functions, our model allows tractable learning of the unknown parameters using EM
(Section 3), which include the reward weights vector ✓ for all reward functions r✓ 2 R, transition
function ⌧! for switching between reward functions, initial state probabilities ⌫(s) , P (Sn1 = s)
for all s 2 S , and initial reward function probabilities  (r✓) , P (R✓n0 = r✓) for all r✓ 2 R.
3 EM Algorithm for Parameter Learning
A straightforward approach to learning the unknown parameters ⇤ , (⌫, , {✓|r✓ 2 R}, ⌧!) is to
select the value of ⇤ that directly maximizes the log-likelihood of the expert’s demonstrated trajec-
tories. Computationally, such an approach is prohibitively expensive due to a large joint parameter
space to be searched for the optimal value of ⇤. To ease this computational burden, our key idea is
to devise an EM algorithm that iteratively refines the estimate for ⇤ to improve the expected log-
likelihood instead, which is guaranteed to improve the original log-likelihood by at least as much:
Expectation (E) step. Q(⇤,⇤i) ,Pr✓1:N P (r✓1:N |s
1:N , a1:N ,⇤i) log P (r✓1:N , s
1:N , a1:N |⇤).
Maximization (M) step. ⇤i+1 = argmax⇤Q(⇤,⇤i)
where ⇤i denotes an estimate for ⇤ at iteration i. The Q function of EM can be reduced to the
following sum of five terms, as shown in Appendix A:
Q(⇤,⇤i) =
PN
n=1 log ⌫(s
n
1 ) +
PN
n=1
P
r✓2R
P (R✓n0 = r✓|sn, an,⇤i) log  (r✓) (2)
+
PN
n=1
PTn
t=1
P
r✓2R
P (R✓nt = r✓|sn, an,⇤i) log ⇡✓(snt , ant ) (3)
+
PN
n=1
PTn
t=1
P
r✓,r✓02R
P (R✓nt 1 = r✓, s
n
t , R✓nt = r✓0 |sn, an,⇤i)⇥ log ⌧!(r✓, snt , r✓0) (4)
+
PN
n=1
PTn 1
t=1 log t(s
n
t , a
n
t , s
n
t+1) . (5)
Interestingly, each of the first four terms in (2), (3), and (4) contains a unique unknown parameter
type (respectively, ⌫,  , {✓|r✓ 2 R}, and ⌧!) and can therefore be maximized separately in the
M step to be discussed below. As a result, the parameter space to be searched can be greatly re-
duced. Note that the third term (3) generalizes the log-likelihood in MLIRL [2] (i.e., assuming all
trajectories to be produced by a single reward function) to that allowing each expert’s trajectory to
be generated by multiple locally consistent reward functions. The last term (5), which contains the
known state transition function t, is independent of unknown parameters ⇤.1
1If the state transition function is unknown, then it can be learned by optimizing the last term (5).
4
Learning initial state probabilities. To maximize the first term in the Q function (2) of EM, we
use the method of Lagrange multipliers with the constraint
P
s2S ⌫(s) = 1 to obtain the estimate
b⌫(s) = (1/N)
PN
n=1 I
n
1 for all s 2 S where In1 is an indicator variable of value 1 if sn1 = s, and
0 otherwise. Since b⌫ can be computed directly from the expert’s demonstrated trajectories in O(N)
time, it does not have to be refined.
Learning initial reward function probabilities. To maximize the second term in Q function (2) of
EM, we utilize the method of Lagrange multipliers with the constraint
P
r✓2R
 (r✓) = 1 to derive
 i+1(r✓i) = (1/N)
PN
n=1 P (R✓n0 = r✓|sn, an,⇤i) (6)
for all r✓i 2 R where  i+1 denotes an estimate for   at iteration i+1, ✓i denotes an estimate for ✓ at
iteration i, and P (R✓nt = r✓|sn, an,⇤i) (in this case, t = 0) can be computed in O(
PN
n=1 |R|2Tn)
time using a procedure inspired by Baum-Welch algorithm [3], as shown in Appendix B.
Learning reward functions. The third term in the Q function (3) of EM is maximized using
gradient ascent and its gradient g1(✓) with respect to ✓ is derived to be
g1(✓) ,
NX
n=1
TnX
t=1
P (R✓nt = r✓|sn, an,⇤i)
⇡✓(snt , a
n
t )
d⇡✓(snt , ant )
d✓
(7)
for all ✓ 2 {✓0|r✓0 2 R}. For ⇡✓(snt , ant ) to be differentiable in ✓, we define the Q✓ function
of MDP using an operator that blends the Q✓ values via Boltzmann exploration [2]: Q✓(s, a) ,
✓> s +  
P
s02S t(s, a, s
0
) ⌦a0 Q✓(s0, a0) where ⌦aQ✓(s, a) ,
P
a2A Q✓(s, a) ⇥ ⇡✓(s, a) such
that ⇡✓(s, a) , exp( Q✓(s, a))/
P
a02A exp( Q✓(s, a
0
)) is defined as a Boltzmann exploration
policy, and   > 0 is a temperature parameter. Then, we update ✓i+1  ✓i +  g1(✓i) where   is the
learning step size. We use backtracking line search method to improve the performance of gradient
ascent. Similar to MLIRL, the time incurred in each iteration of gradient ascent depends mostly on
that of value iteration, which increases with the size of the MDP’s state and action space.
Learning transition function for switching between reward functions. To maximize the fourth
term in the Q function (4) of EM, if the feature measurements of 's cannot be observed by the agent
during the expert’s demonstration (i.e., ! = ;), then we utilize the method of Lagrange multipliers
with the constraints
P
r✓02R
⌧!(r✓, s, r✓0) = 1 for all r✓ 2 R and s 2 S to obtain
⌧!i+1(r✓i , s, r✓0i) = (
PN
n=1
PTn
t=1  n,t,r✓i ,s,r✓0i )/(
P
r✓̄i2R
PN
n=1
PTn
t=1  n,t,r✓i ,s,r✓̄i ) (8)
for r✓i , r✓0i 2 R and s 2 S where S is the set of states visited by the expert, ⌧!i+1 is an estimate
for ⌧! at iteration i + 1, and  n,t,r✓i ,s,r✓̄i , P (R✓nt 1 = r✓, Snt = s, R✓nt = r✓̄|sn, an,⇤i) can be
computed efficiently by exploiting the intermediate results from evaluating P (R✓nt = r✓|sn, an,⇤i)
described previously, as detailed in Appendix B.
On the other hand, if the feature measurements of's can be observed by the agent during the expert’s
demonstration, then recall that we use a generalized linear model to represent ⌧! (1) (Section 2) and
! is the unknown parameter to be estimated. Similar to learning the reward weights vector ✓ for
reward function r✓, we maximize the fourth term (4) in the Q function of EM by using gradient
ascent and its gradient g2(!r✓r✓0 ) with respect to !r✓r✓0 is derived to be
g2(!r✓r✓0 ) ,
NX
n=1
TnX
t=1
X
r✓̄2R
 n,t,r✓,snt ,r✓̄
⌧!(r✓, snt , r✓̄)
d⌧!(r✓, snt , r✓̄)
d!r✓r✓0
(9)
for all !r✓r✓0 2 !. Let !ir✓r✓0 denote an estimate for !r✓r✓0 at iteration i. Then, it is updated
using !i+1r✓r✓0  !ir✓r✓0 +  g2(!ir✓r✓0 ) where   is the learning step size. Backtracking line search
method is also used to improve the performance of gradient ascent here. In both cases, the time
incurred in each iteration i is proportional to the number of  n,t,r✓i ,s,r✓̄i to be computed, which is
O(PNn=1 |R|2|S|Tn) time.
Viterbi algorithm for partitioning a trajectory into segments with different locally consistent
reward functions. Given the final estimate b⇤ = (b⌫, b , {b✓|rb✓ 2 R}, ⌧b!) for the unknown pa-
rameters ⇤ produced by EM, the most likely partition of the expert’s n-th demonstrated trajectory
into segments generated by different locally consistent reward functions is r⇤✓n = (r
⇤
✓nt
)
Tn
t=0 ,
argmaxr✓n P (r✓n |sn, an, b⇤) = argmaxr✓n P (r✓n , sn, an|b⇤), which can be derived using the
Viterbi algorithm [20]. Specifically, define vrb✓,T for T = 1, . . . , Tn as the probability of the most
5
likely reward function sequence (r✓nt )
T 1
t=0 from time steps 0 to T   1 ending with reward function
rb✓ at time step T that produce state and action sequences (s
n
t )
T
t=1 and (ant )Tt=1:
vrb✓,T , max(r✓nt )T 1t=0 P ((r✓
n
t
)
T 1
t=0 , R✓nT = r✓, (s
n
t )
T
t=1, (a
n
t )
T
t=1|b⇤)
= t(snT 1, a
n
T 1, s
n
T ) ⇡b✓(s
n
T , a
n
T ) maxrb✓0
vrb✓0 ,T 1 ⌧b!(rb✓0 , s
n
T , rb✓) ,
vrb✓,1 , maxr✓n0 P (r✓n0 , R✓n1 = r✓, s
n
1 , a
n
1 |b⇤) = b⌫(sn1 ) ⇡b✓(sn1 , an1 ) maxrb✓0 b (rb✓0) ⌧b!(rb✓0 , sn1 , rb✓) .
Then, r⇤✓n0 = argmaxrb✓0 b (rb✓0) ⌧b!(rb✓0 , s
n
1 , r
⇤
✓n1
), r⇤✓nT = argmaxrb✓0 vrb✓0 ,T ⌧b!(rb✓0 , s
n
T+1, r
⇤
✓nT+1
) for
T = 1, . . . , Tn 1, and r⇤✓nTn = argmaxrb✓ vrb✓,Tn . The above Viterbi algorithm can be applied in the
same way to partition an agent’s trajectory traversing through any region (i.e., possibly not visited
by the expert) of the state space during its execution in O(|R|2T ) time.
4 Experiments and Discussion
This section evaluates the empirical performance of our IRL algorithm using 3 datasets featuring
experts’ demonstrated trajectories in two simulated grid worlds and real-world taxi trajectories. The
average log-likelihood of the expert’s demonstrated trajectories is used as the performance metric
because it inherently accounts for the fidelity of our IRL algorithm in learning the locally consistent
reward functions (i.e., R) and the stochastic transitions between them (i.e., ⌧!):
L(⇤) , (1/Ntot)
PNtot
n=1 log P (s
n, an|⇤) (10)
where Ntot is the total number of the expert’s demonstrated trajectories available in the dataset.
As proven in [17], maximizing L(⇤) with respect to ⇤ is equivalent to minimizing an empirical ap-
proximation of the Kullback-Leibler divergence between the distributions of the agent’s and expert’s
generated state-action trajectories. Note that when the final estimate b⇤ produced by EM (Section 3)
is plugged into (10), the resulting P (sn, an|b⇤) in (10) can be computed efficiently using a procedure
similar to that in Section 3, as detailed in Appendix C. To avoid local maxima in gradient ascent,
we initialize our EM algorithm with 20 random ⇤0 values and report the best result based on the Q
value of EM (Section 3). 0 1 2 3 4
0
1
2
3
4
O D
0 1 2 3 4
0
1
2
3
4
O
D
A B
Figure 3: Grid worlds A (states
(0, 0), (1, 1), and (2, 2) are, respec-
tively, examples of water, land, and
obstacle), and B (state (2, 2) is an
example of barrier). ‘O’ and ‘D’ de-
note origin and destination.
To demonstrate the importance of modeling and learning
stochastic transitions between locally consistent reward func-
tions, the performance of our IRL algorithm is compared with
that of its reduced variant assuming no change/switching of
reward function within each trajectory, which is implemented
by initializing ⌧!(r✓, s, r✓) = 1 for all r✓ 2 R and s 2 S
and deactivating the learning of ⌧! . In fact, it can be shown
(Appendix D) that such a reduction, interestingly, is equiva-
lent to EM clustering with MLIRL [2]. So, our IRL algorithm
generalizes EM clustering with MLIRL, the latter of which
has been empirically demonstrated in [2] to outperform many
existing IRL algorithms, as discussed in Section 1.
Simulated grid world A. The environment (Fig. 3) is modeled as a 5 ⇥ 5 grid of states, each of
which is either land, water, water and destination, or obstacle associated with the respective feature
vectors (i.e.,  s) (0, 1, 0)>, (1, 0, 0)>, (1, 0, 1)>, and (0, 0, 0)>. The expert starts at origin (0, 2)
and any of its actions can achieve the desired state with 0.85 probability. It has two possible reward
functions, one of which prefers land to water and going to destination (i.e., ✓ = (0, 20, 30)>), and
the other of which prefers water to land and going to destination (i.e., ✓0 = (20, 0, 30)>). The expert
will only consider switching its reward function at states (2, 0) and (2, 4) from r✓0 to r✓ with 0.5
probability and from r✓ to r✓0 with 0.7 probability; its reward function remains unchanged at all
other states. The feature measurements of 's cannot be observed by the agent during the expert’s
demonstration. So, ! = ; and ⌧! is estimated using (8). We set   to 0.95 and the number |R| of
reward functions of the agent to 2.
Fig. 4a shows results of the average log-likelihood L (10) achieved by our IRL algorithm, EM
clustering with MLIRL, and the expert averaged over 4 random instances with varying number
N of expert’s demonstrated trajectories. It can be observed that our IRL algorithm significantly
outperforms EM clustering with MLIRL and achieves a L performance close to that of the expert,
especially when N increases. This can be explained by its modeling of ⌧! and its high fidelity in
learning and predicting ⌧!: While our IRL algorithm allows switching of reward function within
each trajectory, EM clustering with MLIRL does not.
6
0 200 400 600 800 1000 1200 1400 1600
−25
−23
−21
−19
−17
−15
No. of demonstrated trajectories
A
v
er
ag
e 
lo
g
−
li
k
el
ih
o
o
d
Our IRL algorithm
EM clustering with MLIRL
Expert
0 100 200 300 400 500 600
−29
−28
−27
−26
−25
−24
−23
−22
No. of demonstrated trajectories
A
v
er
ag
e 
lo
g
−
li
k
el
ih
o
o
d
Our IRL algorithm
EM clustering with MLIRL
Expert
(a) (b)
Figure 4: Graphs of average log-likelihood L achieved by our
IRL algorithm, EM clustering with MLIRL, and the expert vs.
number N of expert’s demonstrated trajectories in simulated
grid worlds (a) A (Ntot = 1500) and (b) B (Ntot = 500).
We also observe that the accuracy of
estimating the transition probabili-
ties ⌧!(r✓, s, .) (⌧!(r✓0 , s, .)) using
(8) depends on the frequency and
distribution of trajectories demon-
strated by the expert with its reward
function R✓nt 1 = r✓ (R✓nt 1 = r✓0 )
at time step t 1 and its state snt = s
at time step t, which is expected.
Those transition probabilities that
are poorly estimated due to few rel-
evant expert’s demonstrated trajec-
tories, however, do not hurt the L
performance of our IRL algorithm by much because such trajectories tend to have very low prob-
ability of being demonstrated by the expert. In any case, this issue can be mitigated by using the
generalized linear model (1) to represent ⌧! and observing the feature measurements of 's necessary
for learning and computing ⌧! , as shown next.
Simulated grid world B. The environment (Fig. 3) is also modeled as a 5 ⇥ 5 grid of states, each
of which is either the origin, destination, or land associated with the respective feature vectors (i.e.
 s) (0, 1)>, (1, 0)>, and (0, 0)>. The expert starts at origin (4, 0) and any of its actions can achieve
the desired state with 0.85 probability. It has two possible reward functions, one of which prefers
going to destination (i.e., ✓ = (30, 0)>), and the other of which prefers returning to origin (i.e., ✓0 =
(0, 30)>). While moving to the destination, the expert will encounter barriers at some states with
corresponding feature vectors 's = (1, 1)> and no barriers at all other states with 's = (0, 1)>; the
second component of 's is used as an offset value in the generalized linear model (1). The expert’s
behavior of switching between reward functions is governed by a generalized linear model ⌧! (1)
with re✓ = r✓0 and transition weights !r✓r✓ = ( 11, 12)> and !r✓0r✓ = (13, 12)>. As a result,
it will, for example, consider switching its reward function at states with barriers from r✓ to r✓0
with 0.269 probability. We estimate ⌧! using (9) and set   to 0.95 and the number |R| of reward
functions of the agent to 2. To assess the fidelity of learning and predicting the stochastic transitions
between reward functions at unvisited states, we intentionally remove all demonstrated trajectories
that visit state (2, 0) with a barrier.
Fig. 4b shows results of L (10) performance achieved by our IRL algorithm, EM clustering with
MLIRL, and the expert averaged over 4 random instances with varying N . It can again be observed
that our IRL algorithm outperforms EM clustering with MLIRL and achieves an L performance
comparable to that of the expert due to its modeling of ⌧! and its high fidelity in learning and
predicting ⌧!: While our IRL algorithm allows switching of reward function within each trajectory,
EM clustering with MLIRL does not. Besides, the estimated transition function ⌧b! using (9) is very
close to that of the expert, even at unvisited state (2, 0). So, unlike using (8), the learning of ⌧! with
(9) can be generalized well across different states, thus allowing ⌧! to be predicted accurately at any
state. Hence, we will model ⌧! with (1) and learn it using (9) in the next experiment.
Real-world taxi trajectories. The Comfort taxi company in Singapore has provided GPS traces of
59 taxis with the same origin and destination that are map-matched [18] onto a network (i.e., com-
prising highway, arterials, slip roads, etc) of 193 road segments (i.e., states). Each road segment/state
is specified by a 7-dimensional feature vector  s: Each of the first six components of  s is an indi-
cator describing whether it belongs to Alexandra Road (AR), Ayer Rajah Expressway (AYE), Depot
Road (DR), Henderson Road (HR), Jalan Bukit Merah (JBM), or Lower Delta Road (LDR), while
the last component of  s is the normalized shortest path distance from the road segment to desti-
nation. We assume that the 59 map-matched trajectories are demonstrated by taxi drivers with a
common set R of 2 reward functions and the same transition function ⌧! (1) for switching between
reward functions, the latter of which is influenced by the normalized taxi speed constituting the first
component of 2-dimensional feature vector 's; the second component of 's is used as an offset of
value 1 in the generalized linear model (1). The number |R| of reward functions is set to 2 because
when we experiment with |R| = 3, two of the learned reward functions are similar. Every driver can
deterministically move its taxi from its current road segment to the desired adjacent road segment.
7
10 20 30 40 50 60
−8
−7.5
−7
−6.5
−6
−5.5
−5
−4.5
−4
No. of demonstrated trajectories
A
v
er
ag
e 
lo
g
−
li
k
el
ih
o
o
d
Our IRL algorithm
EM clustering with MLIRL
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Normalized taxi speed
Pr
ob
ab
ili
ty
 
 
⌧b!(rb✓, s, rb✓0)
⌧b!(rb✓0 , s, rb✓0)
(a) (b)
Figure 5: Graphs of (a) average log-likelihood L achieved by
our IRL algorithm and EM clustering with MLIRL vs. no. N
of taxi trajectories (Ntot = 59) and (b) transition probabilities
of switching between reward functions vs. taxi speed.
Fig. 5a shows results of L (10)
performance achieved by our IRL
algorithm and EM clustering with
MLIRL averaged over 3 random in-
stances with varying N . Our IRL
algorithm outperforms EM cluster-
ing with MLIRL due to its modeling
of ⌧! and its high fidelity in learning
and predicting ⌧! .
To see this, our IRL algorithm is
able to learn that a taxi driver is
likely to switch between reward
functions representing different in-
tentions within its demonstrated trajectory: Reward function rb✓ denotes his intention of driving
directly to the destination (Fig. 6a) due to a huge penalty (i.e., reward weight -49) on being far
from destination and a large reward (i.e., reward weight 35.7) for taking the shortest path from ori-
gin to destination, which is via JBM, while rb✓0 denotes his intention of detouring to DR or JBM
(Fig. 6b) due to large rewards for traveling on them (respectively, reward weights 30.5 and 23.7).
As an example, Fig. 6c shows the most likely partition of a demonstrated trajectory into segments
generated from locally consistent reward functions rb✓ and rb✓0 , which is derived using our Viterbi
algorithm (Section 3). It can be observed that the driver is initially in rb✓0 on the slip road exiting
AYE, switches from rb✓0 to rb✓ upon turning into AR to detour to DR, and remains in rb✓ while driving
along DR, HR, and JBM to destination. On the other hand, the reward functions learned by EM
clustering with MLIRL are both associated with his intention of driving directly to destination (i.e.,
similar to rb✓); it is not able to learn his intention of detouring to DR or JBM.
(a)
D
AYE
JBM
DR
AR
HR
LDR
O
(b)
O
D
JBM
DR
AYE
AR
HR
LDR
(c)
O
AR
DR
AYE
JBM
HR
LDR
D
Figure 6: Reward (a) rb✓(s) and (b) rb✓0(s) for each
road segment s with b✓ = (7.4, 3.9, 16.3, 20.3, 35.7,
21.5,  49.0)> and b✓0 = (5.2, 9.2, 30.5, 15.0, 23.7,
21.5,  9.2)> such that more red road segments
give higher rewards. (c) Most likely partition of a
demonstrated trajectory from origin ‘O’ to destina-
tion ‘D’ into red and green segments generated by
rb✓ and rb✓0 , respectively.
Fig. 5b shows the influence of normalized taxi
speed (i.e., first component of 's) on the es-
timated transition function ⌧b! using (9). It
can be observed that when the driver is in
rb✓ (i.e., driving directly to destination), he is
very unlikely to change his intention regard-
less of taxi speed. But, when he is in rb✓0 (i.e.,
detouring to DR or JBM), he is likely (un-
likely) to remain in this intention if taxi speed
is low (high). The demonstrated trajectory in
Fig. 6c in fact supports this observation: The
driver initially remains in rb✓0 on the upslope
slip road exiting AYE, which causes the low
taxi speed. Upon turning into AR to detour to
DR, he switches from rb✓0 to rb✓ because he can
drive at relatively high speed on flat terrain.
5 Conclusion
This paper describes an EM-based IRL al-
gorithm that can learn the multiple reward
functions being locally consistent in differ-
ent segments along a trajectory as well as the
stochastic transitions between them. It gen-
eralizes EM-clustering with MLIRL and has
been empirically demonstrated to outperform
it on both synthetic and real-world datasets.
For our future work, we plan to extend our IRL algorithm to cater to an unknown number of reward
functions [6], nonlinear reward functions [12] modeled by Gaussian processes [4, 8, 13, 14, 15, 25],
other dissimilarity measures described in Section 1, linearly-solvable MDPs [7], active learning with
Gaussian processes [11], and interactions with self-interested agents [9, 10].
Acknowledgments. This work was partially supported by Singapore-MIT Alliance for Research
and Technology Subaward Agreement No. 52 R-252-000-550-592.
8
References
[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. ICML,
2004.
[2] M. Babeş-Vroman, V. Marivate, K. Subramanian, and M. Littman. Apprenticeship learning about multiple
intentions. In Proc. ICML, pages 897–904, 2011.
[3] J. Bilmes. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaus-
sian mixture and Hidden Markov models. Technical Report ICSI-TR-97-02, University of California,
Berkeley, 1998.
[4] J. Chen, N. Cao, K. H. Low, R. Ouyang, C. K.-Y. Tan, and P. Jaillet. Parallel Gaussian process regression
with low-rank covariance matrix approximations. In Proc. UAI, pages 152–161, 2013.
[5] J. Choi and K. Kim. Inverse reinforcement learning in partially observable environments. JMLR, 12:691–
730, 2011.
[6] J. Choi and K. Kim. Nonparametric Bayesian inverse reinforcement learning for multiple reward func-
tions. In Proc. NIPS, pages 314–322, 2012.
[7] K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable MDPs. In Proc. ICML,
pages 335–342, 2010.
[8] T. N. Hoang, Q. M. Hoang, and K. H. Low. A unifying framework of anytime sparse Gaussian process
regression models with stochastic variational inference for big data. In Proc. ICML, pages 569–578, 2015.
[9] T. N. Hoang and K. H. Low. A general framework for interacting Bayes-optimally with self-interested
agents using arbitrary parametric model and model prior. In Proc. IJCAI, pages 1394–1400, 2013.
[10] T. N. Hoang and K. H. Low. Interactive POMDP Lite: Towards practical planning to predict and exploit
intentions for interacting with self-interested agents. In Proc. IJCAI, pages 2298–2305, 2013.
[11] T. N. Hoang, K. H. Low, P. Jaillet, and M. Kankanhalli. Nonmyopic ✏-Bayes-optimal active learning of
Gaussian processes. In Proc. ICML, pages 739–747, 2014.
[12] S. Levine, Z. Popović, and V. Koltun. Nonlinear inverse reinforcement learning with Gaussian processes.
In Proc. NIPS, pages 19–27, 2011.
[13] K. H. Low, J. Chen, T. N. Hoang, N. Xu, and P. Jaillet. Recent advances in scaling up Gaussian process
predictive models for large spatiotemporal data. In S. Ravela and A. Sandu, editors, Proc. Dynamic
Data-driven Environmental Systems Science Conference (DyDESS’14). LNCS 8964, Springer, 2015.
[14] K. H. Low, N. Xu, J. Chen, K. K. Lim, and E. B. Özgül. Generalized online sparse Gaussian processes
with application to persistent mobile robot localization. In Proc. ECML/PKDD Nectar Track, 2014.
[15] K. H. Low, J. Yu, J. Chen, and P. Jaillet. Parallel Gaussian process regression for big data: Low-rank
representation meets Markov approximation. In Proc. AAAI, pages 2821–2827, 2015.
[16] G. Neu and C. Szepesvári. Apprenticeship learning using inverse reinforcement learning and gradient
methods. In Proc. UAI, pages 295–302, 2007.
[17] G. Neu and C. Szepesvári. Training parsers by inverse reinforcement learning. Machine Learning, 77(2–
3):303–337, 2009.
[18] P. Newson and J. Krumm. Hidden Markov map matching through noise and sparseness. In Proc. 17th
ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages
336–343, 2009.
[19] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proc. ICML, 2000.
[20] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc.
IEEE, 77(2):257–286, 1989.
[21] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proc. IJCAI, pages 2586–
2591, 2007.
[22] S. Russell. Learning agents for uncertain environments. In Proc. COLT, pages 101–103, 1998.
[23] U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In Proc.
ICML, pages 1032–1039, 2008.
[24] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Proc. NIPS, pages
1449–1456, 2007.
[25] N. Xu, K. H. Low, J. Chen, K. K. Lim, and E. B. Özgül. GP-Localize: Persistent mobile robot localization
using online sparse Gaussian process observation model. In Proc. AAAI, pages 2585–2592, 2014.
[26] J. Yu, K. H. Low, A. Oran, and P. Jaillet. Hierarchical Bayesian nonparametric approach to modeling and
learning the wisdom of crowds of urban traffic route planning agents. In Proc. IAT, pages 478–485, 2012.
[27] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning.
In Proc. AAAI, pages 1433–1438, 2008.
9
