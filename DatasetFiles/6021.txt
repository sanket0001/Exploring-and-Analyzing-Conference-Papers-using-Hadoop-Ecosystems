


Paper ID = 6021
Title = Efficient Compressive Phase Retrieval
with Constrained Sensing Vectors
Sohail Bahmani, Justin Romberg
School of Electrical and Computer Engineering.
Georgia Institute of Technology
Atlanta, GA 30332
{sohail.bahmani,jrom}@ece.gatech.edu
Abstract
We propose a robust and efficient approach to the problem of compressive phase
retrieval in which the goal is to reconstruct a sparse vector from the magnitude
of a number of its linear measurements. The proposed framework relies on con-
strained sensing vectors and a two-stage reconstruction method that consists of
two standard convex programs that are solved sequentially.
In recent years, various methods are proposed for compressive phase retrieval, but
they have suboptimal sample complexity or lack robustness guarantees. The main
obstacle has been that there is no straightforward convex relaxations for the type
of structure in the target. Given a set of underdetermined measurements, there is a
standard framework for recovering a sparse matrix, and a standard framework for
recovering a low-rank matrix. However, a general, efficient method for recovering
a jointly sparse and low-rank matrix has remained elusive.
Deviating from the models with generic measurements, in this paper we show that
if the sensing vectors are chosen at random from an incoherent subspace, then the
low-rank and sparse structures of the target signal can be effectively decoupled.
We show that a recovery algorithm that consists of a low-rank recovery stage fol-
lowed by a sparse recovery stage will produce an accurate estimate of the target
when the number of measurements is O(k log dk ), where k and d denote the spar-
sity level and the dimension of the input signal. We also evaluate the algorithm
through numerical simulation.
1 Introduction
1.1 Problem setting
The problem of Compressive Phase Retrieval (CPR) is generally stated as the problem of estimating
a k-sparse vector x? âˆˆ Rd from noisy measurements of the form
yi = |ã€ˆai,x?ã€‰|2 + zi (1)
for i = 1, 2, . . . , n, where ai is the sensing vector and zi denotes the additive noise. In this paper,
we study the CPR problem with specific sensing vectors ai of the form
ai = Î¨
Twi, (2)
where Î¨ âˆˆ RmÃ—d and wi âˆˆ Rm are known. In words, the measurement vectors live in a fixed
low-dimensional subspace (i.e, the row space of Î¨ ). These types of measurements can be applied in
imaging systems that have control over how the scene is illuminated; examples include systems that
use structured illumination with a spatial light modulator or a scattering medium [1, 2].
1
By a standard lifting of the signal x? to X? = x?x?T, the quadratic measurements (1) can be
expressed as
yi =
âŒ©
aia
T
i ,X
?
âŒª
+ zi =
âŒ©
Î¨Twiw
T
i Î¨ ,X
?
âŒª
+ zi. (3)
With the linear operatorW and A defined as
W :B 7â†’
[âŒ©
wiw
T
i ,B
âŒª]n
i=1
and A :X 7â†’ W
(
Î¨XÎ¨T
)
,
we can write the measurements compactly as
y = A (X?) + z.
Our goal is to estimate the sparse, rank-one, and positive semidefinite matrixX? from the measure-
ments (3), which also solves the CPR problem and provides an estimate for the sparse signal x? up
to the inevitable global phase ambiguity.
Assumptions We make the following assumptions throughout the paper.
A1. The vectorswi are independent and have the standard Gaussian distribution on Rm: wi âˆ¼
N (0, I) .
A2. The matrix Î¨ is a restricted isometry matrix for 2k-sparse vectors and for a constant Î´2k âˆˆ
[0, 1]. Namely, it obeys
(1âˆ’ Î´2k) â€–xâ€–22â‰¤ â€–Î¨xâ€–
2
2 â‰¤ (1 + Î´2k) â€–xâ€–
2
2 , (4)
for all 2k-sparse vectors x âˆˆ Rd.
A3. The noise vector z is bounded as â€–zâ€–2 â‰¤ Îµ.
As will be seen in Theorem 1 and its proof below, the Gaussian distribution imposed by the assump-
tion A1 will be used merely to guarantee successful estimation of a rank-one matrix through trace
norm minimization. However, other distributions (e.g., uniform distribution on the unit sphere) can
also be used to obtain similar guarantees. Furthermore, the restricted isometry condition imposed
by the assumption A2 is not critical and can be replaced by weaker assumptions. However, the guar-
antees obtained under these weaker assumptions usually require more intricate derivations, provide
weaker noise robustness, and often do not hold uniformly for all potential target signals. Therefore,
to keep the exposition simple and straightforward we assume (4) which is known to hold (with high
probability) for various ensembles of random matrices (e.g., Gaussian, Rademacher, partial Fourier,
etc). Because in many scenarios we have the flexibility of selecting Î¨ , the assumption (4) is realistic
as well.
Notation Let us first set the notation used throughout the paper. Matrices and vectors are denoted
by bold capital and small letters, respectively. The set of positive integers less than or equal to
n is denoted by [n]. The notation f = O (g) is used when f = cg for some absolute constant
c > 0. For any matrix M , the Frobenius norm, the nuclear norm, the entrywise `1-norm, and the
largest entrywise absolute value of the entries are denoted by â€–Mâ€–F , â€–Mâ€–âˆ—, â€–Mâ€–1, and â€–Mâ€–âˆž,
respectively. To indicate that a matrixM is positive semidefinite we writeM < 0.
1.2 Contributions
The main challenge in the CPR problem in its general formulation is to design an accurate estimator
that has optimal sample complexity and computationally tractable. In this paper we address this
challenge in the special setting where the sensing vectors can be factored as (2). Namely, we propose
an algorithm that
â€¢ provably produces an accurate estimate of the lifted targetX? from only n = O
(
k log dk
)
measurements, and
â€¢ can be computed in polynomial time through efficient convex optimization methods.
2
1.3 Related work
Several papers including [3, 4, 5, 6, 7] have already studied the application of convex programming
for (non-sparse) phase retrieval (PR) in various settings and have established estimation accuracy
through different mathematical techniques. These phase retrieval methods attain nearly optimal
sample complexities that scales with the dimension of the target signal up to a constant factor [4, 5, 6]
or at most a logarithmic factor [3]. However, to the best of our knowledge, the exiting methods for
CPR either lack accuracy and robustness guarantees or have suboptimal sample complexities.
The problem of recovering a sparse signal from the magnitude of its subsampled Fourier transforms
is cast in [8] as an `1-minimization with non-convex constraints. While [8] shows that a sufficient
number of measurements would grow quadratically in k (i.e., the sparsity of the signal), the numer-
ical simulations suggest that the non-convex method successfully estimates the sparse signal with
only about k log dk measurements. Another non-convex approach to CPR is considered in [9] which
poses the problem as finding a k-sparse vector that minimizes the residual error that takes a quartic
form. A local search algorithm called GESPAR [10] is then applied to (approximate) the solution
to the formulated sparsity-constrained optimization. This approach is shown to be effective through
simulations, but it also lacks global convergence or statistical accuracy guarantees. An alternating
minimization method for both PR and CPR is studied in [11]. This method is appealing in large
scale problems because of computationally inexpensive iterations. More importantly, [11] proposes
a specific initialization using which the alternating minimization method is shown to converge lin-
early in noise-free PR and CPR. However, the number of measurements required to establish this
convergence is effectively quadratic in k. In [12] and [13] the `1-regularized form of the trace
minimization
argmin
X<0
trace (X) + Î» â€–Xâ€–1
subject to A (X) = y
(5)
is proposed for the CPR problem. The guarantees of [13] are based on the restricted isometry prop-
erty of the sensing operator X 7â†’ [ã€ˆaiaâˆ—i ,Xã€‰]
n
i=1 for sparse matrices. In [12], however, the anal-
ysis is based on construction of a dual certificate through an adaptation of the golfing scheme [14].
Assuming standard Gaussian sensing vectors ai and with appropriate choice of the regularization
parameter Î», it is shown in [12] that (5) solves the CPR when n = O
(
k2 log d
)
. Furthermore, this
method fails to recover the target sparse and rank-one matrix if n is dominated by k2. Estimation
of simultaneously structured matrices through convex relaxations similar to (5) is also studied in
[15] where it is shown that these methods do not attain optimal sample complexity. More recently,
assuming that the sparse target has a Bernoulli-Gaussian distribution, a generalized approximate
message passing framework is proposed in [16] to solve the CPR problem. Performance of this
method is evaluated through numerical simulations for standard Gaussian sensing matrices which
show the empirical phase transition for successful estimation occurs at n = O
(
k log dk
)
and also
the algorithms can have a significantly lower runtime compared to some of the competing algo-
rithms including GESPAR [10] and CPRL [13]. The PhaseCode algorithm is proposed in [17] to
solve the CPR problem with sensing vectors designed using sparse graphs and techniques adapted
from coding theory. Although PhaseCode is shown to achieve the optimal sample complexity, it
lacks robustness guarantees.
While preparing the final version of the current paper, we became aware of [18] which has indepen-
dently proposed an approach similar to ours to address the CPR problem.
2 Main Results
2.1 Algorithm
We propose a two-stage algorithm outlined in Algorithm 1. Each stage of the algorithm is a convex
program for which various efficient numerical solvers exists. In the first stage we solve (6) to obtain
a low-rank matrix BÌ‚ which is an estimator of the matrix
B? = Î¨X?Î¨T.
3
Then BÌ‚ is used in the second stage of the algorithm as the measurements for a sparse estimation
expressed by (7). The constraint of (7) depends on an absolute constant C > 0 that should be
sufficiently large.
Algorithm 1:
input : the measurements y, the operatorW , and the matrix Î¨
output: the estimate XÌ‚
1 Low-rank estimation stage:
BÌ‚ âˆˆ argmin
B<0
trace (B)
subject to â€–W (B)âˆ’ yâ€–2 â‰¤ Îµ
(6)
2 Sparse estimation stage:
XÌ‚ âˆˆ argmin
X
â€–Xâ€–1
subject to
âˆ¥âˆ¥âˆ¥Î¨XÎ¨T âˆ’ BÌ‚âˆ¥âˆ¥âˆ¥
F
â‰¤ CÎµâˆš
n
(7)
Post-processing. The result of the low-rank estimation stage (6) is generally not rank-one. Simi-
larly, the sparse estimation stage does not necessarily produce a XÌ‚ that is k Ã— k-sparse (i.e., it has
at most k nonzero rows and columns) and rank-one. In fact, since we have not imposed the posi-
tive semidefiniteness constraint (i.e., X < 0) in (7), the estimate XÌ‚ is not even guaranteed to be
positive semidefinite (PSD). However, we can enforce the rank-one or the sparsity structure in post-
processing steps simply by projecting the produced estimate on the set of rank-one or k Ã— k-sparse
PSD matrices. The simple but important observation is that projecting XÌ‚ onto the desired sets at
most doubles the estimation error. This fact is shown by Lemma 2 in Section 4 in a general setting.
Alternatives. There are alternative convex relaxations for the low-rank estimation and the sparse
estimation stages of Algorithm (1). For example, (6) can be replaced by its regularized least squares
analog
BÌ‚ âˆˆ argmin
B<0
1
2
â€–W (B)âˆ’ yâ€–22 + Î» â€–Bâ€–âˆ— ,
for an appropriate choice of the regularization parameter Î». Similarly, instead of (7) we can use
an `1-regularized least squares. Furthermore, to perform the low-rank estimation and the sparse
estimation we can use non-convex greedy type algorithms that typically have lower computational
costs. For example, the low-rank estimation stage can be performed via the Wirtinger flow method
proposed in [19]. Furthermore, various greedy compressive sensing algorithms such as the Iterative
Hard Thresholding [20] and CoSaMP [21] can be used to solve the desired sparse estimation. To
guarantee the accuracy of these compressive sensing algorithms, however, we might need to adjust
the assumption A2 to have the restricted isometry property for ck-sparse vectors with c being some
small positive integer.
2.2 Accuracy guarantees
The following theorem shows that any solution of the proposed algorithm is an accurate estimator
ofX?.
Theorem 1. Suppose that the assumptions A1, A2, and A3 hold with a sufficiently small constant
Î´2k. Then, there exist positive absolute constants C1, C2, and C3 such that if
n â‰¥ C1m, (8)
then any estimate XÌ‚ of the Algorithm 1 obeysâˆ¥âˆ¥âˆ¥XÌ‚ âˆ’X?âˆ¥âˆ¥âˆ¥
F
â‰¤ C2Îµâˆš
n
,
4
for all rank-one and k Ã— k-sparse matricesX? < 0 with probability exceeding 1âˆ’ eâˆ’C3n.
The proof of Theorem 1 is straightforward and is provided in Section 4. The main idea is first to
show the low-rank estimation stage produces an accurate estimate of B?. Because this stage can
be viewed as a standard phase retrieval through lifting, we can simply use accuracy guarantees that
are already established in the literature (e.g., [3, 6, 5]). In particular, we use [5, Theorem 2] which
established an error bound that holds uniformly for all valid B?. Thus we can ensure that X? is
feasible in the sparse estimation stage. Then the accuracy of the sparse estimation stage can also be
established by a simple adaptation of the analyses based on the restricted isometry property such as
[22].
The dependence of n (i.e., the number of measurements) and k (i.e., the sparsity of the signal) is
not explicit in Theorem 1. This dependence is absorbed in m which must be sufficiently large for
Assumption A2 to hold. Considering a Gaussian matrix Î¨ , the following corollary gives a concrete
example where the dependence of non k through m is exposed.
Corollary 1. Suppose that the assumptions of Theorem 1 including (8) hold. Furthermore, suppose
that Î¨ is a Gaussian matrix with iid N
(
0, 1m
)
entries and
m â‰¥ c1k log
d
k
, (9)
for some absolute constant c1 > 0. Then any estimate XÌ‚ produced by Algorithm 1 obeysâˆ¥âˆ¥âˆ¥XÌ‚ âˆ’X?âˆ¥âˆ¥âˆ¥
F
â‰¤ C2Îµâˆš
n
,
for all rank-one and kÃ—k-sparse matricesX? < 0 with probability exceeding 1âˆ’3eâˆ’c2m for some
constant c2 > 0.
Proof. It is well-known that if Î¨ has iid N
(
0, 1m
)
and we have (9) then (4) holds with high prob-
ability. For example, using a standard covering argument and a union bound [23] shows that if
(9) holds for a sufficiently large constant c1 > 0 then we have (4) for a sufficiently small con-
stant Î´2k with probability exceeding 1 âˆ’ 2eâˆ’cm for some constant c > 0 that depends only
on Î´2k. Therefore, Theorem 1 yields the desired result which holds with probability exceeding
1âˆ’ 2eâˆ’cm âˆ’ eâˆ’C3n â‰¥ 1âˆ’ 3eâˆ’c2m for some constant c2 > 0 depending only on Î´2k.
3 Numerical Experiments
We evaluated the performance of Algorithm 1 through some numerical simulations. The low-rank
estimation stage and the sparse estimation stage are implemented using the TFOCS package [24].
We considered the target k-sparse signal x? to be in R256 (i.e., d = 256). The support set of
of the target signal is selected uniformly at random and the entry values on this support are drawn
independently from N (0, 1). The noise vector z is also Gaussian with independent N
(
0, 10âˆ’4
)
. The
operatorW and the matrix Î¨ are drawn from some Gaussian ensembles as described in Corollary
1. We measured the relative error
â€–XÌ‚âˆ’X?â€–
F
â€–X?â€–F
of achieved by the compared methods over 100 trials
with sparsity level (i.e., k) varying in the set {2, 4, 6, . . . , 20}.
In the first experiment, for each value of k, the pair (m,n) that determines the size W and Î¨ are
selected from {(8k, 24k) , (8k, 32k) , (12k, 36k) , (12k, 48k) , (16k, 48k)}. Figure 1 illustrates the
0.9 quantiles of the relative error versus k for the mentioned choices of m.
In the second experiment we compared the performance of Algorithm 1 to the convex optimization
methods that do not exploit the structure of the sensing vectors. The setup for this experiment is the
same as in the first experiment except for the size ofW and Î¨ ; we chosem =
âŒˆ
2k
(
1 + log dk
)âŒ‰
and
n = 3m, where dre denotes the smallest integer greater than r. Figure 2 illustrates the 0.9 quantiles
of the measured relative errors for Algorithm 1, the semidefinite program (5) for Î» = 0 and Î» = 0.2,
and the `1-minimization
argmin
X
â€–Xâ€–1
subject to A (X) = y,
5
Figure 1: The empirical 0.9 quantile of the relative estimation error vs. sparsity for various choices
of m and n with d = 256.
Figure 2: The empirical 0.9 quantile of the relative estimation error vs. sparsity for Algorithm 1
and different trace- and/or `1- minimization methods with d = 256, m =
âŒˆ
2k
(
1 + log dk
)âŒ‰
, and
n = 3m.
which are denoted by 2-stage, SDP, SDP+`1, and `1, respectively. The SDP-based method did not
perform significantly different for other values of Î» in our complementary simulations. The relative
error for each trial is also overlaid in Figure 2 visualize its empirical distribution. The empirical
performance of the algorithms are in agreement with the theoretical results. Namely in a regime
where n = O (m) = O
(
k log dk
)
, Algorithm 1 can produce accurate estimates whereas while the
other approaches fail in this regime. The SDP and SDP+`1 show nearly identical performance. The
`1-minimization, however, competes with Algorithm 1 for small values of k. This observation can be
explained intuitively by the fact that the `1-minimization succeeds with n = O
(
k2
)
measurements
which for small values of k can be sufficiently close to the considered n = 3
âŒˆ
2k
(
1 + log dk
)âŒ‰
measurements.
6
4 Proofs
Proof of Theorem 1. Clearly, B? = Î¨X?Î¨T is feasible in 6 because of A3. Therefore, we can
show that any solution BÌ‚ of (6) accurately estimates B? using existing results on nuclear-norm
minimization. In particular, we can invoke [5, Theorem 2 and Section 4.3] which guarantees that for
some positive absolute constants C1, C â€²2, and C3 if (8) holds thenâˆ¥âˆ¥âˆ¥BÌ‚ âˆ’B?âˆ¥âˆ¥âˆ¥
F
â‰¤ C
â€²
2Îµâˆš
n
,
holds for all valid B? , thereby for all valid X?, with probability exceeding 1âˆ’ eâˆ’C3n. Therefore,
with C = C â€²2, the target matrix X
? would be feasible in (7). Now, it suffices to show that the
sparse estimation stage can produce an accurate estimate of X?. Recall that by A2, the matrix Î¨
is restricted isometry for 2k-sparse vectors. Let X be a matrix that is 2k Ã— 2k-sparse, i.e., a matrix
whose entries except for some 2k Ã— 2k submatrix are all zeros. Applying (4) to the columns of X
and adding the inequalities yield
(1âˆ’ Î´2k) â€–Xâ€–2F â‰¤ â€–Î¨Xâ€–
2
F â‰¤ (1 + Î´2k) â€–Xâ€–
2
F . (10)
Because the columns ofXTÎ¨T are also 2k-sparse we can repeat the same argument and obtain
(1âˆ’ Î´2k)
âˆ¥âˆ¥âˆ¥XTÎ¨Tâˆ¥âˆ¥âˆ¥2
F
â‰¤
âˆ¥âˆ¥âˆ¥Î¨XTÎ¨Tâˆ¥âˆ¥âˆ¥2
F
â‰¤ (1 + Î´2k)
âˆ¥âˆ¥âˆ¥XTÎ¨Tâˆ¥âˆ¥âˆ¥2
F
. (11)
Using the facts that
âˆ¥âˆ¥âˆ¥XTÎ¨Tâˆ¥âˆ¥âˆ¥
F
= â€–Î¨Xâ€–F and
âˆ¥âˆ¥âˆ¥Î¨XTÎ¨Tâˆ¥âˆ¥âˆ¥
F
=
âˆ¥âˆ¥âˆ¥Î¨XÎ¨Tâˆ¥âˆ¥âˆ¥
F
, the inequalities (10)
and (11) imply that
(1âˆ’ Î´2k)2 â€–Xâ€–2F â‰¤
âˆ¥âˆ¥âˆ¥Î¨XÎ¨Tâˆ¥âˆ¥âˆ¥2
F
â‰¤ (1 + Î´2k)2 â€–Xâ€–2F . (12)
The proof proceeds with an adaptation of the arguments used to prove accuracy of `1-minimization
in compressive sensing based on the restricted isometry property (see, e.g., [22]). LetE = XÌ‚âˆ’X?.
Furthermore, let S0 âŠ† [d]Ã— [d] denote the support set of the k Ã— k-sparse target X?. Define E0 to
be a d Ã— d matrix that is identical to E over the index set S0 and zero elsewhere. By optimality of
XÌ‚ and feasibility ofX? in (7) we have
â€–X?â€–1 â‰¥
âˆ¥âˆ¥âˆ¥XÌ‚âˆ¥âˆ¥âˆ¥
1
= â€–X? +E âˆ’E0 +E0â€–1 â‰¥ â€–X
? +E âˆ’E0â€–1 âˆ’ â€–E0â€–1
= â€–X?â€–1 + â€–E âˆ’E0â€–1 âˆ’ â€–E0â€–1 ,
where the last line follows from the fact thatX? andEâˆ’E0 have disjoint supports. Thus, we have
â€–E âˆ’E0â€–1 â‰¤ â€–E0â€–1 â‰¤ k â€–E0â€–F . (13)
Now consider a decomposition of E âˆ’E0 as the sum
E âˆ’E0 =
Jâˆ‘
j=1
Ej , (14)
such that for j â‰¥ 0 the dÃ— d matricesEj have disjoint support sets of size kÃ— k except perhaps for
the last few matrices that might have smaller supports. More importantly, the partitioning matrices
Ej are chosen to have a decreasing Frobenius norm (i.e., â€–Ejâ€–F â‰¥ â€–Ej+1â€–F ) for j â‰¥ 1. We haveâˆ¥âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
Jâˆ‘
j=2
Ej
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
F
â‰¤
Jâˆ‘
j=2
â€–Ejâ€–F â‰¤
1
k
Jâˆ‘
j=2
â€–Ejâˆ’1â€–1 â‰¤
1
k
â€–E âˆ’E0â€–1 â‰¤ â€–E0â€–F â‰¤ â€–E0 +E1â€–F , (15)
where the chain of inequalities follow from the triangle inequality, the fact that â€–Ejâ€–âˆž â‰¤
1
k2 â€–Ejâˆ’1â€–1 by construction, the fact that the matrices Ej have disjoint support and satisfy (14),
the bound (13), and the fact that E0 and E1 are orthogonal. Furthermore, we haveâˆ¥âˆ¥âˆ¥Î¨ (E0 +E1)Î¨Tâˆ¥âˆ¥âˆ¥2
F
=
âŒ©
Î¨ (E0 +E1)Î¨
T,Î¨
ï£«ï£­E âˆ’ Jâˆ‘
j=2
Ej
ï£¶ï£¸Î¨TâŒª
â‰¤
âˆ¥âˆ¥âˆ¥Î¨ (E0 +E1)Î¨Tâˆ¥âˆ¥âˆ¥
F
âˆ¥âˆ¥âˆ¥Î¨EÎ¨Tâˆ¥âˆ¥âˆ¥
F
+
1âˆ‘
i=0
Jâˆ‘
j=2
âˆ£âˆ£âˆ£âŒ©Î¨EiÎ¨T,Î¨EjÎ¨TâŒªâˆ£âˆ£âˆ£ ,
(16)
7
where the first term is obtained by the Cauchy-Schwarz inequality and the summation is obtained by
the triangle inequality. BecauseE = XÌ‚ âˆ’X? by definition, the triangle inequality and the fact that
X? and XÌ‚ are feasible in (7) imply that
âˆ¥âˆ¥âˆ¥Î¨EÎ¨Tâˆ¥âˆ¥âˆ¥
F
â‰¤
âˆ¥âˆ¥âˆ¥Î¨XÌ‚Î¨T âˆ’ BÌ‚âˆ¥âˆ¥âˆ¥
F
+
âˆ¥âˆ¥âˆ¥Î¨X?Î¨T âˆ’ BÌ‚âˆ¥âˆ¥âˆ¥
F
â‰¤
2CÎµâˆš
n
. Furthermore, Lemma 1 below which is adapted from [22, Lemma 2.1] guarantees that for
i âˆˆ {0, 1} and j â‰¥ 2 we have
âˆ£âˆ£âˆ£âŒ©Î¨EiÎ¨T,Î¨EjÎ¨TâŒªâˆ£âˆ£âˆ£ â‰¤ 2Î´2k â€–Eiâ€–F â€–Ejâ€–F . Therefore, we obtain
(1âˆ’ Î´2k)2 â€–E0 +E1â€–2F â‰¤
âˆ¥âˆ¥âˆ¥Î¨ (E0 +E1)Î¨Tâˆ¥âˆ¥âˆ¥2
F
â‰¤ 2CÎµâˆš
n
âˆ¥âˆ¥âˆ¥Î¨ (E0 +E1)Î¨Tâˆ¥âˆ¥âˆ¥
F
+ 2Î´2k
1âˆ‘
i=0
Jâˆ‘
j=2
â€–Eiâ€–F â€–Ejâ€–F
â‰¤ 2CÎµâˆš
n
(1 + Î´2k) â€–E0 +E1â€–F + 2Î´2k
1âˆ‘
i=0
Jâˆ‘
j=2
â€–Eiâ€–F â€–Ejâ€–F
â‰¤ 2CÎµâˆš
n
(1 + Î´2k) â€–E0 +E1â€–F + 2Î´2k (â€–E0â€–F + â€–E1â€–F ) â€–E0 +E1â€–F
â‰¤ â€–E0 +E1â€–F
(
2CÎµâˆš
n
(1 + Î´2k) + 2
âˆš
2Î´2k â€–E0 +E1â€–F
)
where the chain of inequalities follow from the lower bound in (12), the bound (16), the upper
bound in (12), the bound (15), and the fact that â€–E0â€–F + â€–E1â€–F â‰¤
âˆš
2 â€–E0 +E1â€–F . If Î´2k <
1 +
âˆš
2
(
1âˆ’
âˆš
1 +
âˆš
2
)
â‰ˆ 0.216, then we have Î³ := (1âˆ’ Î´2k)2 âˆ’ 2
âˆš
2Î´2k > 0 and thus
â€–E0 +E1â€–F â‰¤
2C (1 + Î´2k) Îµ
Î³
âˆš
n
.
Adding the above inequality to (13) and applying the triangle then yields the desired result.
Lemma 1. Let Î¨ be a matrix obeying (4). Then for any pair of k Ã— k-sparse matrices X and X â€²
with disjoint supports we haveâˆ£âˆ£âˆ£âŒ©Î¨XÎ¨T,Î¨X â€²Î¨TâŒªâˆ£âˆ£âˆ£ â‰¤ 2Î´2k â€–Xâ€–F âˆ¥âˆ¥X â€²âˆ¥âˆ¥F .
Proof. Suppose that X and X â€² have unit Frobenius norm. Using the identityâŒ©
Î¨XÎ¨T,Î¨X â€²Î¨T
âŒª
= 14
(âˆ¥âˆ¥âˆ¥Î¨ (X +X â€²)Î¨Tâˆ¥âˆ¥âˆ¥2
F
âˆ’
âˆ¥âˆ¥âˆ¥Î¨ (X âˆ’X â€²)Î¨Tâˆ¥âˆ¥âˆ¥2
F
)
and the fact that
X andX â€² have disjoint supports, it follows from (12) that
âˆ’2Î´2k =
(1âˆ’ Î´2k)2 âˆ’ (1 + Î´2k)2
2
â‰¤
âŒ©
Î¨XÎ¨T,Î¨X â€²Î¨T
âŒª
â‰¤ (1 + Î´2k)
2 âˆ’ (1âˆ’ Î´2k)2
2
= 2Î´2k.
The general result follows immediately as the desired inequality is homogeneous in the Frobenius
norms ofX andX â€².
Lemma 2 (Projected estimator). Let S be a closed nonempty subset of a normed vector space
(V, â€–Â·â€–). Suppose that for v? âˆˆ S we have an estimator vÌ‚ âˆˆ V, not necessarily in S, that obeys
â€–vÌ‚ âˆ’ v?â€– â‰¤ . If vÌƒ denotes a projection of vÌ‚ onto S, then we have â€–vÌƒ âˆ’ v?â€– â‰¤ 2.
Proof. By definition vÌƒ âˆˆ argminvâˆˆS â€–v âˆ’ vÌ‚â€– . Therefore, because v? âˆˆ S we have
â€–vÌƒ âˆ’ v?â€– â‰¤ â€–vÌ‚ âˆ’ v?â€–+ â€–vÌƒ âˆ’ vÌ‚â€– â‰¤ 2 â€–vÌ‚ âˆ’ v?â€– â‰¤ 2.
Acknowledgements
This work was supported by ONR grant N00014-11-1-0459, and NSF grants CCF-1415498 and
CCF-1422540.
8
References
[1] Jacopo Bertolotti, Elbert G. van Putten, Christian Blum, Ad Lagendijk, Willem L. Vos, and Allard P.
Mosk. Non-invasive imaging through opaque scattering layers. Nature, 491(7423):232â€“234, Nov. 2012.
[2] Antoine Liutkus, David Martina, SÃ©bastien Popoff, Gilles Chardon, Ori Katz, Geoffroy Lerosey, Sylvain
Gigan, Laurent Daudet, and Igor Carron. Imaging with nature: Compressive imaging using a multiply
scattering medium. Scientific Reports, volume 4, article no. 5552, Jul. 2014.
[3] Emmanuel J. CandÃ¨s, Thomas Strohmer, and Vladislav Voroninski. PhaseLift: Exact and stable signal
recovery from magnitude measurements via convex programming. Communications on Pure and Applied
Mathematics, 66(8):1241â€“1274, 2013.
[4] Emmanuel J. CandÃ¨s and Xiaodong Li. Solving quadratic equations via PhaseLift when there are about
as many equations as unknowns. Foundations of Computational Mathematics, 14(5):1017â€“1026, 2014.
[5] R. Kueng, H. Rauhut, and U. Terstiege. Low rank matrix recovery from rank one measurements. Applied
and Computational Harmonic Analysis, 2015. In press. Preprint arXiv:1410.6913 [cs.IT].
[6] Joel A. Tropp. Convex recovery of a structured signal from independent random linear measurements.
Preprint arXiv:1405.1102 [cs.IT], 2014.
[7] IrÃ¨ne Waldspurger, Alexandre dâ€™Aspremont, and StÃ©phane Mallat. Phase recovery, MaxCut and complex
semidefinite programming. Mathematical Programming, 149(1-2):47â€“81, 2015.
[8] Matthew L. Moravec, Justin K. Romberg, and Richard G. Baraniuk. Compressive phase retrieval. In
Proceedings of SPIE Wavelets XII, volume 6701, pages 670120 1â€“11, 2007.
[9] Yoav Shechtman, Yonina C. Eldar, Alexander Szameit, and Mordechai Segev. Sparsity based sub-
wavelength imaging with partially incoherent light via quadratic compressed sensing. Optics Express,
19(16):14807â€“14822, Aug. 2011.
[10] Yoav Shechtman, Amir Beck, and Yonina C. Eldar. GESPAR: Efficient phase retrieval of sparse signals.
Signal Processing, IEEE Transactions on, 62(4):928â€“938, Feb. 2014.
[11] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimization. In
Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 2796â€“2804, 2013.
[12] Xiaodong Li and Vladislav Voroninski. Sparse signal recovery from quadratic measurements via convex
programming. SIAM Journal on Mathematical Analysis, 45(5):3019â€“3033, 2013.
[13] Henrik Ohlsson, Allen Yang, Roy Dong, and Shankar Sastry. CPRLâ€“an extension of compressive sensing
to the phase retrieval problem. In Advances in Neural Information Processing Systems 25 (NIPS 2012),
pages 1367â€“1375, 2012.
[14] David Gross. Recovering low-rank matrices from few coefficients in any basis. Information Theory, IEEE
Transactions on, 57(3):1548â€“1566, Mar. 2011.
[15] Samet Oymak, Amin Jalali, Maryam Fazel, Yonina Eldar, and Babak Hassibi. Simultaneously structured
models with application to sparse and low-rank matrices. Information Theory, IEEE Transactions on,
61(5):2886â€“2908, 2015.
[16] P. Schniter and S. Rangan. Compressive phase retrieval via generalized approximate message passing.
Signal Processing, IEEE Transactions on, 63(4):1043â€“1055, February 2015.
[17] Ramtin Pedarsani, Kangwook Lee, and Kannan Ramchandran. Phasecode: Fast and efficient compressive
phase retrieval based on sparse-graph codes. In Communication, Control, and Computing (Allerton), 52nd
Annual Allerton Conference on, pages 842â€“849, Sep. 2014. Extended preprint arXiv:1408.0034
[cs.IT].
[18] Mark Iwen, Aditya Viswanathan, and Yang Wang. Robust sparse phase retrieval made easy. Applied and
Computational Harmonic Analysis, 2015. In press. Preprint arXiv:1410.5295 [math.NA].
[19] Emmanuel J. CandÃ¨s, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory
and algorithms. Information Theory, IEEE Transactions on, 61(4):1985â€“2007, Apr. 2015.
[20] Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Applied
and Computational Harmonic Analysis, 27(3):265â€“274, 2009.
[21] Deanna Needell and Joel A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate
samples. Applied and Computational Harmonic Analysis, 26(3):301â€“321, 2009.
[22] Emmanuel J. CandÃ¨s. The restricted isometry property and its implications for compressed sensing.
Comptes Rendus Mathematique, 346(9-10):589â€“592, 2008.
[23] Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted
isometry property for random matrices. Constructive Approximation, 28(3):253â€“263, 2008.
[24] Stephen R. Becker, Emmanuel J. CandÃ¨s, and Michael C. Grant. Templates for convex cone problems
with applications to sparse signal recovery. Mathematical Programming Computation, 3(3):165â€“218,
2011.
9
