


Paper ID = 5694
Title = Basis Refinement Strategies for Linear Value
Function Approximation in MDPs
Gheorghe Comanici
School of Computer Science
McGill University
Montreal, Canada
gcoman@cs.mcgill.ca
Doina Precup
School of Computer Science
McGill University
Montreal, Canada
dprecup@cs.mcgill.ca
Prakash Panangaden
School of Computer Science
McGill University
Montreal, Canada
prakash@cs.mcgill.ca
Abstract
We provide a theoretical framework for analyzing basis function construction for
linear value function approximation in Markov Decision Processes (MDPs). We
show that important existing methods, such as Krylov bases and Bellman-error-
based methods are a special case of the general framework we develop. We pro-
vide a general algorithmic framework for computing basis function refinements
which â€œrespectâ€ the dynamics of the environment, and we derive approximation
error bounds that apply for any algorithm respecting this general framework. We
also show how, using ideas related to bisimulation metrics, one can translate ba-
sis refinement into a process of finding â€œprototypesâ€ that are diverse enough to
represent the given MDP.
1 Introduction
Finding optimal or close-to-optimal policies in large Markov Decision Processes (MDPs) requires
the use of approximation. A very popular approach is to use linear function approximation over a set
of features [Sutton and Barto, 1998, Szepesvari, 2010]. An important problem is that of determining
automatically this set of features in such a way as to obtain a good approximation of the problem
at hand. Many approaches have been explored, including adaptive discretizations [Bertsekas and
Castanon, 1989, Munos and Moore, 2002], proto-value functions [Mahadevan, 2005], Bellman error
basis functions (BEBFs) [Keller et al., 2006, Parr et al., 2008a], Fourier basis [Konidaris et al.,
2011], feature dependency discovery [Geramifard et al., 2011] etc. While many of these approaches
have nice theoretical guarantees when constructing features for fixed policy evaluation, this problem
is significantly more difficult in the case of optimal control, where multiple policies have to be
evaluated using the same representation.
We analyze this problem by introducing the concept of basis refinement, which can be used as a gen-
eral framework that encompasses a large class of iterative algorithms for automatic feature extrac-
tion. The main idea is to start with a set of basis which are consistent with the reward function, i.e.
which allow only states with similar immediate reward to be grouped together. One-step look-ahead
is then used to find parts of the state space in which the current basis representation is inconsistent
with the environment dynamics, and the basis functions are adjusted to fix this problem. The process
continues iteratively. We show that BEBFs [Keller et al., 2006, Parr et al., 2008a] can be viewed as
a special case of this iterative framework. These methods iteratively expand an existing set of basis
functions in order to capture the residual Bellman error. The relationship between such features and
augmented Krylov bases allows us to show that every additional feature in these sets is consistently
refining intermediate bases. Based on similar arguments, it can be shown that other methods, such
as those based on the concept of MDP homomorphisms [Ravindran and Barto, 2002], bisimulation
metrics [Ferns et al., 2004], and partition refinement algorithms [Ruan et al., 2015], are also spe-
cial cases of the framework. We provide approximation bounds for sequences of refinements, as
1
well as a basis convergence criterion, using mathematical tools rooted in bisimulation relations and
metrics [Givan et al., 2003, Ferns et al., 2004].
A final contribution of this paper is a new approach for computing alternative representations based
on a selection of prototypes that incorporate all the necessary information to approximate values
over the entire state space. This is closely related to kernel-based approaches [Ormoneit and Sen,
2002, Jong and Stone, 2006, Barreto et al., 2011], but we do not assume that a metric over the state
space is provided (which allows one to determine similarity between states). Instead, we use an
iterative approach, in which prototypes are selected to properly distinguish dynamics according to
the current basis functions, then a new metric is estimated, and the set of prototypes is refined again.
This process relies on using pseudometrics which in the limit converge to bisimulation metrics.
2 Background and notation
We will use the framework of Markov Decision Processes, consisting of a finite state space S, a
finite action space A, a transition function P : (S Ã— A) â†’ P(S)1, where P (s, a) is a probability
distribution over the state space S, a reward function2 R : (SÃ—A)â†’ R. For notational convenience,
P a(s), Ra(s) will be used to denote P (s, a) and R(s, a), respectively. One of the main objectives
of MDP solvers is to determine a good action choice, also known as a policy, from every state that
the system would visit. A policy Ï€ : S â†’ P(A) determines the probability of choosing each action
a given the state s (with
âˆ‘
aâˆˆA Ï€(s)(a) = 1). The value of a policy Ï€ given a state s0 is defined as
V Ï€(s0) = E
[âˆ‘âˆ
i=0 Î³
iRai(si)
âˆ£âˆ£ si+1 âˆ¼ P ai(si), ai âˆ¼ Ï€(si)] .
Note that V Ï€ is a real valued function [[S â†’ R]]; the space of all such functions will be denoted
by FS . We will also call such functions features. Let RÏ€ and PÏ€ denote the reward and tran-
sition probabilities corresponding to choosing actions according to Ï€. Note that RÏ€ âˆˆ FS and
PÏ€ âˆˆ [[FS â†’ FS ]], where3 RÏ€(s) = Eaâˆ¼Ï€(s)[Ra(s)] and PÏ€(f)(s) = Eaâˆ¼Ï€(s)
[
EPa(s)[f ]
]
. Let
TÏ€ âˆˆ [[FS â†’ FS ]] denote the Bellman operator: TÏ€(f) = RÏ€ + Î³PÏ€(f). This operator is linear
and V Ï€ is its fixed point, i.e. TÏ€(V Ï€) = V Ï€ . Most algorithms for solving MDPs will either use
the model (RÏ€, PÏ€) to find V Ï€ (if this model is available and/or can be estimated efficiently), or
they will estimate V Ï€ directly using samples of the model, {(si, ai, ri, si+1)}âˆi=0. The value V âˆ—
associated with the best policy Ï€âˆ— is the fixed point of the Bellman optimality operator T âˆ— (not a
linear operator), defined as: T âˆ—(f) = maxaâˆˆA (Ra + Î³P a(f)).
The main problem we address in this paper is that of finding alternative representations for a given
MDP. In particular, we look for finite, linearly independent subsets Î¦ ofFS . These are bases for sub-
spaces that will be used to speed up the search for V Ï€ , by limiting it to span(Î¦). We say that a basis
B is a partition if there exists an equivalence relation âˆ¼ on S such that B = {Ï‡(C) | C âˆˆ S/âˆ¼},
where Ï‡ is the characteristic function (i.e. Ï‡(X)(x) = 1 if x âˆˆ X and 0 otherwise). Given any
equivalence relation âˆ¼, we will use the notation âˆ†(âˆ¼) for the set of characteristic functions on the
equivalence classes of âˆ¼, i.e. âˆ†(âˆ¼) = {Ï‡(C) | C âˆˆ S/âˆ¼}.4.
Our goal will be to find subsets Î¦ âŠ‚ FS which allow a value function approximation with strong
quality guarantees. More precisely, for any policy Ï€ we would like to approximate V Ï€ with
V Ï€Î¦ =
âˆ‘k
i=1 wiÏ†i for some choice of wiâ€™s, which amounts to finding the best candidate inside
the space spanned by Î¦ = {Ï†1, Ï†2, ..., Ï†k}. A sufficient condition for V Ï€ to be an element of
span(Î¦) (and therefore representable exactly using the chosen set of bases), is for Î¦ to span
the reward function and be an invariant subspace of the transition function: RÏ€ âˆˆ span(Î¦) and
âˆ€f âˆˆ Î¦, PÏ€(f) âˆˆ span(Î¦). Linear fixed point methods like TD, LSTD, LSPE [Sutton, 1988,
Bradtke and Barto, 1996, Yu and Bertsekas, 2006] can be used to find the least squares fixed point
approximation V Ï€Î¦ of V
Ï€ for a representation Î¦; these constitute proper approximation schemes, as
1We will use P(X) to denote the set of probability distributions on a given set X .
2For simplicity, we assume WLOG that the reward is deterministic and independent of the state into which
the system arrives.
3We will use EÂµ[f ] =
âˆ‘
x f(x)Âµ(x) to mean the expectation of a function f wrt distribution Âµ. If the
function f is multivariate, we will use Exâˆ¼Âµ[f(x, y)] =
âˆ‘
x f(x, y)Âµ(x) to denote expectation of f when y is
fixed.
4The equivalence class of an element s âˆˆ S is {sâ€² âˆˆ S | s âˆ¼ sâ€²}. S/âˆ¼ is used for the quotient set of all
equivalence classes of âˆ¼.
2
one can determine the number of iterations required to achieve a desired approximation error. Given
a representation Î¦, the approximate value function V Ï€Î¦ is the fixed point of the operator T
Ï€
Î¦ , defined
as: TÏ€Î¦f := Î Î¦(R
Ï€ + Î³PÏ€(f)), where Î Î¦ is the orthogonal projection operator on Î¦. Using the
linearity of Î Î¦, it directly follows that TÏ€Î¦(f) = Î Î¦R
Ï€ + Î³Î Î¦P
Ï€(f) and V Ï€Î¦ is the fixed point of
the Bellman operator over the transformed linear model (RÏ€Î¦, P
Ï€
Î¦) := (Î Î¦R
Ï€,Î Î¦P
Ï€). For more
details, see [Parr et al., 2008a,b].
The analysis tools that we will use to establish our results are based on probabilistic bisimulation and
its quantitative analogues. Strong probabilistic bisimulation is a notion of behavioral equivalence
between the states of a probabilistic system, due to [Larsen and Skou, 1991] and applied to MDPs
with rewards by [Givan et al., 2003]. The metric analog is due to [Desharnais et al., 1999, 2004] and
the extension of the metric to include rewards is due to [Ferns et al., 2004]. An equivalence relation
âˆ¼ is a a bisimulation relation on the state space S if for every pair (s, sâ€²) âˆˆ SÃ—S, s âˆ¼ sâ€² if and only
if âˆ€a âˆˆ A,âˆ€C âˆˆ S/âˆ¼, Ra(s) = Ra(sâ€²) and P a(s)(C) = P a(sâ€²)(C) (we use here P a(s)(C) to de-
note the probability of transitioning into C, under transition s, a). A pseudo-metric is a bisimulation
metric if there exists some bisimulation relation âˆ¼ such that âˆ€s, sâ€², d(s, sâ€²) = 0 â‡â‡’ s âˆ¼ sâ€².
The bisimulation metrics described by [Ferns et al., 2004] are constructed using the Kantorovich
metric for comparing two probability distributions. Given a ground metric d over S, the Kantorovich
metric over P(S) takes the largest difference in the expected value of Lipschitz-1 functions with
respect to d: â„¦(d) := {f âˆˆ FS | âˆ€s, sâ€², f(s) âˆ’ f(sâ€²) â‰¤ d(s, sâ€²)}. The distance between two
probabilities Âµ and Î½ is computed as: K(d) : (Âµ, Î½) 7â†’ supÏ•âˆˆâ„¦(d) EÂµ[Ï•]âˆ’ EÎ½ [Ï•]. For more details
on the Kantorovich metric, see [Villani, 2003]. The following approximation scheme converges to
a bisimulation metric (starting with d0 = 0, the metric that associates 0 to all pairs):
dk+1(s, s
â€²) = T (dk)(s, sâ€²) := max
a
(
(1âˆ’ Î³)
âˆ£âˆ£Ra(s)âˆ’Ra(sâ€²)âˆ£âˆ£+ Î³K(dk)(P a(s), P a(sâ€²))). (1)
The operator T has a fixed point dâˆ—, which is a bisimulation metric, and dk â†’ dâˆ— as k â†’âˆ. [Ferns
et al., 2004] provide bounds which allow one to assess the quality of general state aggregations using
this metric. Given a relationâˆ¼ and its corresponding partition âˆ†(âˆ¼), one can define an MDP model
over âˆ†(âˆ¼) as: RÌ‚a = Î âˆ†(âˆ¼)Ra and PÌ‚ a = Î âˆ†(âˆ¼)P a, âˆ€a âˆˆ A. The approximation error between the
true MDP optimal value function V âˆ— and its approximation using this reduced MDP model, denoted
by V âˆ—âˆ†(âˆ¼), is bounded above by:âˆ£âˆ£âˆ£V âˆ—âˆ†(âˆ¼)(s)âˆ’ V âˆ—(s)âˆ£âˆ£âˆ£ â‰¤ 11âˆ’ Î³ dâˆ—âˆ¼(s) + maxsâ€²âˆˆS Î³(1âˆ’ Î³)2 dâˆ—âˆ¼(sâ€²). (2)
where dâˆ—âˆ¼(s) is average distance from a state s to its âˆ¼-equivalence class, defined as an expectation
over the uniform distribution U : dâˆ—âˆ¼(s) = EsÌ‚âˆ¼U [dâˆ—(s, sÌ‚) | s âˆ¼ sÌ‚]. Similar bounds for representa-
tions that are not partitions can be found in [Comanici and Precup, 2011]. Note that these bounds
are minimized by aggregating states which are â€œcloseâ€ in terms of the bisimulation distance dâˆ—.
3 Basis refinement
In this section we describe the proposed basis refinement framework, which relies on â€œdetectingâ€
and â€œfixingâ€ inconsistencies in the dynamics induced by a given set of features. Intuitively, states
are dynamically consistent with respect to a set of basis functions if transitions out of these states
are evaluated the same way by the model {P a | a âˆˆ A}. Inconsistencies are â€œfixedâ€ by augmenting
a basis with features that are able to distinguish inconsistent states, relative to the initial basis. We
are now ready to formalize these ideas.
Definition 3.1. Given a subset F âŠ‚ FS , two states s, sâ€² âˆˆ S are consistent with respect to F ,
denoted s âˆ¼F sâ€², if âˆ€f âˆˆ F,âˆ€a âˆˆ A, f(s) = f(sâ€²) and EPa(s)[f ] = EPa(sâ€²)[f ].
Definition 3.2. Given two subspaces F,G âŠ‚ FS , G refines F in an MDP M , and write F nG, if
F âŠ† G and
âˆ€s, sâ€² âˆˆ S, s âˆ¼F sâ€² â‡â‡’ [âˆ€g âˆˆ G, g(s) = g(sâ€²)].
Using the linearity of expectation, one can prove that, given two probability distributions Âµ, Î½,
and a finite subset Î“ âŠ‚ F , if span(Î“) = F , then
[
âˆ€f âˆˆ F, EÂµ[f ] = EÎ½ [f ]
]
â‡â‡’[
âˆ€b âˆˆ Î“, EÂµ[b] = EÎ½ [b]
]
. For the special case of Dirac distributions Î´s and Î´sâ€² , for which
3
EÎ´s [f ] = f(s), it also holds that
[
âˆ€f âˆˆ F, f(s) = f(sâ€²)
]
â‡â‡’
[
âˆ€b âˆˆ Î“, b(s) = b(sâ€²)
]
.
Therefore, Def. 3.2 gives a relation between two subspaces, but the refinement conditions could be
checked on any basis choice. It is the subspace itself rather than a particular basis that matters, i.e.
Î“ n Î“â€² if span(Î“) n span(Î“â€²). To fix inconsistencies on a pair (s, sâ€²), for which we can find f âˆˆ Î“
and a âˆˆ A such that either f(s) 6= f(sâ€²) or EPa(s)[f ] 6= EPa(sâ€²)[f ], one should construct a new
function Ï• with Ï•(s) 6= Ï•(sâ€²) and add it to Î“â€². To guarantee that all inconsistencies have been
addressed, if Ï•(s) 6= Ï•(sâ€²) for some Ï• âˆˆ Î“â€², Î“ must contain a feature f such that, for some a âˆˆ A,
either f(s) 6= f(sâ€²) or EPa(s)[f ] 6= EPa(sâ€²)[f ].
In Sec. 5 we present an algorithmic framework consisting of sequential improvement steps, in which
a current basis Î“ is refined into a new one, Î“â€², with span(Î“) n span(Î“â€²). Def 3.2 guarantees that
following such strategies expands span(Î“) and that the approximation error for any policy will be
decreased as a result. We now discuss bounds that can be obtained based on these definitions.
3.1 Value function approximation results
One simple way to create a refinement is to add to Î“ a single element that would address all incon-
sistencies: a feature that is valued differently for every element of âˆ†(âˆ¼Î“). Given Ï‰ : âˆ†(âˆ¼Î“)â†’ R,[
âˆ€b, bâ€² âˆˆ âˆ†(âˆ¼Î“), b 6= bâ€² â‡’ Ï‰(b) 6= Ï‰(bâ€²)
]
â‡’ Î“ n Î“ âˆª
{âˆ‘
bâˆˆâˆ†(âˆ¼Î“) Ï‰(b)b
}
. On the other hand,
such a construction provides no approximation guarantee for the optimal value function (unless we
make additional assumptions on the problem - we will discuss this further in Section 3.2). Although
it addresses inconsistencies in the dynamics over the set of features spanned by Î“, it does not nec-
essarily provide the representation power required to properly approximate the value of the optimal
policy. The main theoretical result in this section provides conditions for describing refining se-
quences of bases, which are not necessarily accurate, but have approximation errors bounded by
an exponentially decreasing function. These results are based on âˆ†(âˆ¼Î“), the largest basis refining
subspace: any feature that is constant over equivalence classes of âˆ¼Î“ will be spanned by âˆ†(âˆ¼), i.e.
for any refinement V nW , V âŠ†W âŠ† span(âˆ†(âˆ¼V )). These subsets are convenient as they can be
analyzed using the bisimulation metric introduced in [Ferns et al., 2004].
Lemma 3.1. The bisimulation operator in Eq. 1) is a contraction with constant Î³. That is, for any
metric d over S, sups,sâ€²âˆˆS |T (d)(s, sâ€²)| â‰¤ Î³ sups,sâ€²âˆˆS |d(s, sâ€²)|.
The proof relies on the Monge-Kantorovich duality (see [Villani, 2003]) to check that T satisfies
sufficient conditions to be a contraction operator. An operator Z is a contraction (with constant
Î³ < 1) if Z(x) â‰¤ Z(xâ€²) whenever x â‰¤ xâ€², and if Z(x + c) = Z(x) + Î³c for any constant
c âˆˆ R [Blackwell, 1965]. One could easily check these conditions on the operator in Equation 1.
Theorem 3.1. Let âˆ¼0 represent reward consistency, i.e. s âˆ¼0 sâ€² â‡â‡’ âˆ€a âˆˆ A,Ra(s) = Ra(sâ€²),
and Î“1 = âˆ†(âˆ¼0). Additionally, assume {Î“n}âˆn=1 is a sequence of bases such that for all n â‰¥ 1,
Î“n n Î“n+1 and Î“n+1 is as large as the partition corresponding to consistency over Î“n, i.e.
|Î“n+1| = |S/âˆ¼Î“n |. If V âˆ—Î“n is the optimal value function computed with respect to representa-
tion Î“n, then
âˆ£âˆ£âˆ£âˆ£V âˆ—Î“n âˆ’ V âˆ—âˆ£âˆ£âˆ£âˆ£âˆ â‰¤ Î³n+1 sups,sâ€²,a |Ra(s)âˆ’Ra(sâ€²)|/(1âˆ’ Î³)2.
Proof. We will use the bisimulation metric defined in Eq. 1 and Eq. 2 applied to the special case of
reduced models over bases {Î“n}âˆn=1.
First, note that Monge-Kantorovich duality is crucial in this proof. It basically states that the Kan-
torovich metric is a solution to the Monge-Kantorovich problem, when its cost function is equal
to the base metric for the Kantorovich metric. Specifically, for two measures Âµ and Î½, and a cost
function f âˆˆ [S Ã— S â†’ R], the Monge-Kantorovich problem computes:
J (f)(Âµ, Î½) = inf{EÎ¾[f(x, y)] | Î¾ âˆˆ P(SÃ—S) s.t. Âµ, Î½ are the marginals corresponding to x and y}
The set of measures Î¾ with marginals Âµ and Î½ is also known as the set of couplings of Âµ and Î½. For
any metric d over S, J (d)(Âµ, Î½) = K(d)(Âµ, Î½) (for proof, see [Villani, 2003]).
Next, we describe a relation between the metric T n(0) and Î“n. Since
|Î“n+1| = |S/âˆ¼Î“n | = |âˆ†(âˆ¼Î“n)| and Î“n+1 âŠ† span(âˆ†(âˆ¼Î“n)), it must be the case that
span(Î“n+1) = span(âˆ†(âˆ¼Î“n)). It is not hard to see that for the special case of parti-
tions, a refinement can be determined based on transitions into equivalence classes. Given
4
two equivalence relations âˆ¼1 and âˆ¼2, the refinement âˆ†(âˆ¼1) n âˆ†(âˆ¼2) holds if and only if
s âˆ¼2 sâ€² â‡’ s âˆ¼1 sâ€² and s âˆ¼2 sâ€² â‡’
[
âˆ€a âˆˆ A,âˆ€C âˆˆ S/âˆ¼1 P a(s)(C) = P a(sâ€²)(C)
]
. In particular,
âˆ€s, sâ€² with s âˆ¼Î“n+1 sâ€², and âˆ€C âˆˆ S/âˆ¼Î“n , P a(s)(C) = P a(sâ€²)(C). This equality is crucial in
defining the following coupling for J (f)(P a(s), P a(sâ€²)): let Î¾C âˆˆ P(S Ã— S) be any coupling of
P a(s)|C and P a(sâ€²)|C , the restrictions of P a(s) and P a(sâ€²) to C; the latter is possible as the two
distributions are equal. Next, define the coupling Î¾ of Âµ and Î½ as Î¾ =
âˆ‘
CâˆˆS/âˆ¼Î“n
Î¾C . For any cost
function f , if s âˆ¼Î“n+1 sâ€², then J (f)(P a(s), P a(sâ€²)) â‰¤
âˆ‘
CâˆˆS/âˆ¼Î“n
EÎ¾C [f ].
Using an inductive argument, we will now show that âˆ€n, s âˆ¼Î“n sâ€² â‡’ T n(0)(s, sâ€²) = 0. The base
case is clear from the definition: s âˆ¼0 sâ€² â‡’ T (0)(s, sâ€²) = 0. Now, assume the former holds for n;
that is, âˆ€C âˆˆ S/âˆ¼Î“n , âˆ€s, sâ€² âˆˆ C, T n(0)(s, sâ€²) = 0. But Î¾C is zero everywhere except on the set
C Ã— C, so EÎ¾C [T n(0)] = 0. Combining the last two results, we get the following upper bound:
s âˆ¼Î“n+1 sâ€² â‡’ J (T n(0))(P a(s), P a(sâ€²)) â‰¤
âˆ‘
CâˆˆS/âˆ¼Î“n
EÎ¾C [T n(0)] = 0.
Since T n(0) is a metric, it also holds that J (T n(0))(P a(s), P a(sâ€²)) â‰¥ 0. Moreover, as s and
sâ€² are consistent over Î“n âŠ‡ âˆ†(âˆ¼0), this pair of states agree on the reward function. Therefore,
T n+1(0)(s, sâ€²) = maxa((1âˆ’ Î³)|Ra(s)âˆ’Ra(sâ€²)|+ Î³J (T n(0))(P a(s), P a(sâ€²))) = 0.
Finally, for any b âˆˆ âˆ†(âˆ¼Î“n) and s âˆˆ S with b(s) = 1, and any other state sÌ‚ with b(sÌ‚) = 1, it must
be the case that s âˆ¼Î“n sÌ‚ and T n(0)(s, sÌ‚) = 0. Therefore,
E
sÌ‚âˆ¼U
[dâˆ—(s, sÌ‚) | s âˆ¼Î“n sÌ‚] = E
sÌ‚âˆ¼U
[dâˆ—(s, sÌ‚)âˆ’ T n(0)(s, sÌ‚) | s âˆ¼Î“n sÌ‚] â‰¤ ||dâˆ— âˆ’ T n(0)||âˆ. (3)
As span(Î“n) = span(âˆ†(âˆ¼n)), V âˆ—Î“n is the optimal value function for the MDP model over âˆ†(âˆ¼n).
Based on (2) and (3), we can conclude thatâˆ£âˆ£âˆ£âˆ£V âˆ—Î“n âˆ’ V âˆ—âˆ£âˆ£âˆ£âˆ£âˆ â‰¤ Î³||dâˆ— âˆ’ T n(0)||âˆ/(1âˆ’ Î³)2. (4)
But we already know from Lemma 3.1 that dâˆ— (defined in Eq. 1) is the fixed point of a contraction
operator with constant Î³. As J (0)(Âµ, Î½) = 0, the following holds for all n â‰¥ 1
||dâˆ— âˆ’ T n(0)||âˆ â‰¤ Î³n||T (0)âˆ’ 0||âˆ/(1âˆ’ Î³) â‰¤ Î³n sup
s,sâ€²,a
|Ra(s)âˆ’Ra(sâ€²)|. (5)
The final result is easily obtained by putting together Equations 4 and 5.
The result of the theorem provides a strategy for constructing refining sequences with strong approx-
imation guarantees. Still, it might be inconvenient to generate refinements as large as S/âˆ¼Î“n , as
this might be over-complete; although faithful to the assumptions of the theorem, it might generate
features that distinguish states that are not often visited, or pairs of states which are only slightly
different. To address this issue, we provide a variation on the concept of refinement that can be used
to derive more flexible refining algorithms: refinements that concentrate on local properties.
Definition 3.3. Given a subset F âŠ‚ FS , and a subset Î¶ âŠ‚ S, two states s, sâ€² âˆˆ S are con-
sistent on Î¶ with respect to F , denoted s âˆ¼F,Î¶ sâ€², if âˆ€f âˆˆ F,âˆ€a âˆˆ A, f(s) = f(sâ€²) and
âˆ€sÌ‚ âˆˆ Î¶, EPa(sÌ‚)[f ] = EPa(s)[f ] â‡â‡’ EPa(sÌ‚)[f ] = EPa(sâ€²)[f ].
Definition 3.4. Given two subspaces F,G âŠ‚ FS , G refines F locally with respect to Î¶, denoted
F nÎ¶ G, if F âŠ† G and âˆ€s, sâ€² âˆˆ S, s âˆ¼F,Î¶ sâ€² â‡â‡’ [âˆ€g âˆˆ G, g(s) = g(sâ€²)].
Definition 3.2 is the special case of Definition 3.4 corresponding to a refinement with respect to the
whole state space S, i.e. F n G â‰¡ F nS G. When the subset Î¶ is not important, we will use
the notation V nâ—¦ W to say that W refines V locally with respect to some subset of S. The result
below states that even if one provides local refinements nâ—¦, one will eventually generate a pair of
subspaces which are related through a global refinement property n.
Proposition 3.1. Let {Î“i}ni=0 be a set of bases over S with Î“iâˆ’1 nÎ¶i Î“i, i = 1, ..., n, for some
{Î¶i}ni=1 . Assume that Î“n is the maximal refinement (i.e. |Î“n| = |S/âˆ¼Î“nâˆ’1,Î¶n |). Let Î· = âˆªiÎ¶i.
Then âˆ†(âˆ¼Î“0,Î·) âŠ† span(Î“n).
Proof. Assume s âˆ¼Î“nâˆ’1,Î¶n sâ€². We will check below all conditions necessary to conclude that
s âˆ¼Î“0,Î· sâ€². First, let f âˆˆ Î“0. It is immediate from the definition of local refinements that
âˆ€j â‰¤ nâˆ’ 1,Î“j âŠ† Î“nâˆ’1, so that s âˆ¼Î“0,Î¶n sâ€². It follows that âˆ€f âˆˆ Î“0, f(s) = f(sâ€²).
5
Next, fix f âˆˆ Î“0, a âˆˆ A and sÌ‚ âˆˆ Î·. If sÌ‚ âˆˆ Î¶n, then EPa(sÌ‚)[f ] = EPa(s)[f ] â‡â‡’
EPa(sÌ‚)[f ] = EPa(sâ€²)[f ], by the assumption above on the pair s, sâ€². Otherwise, âˆƒj < n such that
sÌ‚ âˆˆ Î¶j and Î“jâˆ’1 nÎ¶j Î“j . But we already know that âˆ€f âˆˆ Î“j , f(s) = f(sâ€²), as Î“j âŠ† Î“nâˆ’1. We
can use this result in the definition of local refinement Î“jâˆ’1 nÎ¶j Î“j to conclude that s âˆ¼Î“jâˆ’1,Î¶j sâ€².
Moreover, as sÌ‚ âˆˆ Î¶j , f âˆˆ Î“0 âŠ† Î“jâˆ’1, EPa(sÌ‚)[f ] = EPa(s)[f ] â‡â‡’ EPa(sÌ‚)[f ] = EPa(sâ€²)[f ]. This
completes the definition of consistency on Î·, and it becomes clear that s âˆ¼Î“nâˆ’1,Î¶n sâ€² â‡’ s âˆ¼Î“0,Î· sâ€²,
or âˆ†(âˆ¼Î“0,Î·) âŠ† span(âˆ†(âˆ¼Î“nâˆ’1,Î¶n)).
Finally, both Î“n and âˆ†(âˆ¼Î“nâˆ’1,Î·) are bases of the same size, and both refine Î“nâˆ’1. It must be that
span(Î“n) = span(âˆ†(âˆ¼Î“nâˆ’1,Î¶n)) âŠ‡ âˆ†(âˆ¼Î“0,Î·).
3.2 Examples of basis refinement for feature extraction
The concept of basis refinement is not only applicable to the feature extraction methods we will
present later, but to methods that have been studied in the past. In particular, methods based on
Bellman error basis functions, state aggregation strategies, and spectral analysis using bisimulation
metrics are all special cases of basis refinement. We briefly describe the refinement property for
the first two cases, and, in the next section, we elaborate on the connection between refinement and
bisimulation metrics to provide a new condition for convergence to self-refining bases.
Krylov bases: Consider the uncontrolled (policy evaluation) case, in which one would like to
find a set of features that is suited to evaluating a single policy of interest. A common approach to
automatic feature generation in this context computes Bellman error basis functions (BEBFs), which
have been shown to generate a sequence of representations known as Krylov bases. Given a policy
Ï€, a Krylov basis Î¦n of size n is built using the model (RÏ€, PÏ€) (defined in Section 2 as elements
of FS and [[FS â†’ FS ]], respectively): Î¦n = span{RÏ€, PÏ€RÏ€, (PÏ€)2RÏ€, ..., (PÏ€)nRÏ€}. It is not
hard to check that Î¦n n Î¦n+1, where n is the refinement relational property in Def 3.2. Since the
initial feature RÏ€ âˆˆ âˆ†(âˆ¼0), the result in Theorem 3.1 holds for the Krylov bases.
Under the assumption of a finite-state MDP (i.e. |S| < âˆ), Î“Ï‡ := {Ï‡({s}) | s âˆˆ S} is a basis for
FS , therefore this set of features is finite dimensional. It follows that one can find N â‰¤ |S| such
that one of the Krylov bases is a self-refinement, i.e. Î¦N n Î¦N . This would by no means be the
only self-refining basis. In fact this property holds for the basis of characteristic functions, Î“Ï‡nÎ“Ï‡.
The purpose our framework is to determine other self-refining bases which are suited for function
approximation methods in the context of controlled systems.
State aggregation: One popular strategy used for solving MDPs is that of computing state aggre-
gation maps. Instead of working with alternative subspaces, these methods first compute equiv-
alence relations on the state space. An aggregate/collapsed model is then derived, and the so-
lution to this model is translated to one for the original problem: the resulting policy provides
the same action choice for states that have originally been related. Given any equivalence rela-
tion âˆ¼ on S, a state aggregation map is a function from S to any set X , Ï : S â†’ X , such that
âˆ€s, sâ€², Ï(sâ€²) = Ï(s) â‡â‡’ s âˆ¼ sâ€². In order to obtain a significant computational gain, one would
like to work with aggregation maps Ï that reduce the size of the space for which one looks to provide
action choices, i.e. |X|  |S|. As discussed in Section 3.1, one could work with features that are
defined on an aggregate state space instead of the original state space. That is, instead of computing
a set of state features Î“ âŠ‚ FS , we could work instead with an aggregation map Ï : S â†’ X and a
set of features over X , Î“Ì‚ âŠ‚ FX . If âˆ¼ is the relation such that s âˆ¼ sâ€² â‡â‡’ Ï(s) = Ï(sâ€²), then
âˆ€Ï• âˆˆ Î“Ì‚, Ï• â—¦ Ï âˆˆ span(âˆ†(âˆ¼)).
4 Using bisimulation metrics for convergence of bases
In Section 3.2 we provide two examples of self-refining subspaces: the Krylov bases and the charac-
teristic functions on single states. The latter is the largest and sparsest basis; it spans the entire state
space and the features share no information. The former is potentially smaller and it spans the value
of the fixed policy for which it was designed. In this section we will present a third self-refining
construction, which is designed to capture bisimulation properties. Based on the results presented
in Section 3.1, it can be shown that given a bisimulation relation âˆ¼, the partition it generates is
self-refining, i.e. âˆ†(âˆ¼) n âˆ†(âˆ¼).
6
Desirable self-refining bases might be be computationally demanding and/or too complex to use or
represent. We propose iterative schemes which ultimately provide a self-refining result - albeit we
would have the flexibility of stopping the iterative process before reaching the final result. At the
same time, we need a criterion to describe convergence of sequences of bases. That is, we would
want to know how close an iterative process is to obtaining a self-refining basis. Inspired by the fixed
point theory used to study bisimulation metrics [Desharnais et al., 1999], instead of using a metric
over the set of all bases to characterize convergence of such sequences, we will use corresponding
metrics over the original state space. This choice is better suited for generalizing previously existing
methods that compare pairs of states for bisimilarity through their associated reward models and
expected realizations of features over the next state distribution model associated with these states.
We will study metric construction strategies based on a map D, defined below, which takes an
element of the powerset P(FS) of FS and returns an element of all pseudo-metrics M (S) over S.
D(Î“) : (s, sâ€²) 7â†’ maxa
[
(1âˆ’ Î³) |Ra(s)âˆ’Ra(sâ€²)|+ Î³ supÏ•âˆˆÎ“
âˆ£âˆ£EPa(s)[Ï•]âˆ’ EPa(sâ€²)[Ï•]âˆ£âˆ£] (6)
Î“ is a set of features whose expectation over next-state distributions should be matched. It is not hard
to see that bases Î“ for whichD(Î“) is a bisimulation metric are by definition self-refining. For exam-
ple, consider the largest bisimulation relation âˆ¼ on a given MDP. It is not hard to see that D(âˆ†(âˆ¼))
is a bisimulation. A more elaborate example involves the set â„¦(d) of Lipschitz-1 continuous func-
tions on [[(S, d) â†’ (R, L1)]] (recall definition and computation details from Section 2). Define dâˆ—
to be the fixed point of the operator T : d 7â†’ D(â„¦(d)), i.e. dâˆ— = supnâˆˆN Tn(0). dâˆ— has the same
property as the bisimulation metric defined in Equation 1. Moreover, given any bisimulation metric
d, D(â„¦(d)) is a bisimulation metric.
Definition 4.1. We say a sequence {Î“n}âˆn=1 is a a bisimulation sequence of bases if D(Î“n) con-
verges uniformly from below to a bisimulation metric. If one has the a sequence of refining bases
with Î“nnÎ“n+1,âˆ€n, then {D(Î“n)}âˆn=1 is an increasing sequence, but not necessarily a bisimulation
sequence.
A bisimulation sequence of bases provide an approximation scheme for bases that satisfy two im-
portant properties studied in the past: self-refinement and bisimilarity. One could show that the
approximation schemes presented in [Ferns et al., 2004], [Comanici and Precup, 2011], and [Ruan
et al., 2015] are all examples of bisimulation sequences. We will present in the next section a
framework that generalizes all these examples, but which can be easily extended to a broader set of
approximation schemes that incorporate both refining and bisimulation principles.
5 Prototype based refinements
In this section we propose a strategy that iteratively builds sequences of refineing sets of fea-
tures, based on the concepts described in the previous sections. This generates layered sets of
features, where the nth layer in the construction will be dependent only on the (n âˆ’ 1)th layer.
Additionally, each feature will be associated with a reward-transition prototype: elements of
Q := [[A â†’ (R Ã— P(S))]], associating to each action a reward and a next-state probability dis-
tribution. Prototypes can be viewed as â€œabstractâ€ or representative states, such as used in KBRL
methods [Ormoneit and Sen, 2002]. In the layered structure, the similarity between prototypes at
the nth layer is based on a measure of consistency with respect to features at the (nâˆ’ 1)th layer. The
same measure of similarity is used to determine whether the entire state space is â€œcoveredâ€ by the
set of prototypes/features chosen for the nth layer. We say that a space is covered if every state of
the space is close to at least one prototype generated by the construction, with respect to a prede-
fined measure of similarity. This measure is designed to make sure that consecutive layers represent
refining sets of features. Note that for any given MDP, the state space S is embedded into Q (i.e.
S âŠ‚ Q), as (Ra(s), P a(s)) âˆˆ Q for every state s âˆˆ S. Additionally, The metric generator D, as
defined in Equation 6, can be generalized to a map from P(FS) to M (Q).
The algorithmic strategy will look for a sequence {Jn, Î¹n}âˆn=1, where Jn âŠ‚ Q is a set of covering
prototypes, and Î¹n : Jn â†’ FS is a function that associates a feature to every prototype in Jn.
Starting with J0 = âˆ… and Î“0 = âˆ…, the strategy needs to find, at step n > 0, a cover JÌ‚n for S,
based on the distance metric D(Î“nâˆ’1). That is, it has to guarantee that âˆ€s âˆˆ S,âˆƒÎº âˆˆ JÌ‚n with
D(Î“nâˆ’1)(s, Îº) = 0. With Jn = JÌ‚n âˆª Jnâˆ’1 and using a strictly decreasing function Ï„ : Râ‰¥0 â†’ R
(e.g. the energy-based Gibbs measure Ï„(x) = exp(âˆ’Î²x) for some Î² > 0), the framework constructs
Î¹n : Jn â†’ FS , a map that associates prototypes to features as Î¹n(Îº)(s) = Ï„(D(Î“nâˆ’1)(Îº, s)).
7
Algorithm 1 Prototype refinement
1: J0 = âˆ… and Î“0 = âˆ…
2: for n = 1 toâˆ do
3: choose a representative subset Î¶n âŠ‚ S and a cover approximation error n â‰¥ 0
4: find an n-cover JÌ‚n for Î¶n
5: define Jn = JÌ‚n âˆª Jnâˆ’1
6: choose a strictly decreasing function Ï„ : Râ‰¥0 â†’ R
7: define Î¹n(Îº) =
{
s 7â†’ Ï„(D(Î“nâˆ’1)(Îº, s)) if âˆƒsÌ‚ âˆˆ Î¶n, such that D(Î“nâˆ’1)(Îº, sÌ‚) â‰¤ n
Î¹nâˆ’1(Îº) otherwise
8: define Î“n = {Î¹n(Îº) | Îº âˆˆ Jn} (note that Î“n is a local refinement, Î“nâˆ’1 nÎ¶n Î“n)
It is not hard to see that the refinement property holds at every step, i.e. Î“n n Î“n+1. First, every
equivalence class of âˆ¼Î“n is represented by some prototype in Jn. Second, Î¹n is purposely defined
to make sure that a distinction is made between each prototype in Jn+1. Moreover, {Î“n}âˆn=1 is a
bisimulation sequence of bases, as the metric generator D is the main tool used in â€œcoveringâ€ the
state space with the set of prototypes Jn. Two states will be represented by the same prototype (i.e.
they will be equivalent with respect to âˆ¼Î“n ) if and only if the distance between their corresponding
reward-transition models is 0.
Algorithm 1 provides pseudo-code for the framework described in this section. Note that it also con-
tains two additional modifications, used to illustrate the flexibility of this feature extraction process.
Through the first modification, one could use the intermediate results at time step n to determine
a subset Î¶n âŠ‚ S of states which are likely to have a model with significantly distinct dynamics
over Î“nâˆ’1. As such, the prototypes JÌ‚nâˆ’1 can be specialized to cover only the significant subset Î¶n.
Moreover Theorem 3.1 guarantees that if every state in S is picked in Î¶n infinitely often, as nâ†’âˆ,
then the approximation power of the final result is not be compromised. The second modification
is based on using the values in the metric D(Î“nâˆ’1) for more than just choosing feature activations:
one could set at every step constants n â‰¥ 0 and then find Jn such that Î¶n is covered using n-balls,
i.e. for every state in Î¶n, there exists a prototype Îº âˆˆ Jn with D(Î“nâˆ’1)(Îº, s) â‰¤ n. One can easily
show that the refinement property can be maintained using the modified defition of Î¹n described in
Algorithm 1.
6 Discussion
We proposed a general framework for basis refinement for linear function approximation. The the-
oretical results show that any algorithmic scheme of this type satisfies strong bounds on the quality
of the value function that can be obtained. In other words, this approach provides a â€œblueprintâ€ for
designing algorithms with good approximation guarantees. As discussed, some existing value func-
tion construction schemes fall into this category (such as state aggregation refinement, for example).
Other methods, like BEBFs, can be interpreted in this way in the case of policy evaluation; however,
the â€œtraditionalâ€ BEBF approach in the case of control does not exactly fit this framework. However,
we suspect that it could be adapted to exactly follow this blueprint (something we leave for future
work).
We provided ideas for a new algorithmic approach to this problem, which would provide strong
guarantees while being significantly cheaper than other existing methods with similar bounds (which
rely on bisimulation metrics). We plan to experiment with this approach in the future. The focus
of this paper was to establish the theoretical underpinnings of the algorithm. The algorithm struc-
ture we propose is close in spirit to [Barreto et al., 2011], which selects prototype states in order
to represent well the dynamics of the system by means of stochastic factorization. However, their
approach assumes a given metric which measures state similarity, and selects representative states
using k-means clustering based on this metric. Instead, we iterate between computing the metric
and choosing prototypes. We believe that the theory presented in this paper opens up the possibil-
ity of further development of algorithms for constructive function approximation that have quality
guarantees in the control case, and which can be effective also in practice.
8
References
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
Cs. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool, 2010.
D. P. Bertsekas and D. A. Castanon. Adaptive Aggregation Methods for Infinite Horizon Dynamic
Programming. IEEE Transactions on Automatic Control, 34, 1989.
R. Munos and A. Moore. Variable Resolution Discretization in Optimal Control. Machine Learning,
49(2-3):291â€“323, 2002.
S. Mahadevan. Proto-Value Functions: Developmental Reinforcement Learning. In ICML, pages
553â€“560, 2005.
P. W. Keller, S. Mannor, and D. Precup. Automatic Basis Function Construction for Approximate
Dynamic Programming and Reinforcement Learning. In ICML, pages 449â€“456, 2006.
R. Parr, C. Painter-Wakefiled, L. Li, and M. L. Littman. Analyzing Feature Generation for Value
Function Approximation. In ICML, pages 737â€“744, 2008a.
G. D. Konidaris, S. Osentoski, and P. S. Thomas. Value Function Approximation in Reinforcement
Learning using the Fourier Basis. In AAAI, pages 380â€“385, 2011.
A. Geramifard, F. Doshi, J. Redding, N. Roy, and J. How. Online Discovery of Feature Dependen-
cies. In ICML, pages 881â€“888, 2011.
B. Ravindran and A. G. Barto. Model Minimization in Hierarchical Reinforcement Learning. In
Symposium on Abstraction, Reformulation and Approximation (SARA), pages 196â€“211, 2002.
N. Ferns, P. Panangaden, and D. Precup. Metrics for finite Markov Decision Processes. In UAI,
pages 162â€“169, 2004.
S. Ruan, G. Comanici, P. Panangaden, and D. Precup. Representation Discovery for MDPs using
Bisimulation Metrics. In AAAI, pages 3578â€“3584, 2015.
R. Givan, T. Dean, and M. Greig. Equivalence Notions and Model Minimization in Markov Decision
Processes. Artificial Intelligence, 147(1-2):163â€“223, 2003.
D. Ormoneit and S. Sen. Kernel-Based Reinforcement Learning. Machine Learning, 49(2-3):161â€“
178, 2002.
N. Jong and P. Stone. Kernel-Based Models for Reinforcement Learning. In ICML Workshop on
Kernel Machines and Reinforcement Learning, 2006.
A. S. Barreto, D. Precup, and J. Pineau. Reinforcement Learning using Kernel-Based Stochastic
Factorization. In NIPS, pages 720â€“728, 2011.
R. S. Sutton. Learning to Predict by the Methods of Temporal Differences. Machine Learning, 3
(1):9â€“44, 1988.
S. J. Bradtke and A. G. Barto. Linear Least-Squares Algorithms for Temporal Difference Learning.
Machine Learning, 22(1-3):33â€“57, 1996.
H. Yu and D. Bertsekas. Convergence Results for Some Temporal Difference Methods Based on
Least Squares. Technical report, LIDS MIT, 2006.
R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. L. Littman. An Analysis of Linear Models,
Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning. In
ICML, pages 752â€“759, 2008b.
K. G. Larsen and A. Skou. Bisimulation through Probabilistic Testing. Information and Computa-
tion, 94:1â€“28, 1991.
J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. Metrics for Labeled Markov Systems.
In CONCUR, 1999.
J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. A metric for labelled Markov processes.
Theoretical Computer Science, 318(3):323â€“354, 2004.
C. Villani. Topics in optimal transportation. American Mathematical Society, 2003.
G. Comanici and D. Precup. Basis Function Discovery Using Spectral Clustering and Bisimulation
Metrics. In AAAI, 2011.
D. Blackwell. Discounted Dynamic Programming. Annals of Mathematical Statistics, 36:226â€“235,
1965.
9
