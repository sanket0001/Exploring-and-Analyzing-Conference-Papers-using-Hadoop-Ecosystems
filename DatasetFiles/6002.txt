


Paper ID = 6002
Title = Smooth Interactive Submodular Set Cover
Bryan He
Stanford University
bryanhe@stanford.edu
Yisong Yue
California Institute of Technology
yyue@caltech.edu
Abstract
Interactive submodular set cover is an interactive variant of submodular set cover
over a hypothesis class of submodular functions, where the goal is to satisfy
all sufficiently plausible submodular functions to a target threshold using as few
(cost-weighted) actions as possible. It models settings where there is uncertainty
regarding which submodular function to optimize. In this paper, we propose a new
extension, which we call smooth interactive submodular set cover, that allows the
target threshold to vary depending on the plausibility of each hypothesis. We
present the first algorithm for this more general setting with theoretical guarantees
on optimality. We further show how to extend our approach to deal with real-
valued functions, which yields new theoretical results for real-valued submodular
set cover for both the interactive and non-interactive settings.
1 Introduction
In interactive submodular set cover (ISSC) [10, 11, 9], the goal is to interactively satisfy all plausible
submodular functions in as few actions as possible. ISSC is a wide-encompassing framework that
generalizes both submodular set cover [24] by virtue of being interactive, as well as some instances
of active learning by virtue of many active learning criteria being submodular [12, 9].
A key characteristic of ISSC is the a priori uncertainty regarding the correct submodular function to
optimize. For example, in personalized recommender systems, the system does not know the user‚Äôs
preferences a priori, but can learn them interactively via user feedback. Thus, any algorithm must
choose actions in order to disambiguate between competing hypotheses as well as optimize for the
most plausible ones ‚Äì this issue is also known as the exploration-exploitation tradeoff.
In this paper, we propose the smooth interactive submodular set cover problem, which addresses
two important limitations of previous work. The first limitation is that conventional ISSC [10, 11, 9]
only allows for a single threshold to satisfy, and this ‚Äúall or nothing‚Äù nature can be inflexible for
settings where the covering goal should vary smoothly (e.g., based on plausibility). In smooth ISSC,
one can smoothly vary the target threshold of the candidate submodular functions according to their
plausibility. In other words, the less plausible a hypothesis is, the less we emphasize maximizing
its associated utility function. We present a simple greedy algorithm for smooth ISSC with prov-
able guarantees on optimality. We also show that our smooth ISSC framework and algorithm fully
generalize previous instances of and algorithms for ISSC by reducing back to just one threshold.
One consequence of smooth ISSC is the need to optimize for real-valued functions, which leads
to the second limitation of previous work. Many natural classes of submodular functions are real-
valued (cf. [25, 5, 17, 21]). However, submodular set cover (both interactive and non-interactive)
has only been rigorously studied for integral or rational functions with fixed denominator, which
highlights a significant gap between theory and practice. We propose a relaxed version of smooth
ISSC using an approximation tolerance , such that one needs only to satisfy the set cover criterion to
within . We extend our greedy algorithm to provably optimize for real-valued submodular functions
within this  tolerance. To the best of our knowledge, this yields the first theoretically rigorous
algorithm for real-valued submodular set cover (both interactive and non-interactive).
1
Problem 1 Smooth Interactive Submodular Set Cover
1: Given:
1. Hypothesis class H (does not necessarily contain h‚àó)
2. Query setQ and response setR with known q(h) ‚äÜ R for q ‚àà Q, h ‚àà H
3. Modular query cost function c defined overQ
4. Monotone submodular objective functions Fh : 2Q√óR ‚Üí R‚â•0 for h ‚àà H
5. Monotone submodular distance functionsGh : 2Q√óR ‚Üí R‚â•0 for h ‚àà H , withGh(S‚äï(q, r))‚àí
Gh(S) = 0 for any S if r ‚àà q(h)
6. Threshold function Œ± : R‚â•0 ‚Üí R‚â•0 mapping a distance to required objective function value
2: Protocol: For i = 1, . . . ,‚àû: ask a question qÃÇi ‚àà Q and receive a response rÃÇi ‚àà qÃÇi(h‚àó).
3: Goal: Using minimal cost
‚àë
i c(qÃÇi), terminate when Fh(SÃÇ) ‚â• Œ±(Gh(S
‚àó)) for all h ‚àà H , where SÃÇ =
{(qÃÇi, rÃÇi)}i and S‚àó
4
=
‚ãÉ
q‚ààQ,r‚ààq(h‚àó){(q, r)}.
2 Background
Submodular Set Cover. In the basic submodular set cover problem [24], we are given an action
set Q and a monotone submodular set function F : 2Q ‚Üí R‚â•0 that maps subsets A ‚äÜ Q to
non-negative scalar values. A set function F is monotone and submodular if and only if:
‚àÄA ‚äÜ B ‚äÜ Q, q ‚àà Q : F (A‚äï q) ‚â• F (A) and F (A‚äï q)‚àí F (A) ‚â• F (B ‚äï q)‚àí F (B),
respectively, where ‚äï denotes set addition (i.e., A ‚äï q ‚â° A ‚à™ {q}). In other words, monotonicity
implies that adding a set always yields non-negative gain, and submodularity implies that adding to
a smaller set A results in a larger gain than adding to a larger set B. We also assume that F (‚àÖ) = 0.
Each q ‚àà Q is associated with a modular or additive cost c(q). Given a target threshold Œ±, the goal is
to select a setA that satisfies F (A) ‚â• Œ± with minimal cost c(A) = ‚àëq‚ààA c(q). This problem is NP-
hard; but for integer-valued F , simple greedy forward selection can provably achieve near-optimal
cost of at most (1 + ln(maxa‚ààQ F ({a}))OPT [24], and is typically very effective in practice.
One motivating application is content recommendation [5, 4, 25, 11, 21], where Q are items to
recommend, F (A) captures the utility of A ‚äÜ Q, and Œ± is the satisfaction goal. Monotonicity
of F captures the property that total utility never decreases as one recommends more items, and
submodularity captures the the diminishing returns property when recommending redundant items.
Interactive Submodular Set Cover. In the basic interactive setting [10], the decision maker must
optimize over a hypothesis class H of submodular functions Fh. The setting is interactive, whereby
the decision maker chooses an action (or query) q ‚àà Q, and the environment provides a response r ‚àà
R. Each query q is now a function mapping hypotheses H to responses R (i.e., q(h) ‚àà R), and the
environment provides responses according to an unknown true hypothesis h‚àó ‚àà H (i.e., r ‚â° q(h‚àó)).
This process iterates until Fh‚àó(S) ‚â• Œ±, where S denotes the set of observed question/response pairs:
S = {(q, r)} ‚äÜ Q√óR. The goal is to satisfy Fh‚àó(S) ‚â• Œ±with minimal cost c(S) =
‚àë
(q,r)‚ààS c(q).
For example, when recommending movies to a new user with unknown interests (cf. [10, 11]), H
can be a set of user types or movie genres (e.g., H = {Action,Drama,Horror, . . .}). ThenQ would
contain individual movies that can be recommended, and R would be a ‚Äúyes‚Äù or ‚Äúno‚Äù response or
an integer rating representing how interested the user (modeled as h‚àó) is in a given movie.
The interactive setting is both a learning and covering problem, as opposed to just a covering prob-
lem. The decision maker must balance between disambiguating between hypotheses in H (i.e.,
identifying which is the true h‚àó) and satisfying the covering goal Fh‚àó(S) ‚â• Œ±; this issue is also
known as the exploration-exploitation tradeoff. Noisy ISSC [11] extends basic ISSC by no longer
assuming the true h‚àó is in H , and uses a distance function Gh and tolerance Œ∫ such that the goal is
to satisfy Fh(S) ‚â• Œ± for all sufficiently plausible h, where plausibility is defined as Gh(S) ‚â§ Œ∫.
3 Problem Statement
We now present the smooth interactive submodular set cover problem, which generalizes basic
and noisy ISSC [10, 11] (described in Section 2). Like basic ISSC, each hypothesis h ‚àà H is
associated with a utility function Fh : 2Q√óR ‚Üí R‚â•0 that maps sets of query/response pairs to
2
Œ±1
Œ±2
Œ±3
Œ∫1Œ∫2 Œ∫3
Fh
Gh
(a)
Œ±1
Œ±2
Œ±3
Œ∫1Œ∫2 Œ∫3
Fh
Gh
(b)
Fh
Gh
(c)

Fh
Gh
(d)
Figure 1: Examples of (a) multiple thresholds, (b) approximate multiple thresholds, (c) a continuous
convex threshold, and (d) an approximate continuous convex threshold. For the approximate setting,
we essentially allow for satisfying any threshold function that resides in the yellow region.
non-negative scalars. Like noisy ISSC, the hypothesis class H does not necessarily contain the true
h‚àó (i.e., the agnostic setting). Each h ‚àà H is associated with a distance or disagreement function
Gh : 2
Q√óR ‚Üí R‚â•0 which maps sets of question/response pairs to a disagreement score (i.e., the
larger Gh(S) is, the more h disagrees with S). We further require that Fh(‚àÖ) = 0 and Gh(‚àÖ) = 0.
Problem 1 describes the general problem setting. Let S‚àó
4
=
‚ãÉ
q‚ààQ,r‚ààq(h‚àó){(q, r)} denote the set of
all possible question/responses pairs given by h‚àó. The goal is to construct a question/response set
SÃÇ with minimal cost such that, for every h ‚àà H we have Fh(SÃÇ) ‚â• Œ±(Gh(S‚àó)), where Œ±(¬∑) maps
disagreement values to desired utilities. In general, Œ±(¬∑) is a non-increasing function, since the goal
is to optimize more the most plausible hypotheses in H . We describe two versions of Œ±(¬∑) below.
Version 1: Step Function (Multiple Thresholds). The first version uses a decreasing step function
(see Figure 1(a)). Given a pair of sequences Œ±1 > . . . > Œ±N > 0 and 0 < Œ∫1 < . . . < Œ∫N ,
the threshold function is Œ±(v) = Œ±nŒ∫(v) where nŒ∫(v) = min{n ‚àà {0, . . . , N + 1}|v < Œ∫n}, and
Œ±0
4
= ‚àû, Œ±N+1 4= 0, Œ∫0 4= 0, Œ∫N+1 4= ‚àû. The goal in Problem 1 is equivalently: ‚Äú ‚àÄh ‚àà H and
n = 1, . . . , N : satisfy Fh(SÃÇ) ‚â• Œ±n whenever Gh(S‚àó) < Œ∫n.‚Äù This version is a strict generalization
of noisy ISSC, which uses only a single Œ± and Œ∫.
Version 2: Convex Threshold Curve. The second version uses a convex Œ±(¬∑) that decreases con-
tinuously as Gh(S‚àó) increases (see Figure 1(c)), and is not a strict generalization of noisy ISSC.
Approximate Thresholds. Finally, we also consider a relaxed version of smooth ISSC, whereby
we only require that the objectives Fh be satisfied to within some tolerance  ‚â• 0. More formally,
we say that we approximately solve Problem 1 with tolerance  if its goal is redefined as: ‚Äúusing
minimal cost,
‚àë
i c(qÃÇi), guarantee Fh(SÃÇ) ‚â• Œ±(Gh(S‚àó))‚àí  for all h ‚àà H .‚Äù See Figure 1(b) & 1(d)
for the approximate versions of the multiple tresholds and convex versions, respectively.
ISSC has only been rigorously studied when the utility functions are Fh are rational-valued with
a fixed denominator. We show in Section 4.3 how to efficiently solve the approximate version of
smooth ISSC when Fh are real-valued, which also yields a new approach for approximately solving
the classical non-interactive submodular set cover problem with real-valued objective functions.
4 Algorithm & Main Results
A key question in the study of interactive optimization is how to balance the exploration-exploitation
tradeoff. On the one hand, one should exploit current knowledge to efficiently satisfy the plausible
submodular functions. However, hypotheses that seem plausible might actually not be due to imper-
fections in the algorithm‚Äôs knowledge. One should thus explore by playing actions that disambiguate
the plausibility of competing hypotheses. Our setting is further complicated due to also solving a
combinatorial optimization problem (submodular set cover), which is in general intractable.
4.1 Approach Outline
We present a general greedy algorithm, described in Algorithm 1 below, for solving smooth ISSC
with provably near-optimal cost. Algorithm 1 requires as input a submodular meta-objective FÃÑ
3
Algorithm 1 Worst Case Greedy Algorithm for Smooth Interactive Submodular Set Cover
1: input: FÃÑ // Submodular Meta-Objective
2: input: FÃÑmax // Termination Threshold for FÃÑ
3: input: Q // Query or Action Set
4: input: R // Response Set
5: S ‚Üê ‚àÖ
6: while FÃÑ (S) < FÃÑmax do
7: qÃÇ ‚Üê argmaxq‚ààQminr‚ààR
(
FÃÑ (S ‚äï (q, r))‚àí FÃÑ (S)
)
/c(q)
8: Play qÃÇ, observe rÃÇ
9: S ‚Üê S ‚äï (qÃÇ, rÃÇ)
10: end while
Variable Definition
H Set of hypotheses
Q Set of actions or queries
R Set of responses
Fh Monotone non-decreasing submodular utility function
Gh Monotone non-decreasing submodular distance function
FÃÑ Monotone non-decreasing submodular function unifying Fh, Gh and the thresholds
FÃÑmax Maximum value held by FÃÑ
DF Denominator for Fh (when rational)
DG Denominator for Gh (when rational)
Œ±(¬∑) Continuous convex threshold
Œ±i Thresholds for F (Œ±1 is largest)
Œ∫i Thresholds for G (Œ∫1 is smallest)
N Number of thresholds
 Approximation tolerance for the real-valued case
F ‚Ä≤h Surrogate utility function for the approximate version
Œ±‚Ä≤n Surrogate thresholds for the approximate version
Figure 2: Summary of notation used. The top portion is used in all settings. The middle portion is
used for the multiple thresholds setting. The bottom portion is used for real-valued functions.
that quantifies the exploration-exploitation trade-off, and the specific instantiation of FÃÑ depends on
which version of smooth ISSC is being solved. Algorithm 1 greedily optimizes for the worst case
outcome at each iteration (Line 7) until a termination condition FÃÑ ‚â• FÃÑmax has been met (Line 6).
The construction of FÃÑ is essentially a reduction of smooth ISSC to a simpler submodular set cover
problem, and generalizes the reduction approach in [11]. In particular, we first lift the analysis of
[11] to deal with multiple thresholds (Section 4.2). We then show how to deal with approximate
thresholds in the real-valued setting (Section 4.3), which finally allows us to address the continuous
threshold setting (Section 4.4). Our cost guarantees are stated relative to the general cover cost
(GCC), which lower bounds the optimal cost, as stated in Definition 4.1 and Lemma 4.2 below. Via
this reduction, we can show that our approach achieves cost bounded by (1 + ln FÃÑmax)GCC ‚â§
(1 + ln FÃÑmax)OPT . For clarity of exposition, all proofs are deferred to the supplementary material.
Definition 4.1 (General Cover Cost (GCC)). Define oracles T ‚àà RQ to be functions mapping
questions to responses and T (QÃÇ) ‚àÜ=
‚ãÉ
qÃÇi‚ààQÃÇ{(qÃÇi, T (qÃÇi))}. T (QÃÇ) is the set of question-response pairs
given by T for the set of questions QÃÇ. Define the General Cover Cost as:
GCC
‚àÜ
= max
T‚ààRQ
(
min
QÃÇ:FÃÑ (T (QÃÇ))‚â•FÃÑmax
c(QÃÇ)
)
.
Lemma 4.2 (Lemma 3 from [11]). If there is a question asking strategy for satisfying FÃÑ (SÃÇ) ‚â• FÃÑmax
with worst case cost C‚àó, then GCC ‚â§ C‚àó. Thus GCC ‚â§ OPT .
4.2 Multiple Thresholds Version
We begin with the multiple thresholds version. In this section, we assume that each Fh and Gh
are rational-valued with fixed denominators DF and DG, respectively.1 We first define a doubly
1When each Fh and/or Gh are integer-valued, then DF = 1 and/or DG = 1, respectively.
4
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
FÃÑ   FÃÑmax
C
^
FÃÑh1   FÃÑhmax
¬∑¬∑
¬∑
FÃÑhi   FÃÑhmax
¬∑¬∑
¬∑
FÃÑh|H|   FÃÑhmax
B
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
^
FÃÑhi,1   FÃÑh,1max
¬∑¬∑
¬∑
FÃÑhi,N   FÃÑh,Nmax
A
_
¬∑¬∑
¬∑
_
Fhi   ‚Üµ1
Ghi   Ô£ø1
¬∑¬∑
¬∑
Fhi   ‚ÜµN
Ghi   Ô£øN
Figure 2: This figure shows the relationship between the terms defined in Definition 4.3. (A) For
FÃÑhi,n   FÃÑhi,nmax = (‚Üµn   ‚Üµn+1)(Ô£øn   Ô£øn 1), either Fhi   ‚Üµn or Ghi   Ô£øn. This generates the
tradeoff between satisfying the either of the two thresholds. (B) For FÃÑhi   FÃÑhmax , FÃÑhi,n   FÃÑhi,nmax
for all i 2 {1, . . . , N}. This creates the requirement that all of the thresholds must be satisfied. (C)
For FÃÑ   FÃÑmax, FÃÑh   FÃÑhmax for all h 2 H . This creates the requirement that all of the hypotheses
must be satisfied.
Using (1) and (2), we define the general forms of FÃÑ and FÃÑmax used in Sections 4.2, 4.3, and 4.4.
Each of these sections will apply this definition to different choices of Fh, Gh, N , ‚Üµ1, . . . ,‚ÜµN , and
Ô£ø1, . . . ,Ô£øN to solve their variants of the problem. In this definition, X is a constant to make FÃÑh
to be integer-valued, Y is the contribution to the maximum value from Fh and ‚Üµn, and Z is the
contribution to the maximum value from Gh and Ô£øn.
Definition 4.3 (General FÃÑ and FÃÑmax).
FÃÑh,n(SÃÇ)
4
=
‚á£
(Ô£øn   Ô£øn 1) Gh,Ô£øn,Ô£øn 1(SÃÇ)
‚åò
Fh,‚Üµn,‚Üµn+1(SÃÇ) + Gh,Ô£øn,Ô£øn 1(SÃÇ)(‚Üµn   ‚Üµn+1),
FÃÑh(SÃÇ)
4
= X
NX
n=1
2
4
0
@Y
j 6=n
(Ô£øj   Ô£øj 1)
1
A FÃÑh,n(SÃÇ)
3
5 ,
FÃÑ (SÃÇ)
4
=
X
h2H
FÃÑh(SÃÇ), FÃÑmax
4
= |H|Y Z
Definition 4.4 (Multiple Thresholds). To solve the multiple thresholds version of the problem, Fh,
Gh, N , ‚Üµ1, . . . ,‚ÜµN , and Ô£ø1, . . . ,Ô£øN are used without modification. The constants are set as the
following:
X = DF D
N
G , Y = DF‚Üµ1, Z = D
N
G
NY
n=1
(Ô£øn   Ô£øn 1)
This definition of FÃÑ trades off between exploitation (maximizing the most plausible Fh) and ex-
ploration (distinguishing between more and less plausible Fh) by allowing each FÃÑi to reach its
maximum value either by having Fh reach ‚Üµi or having Gh reach Ô£øi. In other words, each of the
thresholds can be satisfied with either a sufficiently large utility Fh or a sufficiently large distance
Gh. Figure 2 shows the logical relationships between these components.
We prove in Appendix A that FÃÑ is monotone submodular, and that finding a S such that FÃÑ (S)  
FÃÑmax is equivalent to solving Problem 1. For Definition 4.4, we also require that ‚Üµn and Ô£øn thresh-
olds satisfy Condition 4.5 for FÃÑ to be submodular.
Condition 4.5. The sequence h ‚Üµi ‚Üµn+1Ô£øn Ô£øn 1 i
N
i=1 is non-increasing.
Theorem 4.6. Let Fh and Gh be monotone submodular and rational-valued with fixed denominator
DF and DG, respectively. Then, if Condition 4.5 holds, then applying Algorithm 1 using FÃÑ and
FÃÑmax from Definition 4.4 solves the multiple thresholds version of Problem 1 with cost at most‚á£
1 + ln
‚á£
|H|DF DNG‚Üµ1
QN
n=1(Ô£øn   Ô£øn 1)
‚åò‚åò
GCC.
5
Figure 3: Depicting the relationship between the terms defined in Definition 4.3. (A) If FÃÑhi,n ‚â•
FÃÑhi,nmax = (Œ±n‚àíŒ±n+1)(Œ∫n‚àíŒ∫n‚àí1), then either Fhi ‚â• Œ±n orG ‚â• Œ∫n; this generates the tradeoff
between satisfying the either of the two thresholds. (B) If FÃÑhi ‚â• FÃÑhmax , then FÃÑhi,n ‚â• FÃÑhi,nmax‚àÄi ‚àà {1, . . . , N}; this enforces that all i, at least one of the thresholds Œ±i or Œ∫i must be satisfied. (C)
If FÃÑ ‚â• FÃÑmax, then FÃÑh ‚â• FÃÑhmax ‚àÄh ‚àà H; this enforces that all hypotheses must be satisfied.
truncated version of each hypothesis submodular utility and distance function:
Fh,Œ±n,Œ±j (SÃÇ)
4
= max(min(Fh(SÃÇ), Œ±n), Œ±j)‚àí Œ±j , (1)
Gh,Œ∫n,Œ∫j (SÃÇ)
4
= max(min(Gh(SÃÇ), Œ∫n), Œ∫j)‚àí Œ∫j . (2)
In other words, Fh,Œ±n,Œ±j is truncated from below at Œ±j and from above at Œ±n (it is assumed that
Œ±n > Œ±j), and is offset by ‚àíŒ±j so that Fh,Œ±n,Œ±j (‚àÖ) = 0. Gh,Œ∫n,Œ∫j is constructed analogously.
Using (1) and (2), we can define the general forms of FÃÑ and FÃÑmax, which can be instantiated to
address different versions of smooth ISSC.
Definition 4.3 (General form of FÃÑ and FÃÑmax).
FÃÑh,n(SÃÇ)
4
=
(
(Œ∫n ‚àí Œ∫n‚àí1 ‚àíGh,Œ∫n,Œ∫n‚àí1(SÃÇ)
)
Fh,Œ±n,Œ±n+1(SÃÇ) +Gh,Œ∫n,Œ∫n‚àí1(SÃÇ)(Œ±n ‚àí Œ±n+1),
FÃÑh(SÃÇ)
4
= CFÃÑ
N‚àë
n=1
Ô£ÆÔ£∞Ô£´Ô£≠‚àè
j 6=n
(Œ∫j ‚àí Œ∫j‚àí1)
Ô£∂Ô£∏ FÃÑh,n(SÃÇ)
Ô£πÔ£ª ,
FÃÑ (SÃÇ)
4
=
‚àë
h‚ààH
FÃÑh(SÃÇ), FÃÑmax
4
= |H|CFCG.
The coefficient CFÃÑ converts each FÃÑh to be integer-valued, CF is the contribution to FÃÑmax from Fh
and Œ±n, and CG is the con ibution to FÃÑmax fr m Gh and Œ∫n.
Definition 4.4 (Multiple Thresholds Version of ISSC). Given Œ±1, . . . , Œ±N and Œ∫1, . . . , Œ∫N , we in-
stantiate FÃÑ and FÃÑmax in Definition 4.3 via:
CFÃÑ = DFD
N
G , CF = DFŒ±1, CG = D
N
G
N‚àè
n=1
(Œ∫n ‚àí Œ∫n‚àí1).
FÃÑ in Definition 4.4 tra es off between xploitation (maximizing the plausible Fh‚Äôs) and exploration
(disambiguating plausibility in Fh‚Äôs) by allowing each FÃÑh to reach its maximum by either Fh reach-
ing Œ±i or Gh reaching Œ∫i. In other words, each FÃÑh can be satisfied with either a sufficiently large
utility Fh or large distance Gh. Figure 3 shows the logical relationships between these components.
We prove in Appendix A that FÃÑ is monotone submodular, and that finding an S such that FÃÑ (S) ‚â•
FÃÑmax is equivalent to solving Problem 1. For FÃÑ to be submodular, we also require Condition 4.5,
which is essentially a discrete analogue to the condition that a continuous Œ±(¬∑) should be convex.
Condition 4.5. The sequence „ÄàŒ±n‚àíŒ±n+1Œ∫n‚àíŒ∫n‚àí1 „Äâ
N
n=1 is non-increasing.
Theorem 4.6. Given Condition 4.5, Algorithm 1 using Definition 4.4 solves the multiple thresholds
version of Problem 1 using cost at most
(
1 + ln
(
|H|DFDNGŒ±1
‚àèN
n=1(Œ∫n ‚àí Œ∫n‚àí1)
))
GCC.
If each Gh is integral and Œ∫n = Œ∫n‚àí1 + 1, then the bound simplifies to (1 + ln (|H|DFŒ±1))GCC.
We present an alternative formulation in Appendix D.2 that has better bounds when DG is large, but
is less flexible and cannot be easily extended to the real-valued and convex threshold curve settings.
5
4.3 Approximate Thresholds for Real-Valued Functions
Solving even non-interactive submodular set cover is extremely challenging when the utility func-
tions Fh are real-valued. For example, Appendix B.1 describes a setting where the greedy algorithm
performs arbitrarily poorly. We now extend the results from Section 4.2 to real-valued Fh and
Œ±1, . . . , Œ±N .
Rather than trying to solve the problem exactly, we instead solve a relaxed or approximate version,
which will be useful for the convex threshold curve setting. Let  > 0 denote a pre-specified
approximation tolerance for Fh, d¬∑eŒ≥ denote rounding up to the nearest multiple of Œ≥, and b¬∑cŒ≥
denote rounding down to the nearest multiple of Œ≥. We define a surrogate problem:
Definition 4.7 (Approximate Thresholds for Real-Valued Functions). Define the following approx-
imations to Fh and Œ±n:
F ‚Ä≤h(SÃÇ)
4
=
D

Ô£ÆÔ£ØÔ£ØÔ£ØFh(SÃÇ) + D
|SÃÇ|‚àë
i=1
(|Q|+ 1‚àí i)
Ô£πÔ£∫Ô£∫Ô£∫

D
,
Œ±‚Ä≤n
4
=
D

‚åä
Œ±n ‚àí

D
n‚àë
i=1
[
(2N ‚àí 2i)DN‚àíi+1G
N‚àè
j=i
(Œ∫j ‚àí Œ∫j‚àí1)
]‚åã

D
D
4
=
Ô£ÆÔ£∞ |Q|‚àë
i=1
(|Q|+ 1‚àí i) +
N‚àë
i=1
[
(2N ‚àí 2i)DN‚àíi+1G
N‚àè
j=i
(Œ∫j ‚àí Œ∫j‚àí1)
]
+ 2
Ô£πÔ£ª
Instantiate FÃÑ and FÃÑmax in Definition 4.3 using F ‚Ä≤h, Œ±
‚Ä≤
n above, Gh, Œ∫n and:
CFÃÑ = D
N
G , CF = Œ±
‚Ä≤
1, CG = D
N
G
N‚àè
n=1
(Œ∫n ‚àí Œ∫n‚àí1).
We prove in Appendix B that Definition 4.7 is an instance of a smooth ISSC problem, and that
solving Definition 4.7 will approximately solve the original real-valued smooth ISSC problem.
Theorem 4.8. Given Condition 4.5, Algorithm 1 using Definition 4.7 will approximately solve
the real-valued multiple thresholds version of Problem 1 with tolerance  using cost at most(
1 + ln
(
|H|Œ±‚Ä≤1DNG
‚àèN
n=1(Œ∫n ‚àí Œ∫n‚àí1)
))
GCC.
We show in Appendix B.2 how to apply this result to approximately solve the basic submodular set
cover problem with real-valued objectives. Note that if  is selected as the smallest distinct difference
between values in Fh, then the approximation will be exact.
4.4 Convex Threshold Curve Version
We now address the setting where the threshold curve Œ±(¬∑) is continuous and convex. We again
solve the approximate version, since the threshold curve Œ±(¬∑) is necessarily real-valued. Let  > 0
be the pre-specified tolerance for F ‚Ä≤h. Let N be defined so that NDG is the maximal value of Gh.
We convert the continuous version Œ±(¬∑) to a multiple threshold version (with N thresholds) that is
within an -approximation of the former, as shown below.
Definition 4.9 (Equivalent Multiple Thresholds for Continuous Convex Curve). Instantiate FÃÑ and
FÃÑmax in Definition 4.3 using Gh without modification, and a sequence of thresholds:
F ‚Ä≤h(SÃÇ)
4
=
D

Ô£ÆÔ£ØÔ£ØÔ£ØFh(SÃÇ) + D
|SÃÇ|‚àë
i=1
(|Q|+ 1‚àí i)
Ô£πÔ£∫Ô£∫Ô£∫

D
,
Œ±‚Ä≤n
4
=
D

‚åä
Œ±(n)‚àí 
D
n‚àë
i=1
[
(2N ‚àí 2i)DN‚àíi+1G
N‚àè
j=i
(Œ∫j ‚àí Œ∫j‚àí1)
]‚åã

D
Œ∫n
4
= DGn
6
with constants set as:
CFÃÑ = 1, CF = Œ±
‚Ä≤
1, CG = D
N
G
N‚àè
n=1
(Œ∫n ‚àí Œ∫n‚àí1) = DNG .
Note that the F ‚Ä≤h are not too expensive to compute. We prove in Appendix C that satisfying this set of
thresholds is equivalent to satisfying the original curve Œ±(¬∑) within -error. Note also that Definition
4.9 uses the same form as Definition 4.7 to handle the approximation of real-valued functions.
Theorem 4.10. Applying Algorithm 1 using Definition 4.9 approximately solves the convex thresh-
old version of Problem 1 with tolerance  using cost at most:
(
1 + ln
(
|H|Œ±‚Ä≤1DNG
))
GCC.
Note that if  is sufficiently large, then N could in principle be smaller, which can lead to less
conservative approximations. There may also be more precise approximations by reducing to other
formulations for the multi-threshold setting (e.g., Appendix D.2).
5 Simulation Experiments
Comparison of Methods to Solve Multiple Thresholds. We compared our multiple threshold
method against multiple baselines (see Appendix D for more details) in a range of simulation settings
(see Appendix E.1). Figure 4 shows the results. We see that our approach is consistently amongst the
best performing methods. The primary competitor is the circuit of constraints approach from [11]
(see Appendix D.3 for a comparison of the theoretical guarantees). We also note that all approaches
dramatically outperform their worst-case guarantees.
Percentile
0 50 100
C
os
t
25
30
35
40
45
50
Cost for Setting A
Percentile
0 50 100
C
os
t
20
25
30
35
Cost for Setting B
Percentile
0 50 100
C
os
t
20
25
30
35
Cost for Setting C
Multiple Threshold (Def 4.4)
Alternative (Def D.1)
Circuit (Def D.6)
Forward (Sec D.1)
Backward (Sec D.1)
Figure 4: Comparison against baselines in three simulation settings.
Validating Approximation Tolerances. We also validated the efficacy of our approximate thresh-
olds relaxation (see Appendix E.2 for more details of the setup). Figure 5 shows the results. We see
that the actual deviation from the original smooth ISSC problem is much smaller than the specified
, which suggests that our guarantees are rather conservative. For instance, at  = 15, the algorithm
is allowed to terminate immediately. We also see that the cost to completion steadily decreases as 
increases, which agrees with our theoretical results.
0
5 10 15 20 25
C
os
t
26
28
30
32
34
Cost vs 0
0
5 10 15 20 25
D
ev
ia
tio
n
0
0.5
1
1.5
2
Deviation vs 0
Figure 5: Comparing cost and deviation from the exact function for varying .
6 Summary of Results & Discussion
Figure 6 summarizes the size of FÃÑmax (or FÃÑ ‚Ä≤max for real-valued functions) for the various settings.
Recall that our cost guarantees take the form (1 + ln FÃÑmax)OPT . When Fh are real-valued, then
we instead solve the smooth ISSC problem approximately with cost guarantee (1 + ln FÃÑ ‚Ä≤max)OPT .
Our results are well developed for many different versions of the utility functions Fh, but are less
flexible for the distance functions Gh. For example, even for rational-valued Gh, FÃÑmax scales as
DNG , which is not desirable. The restriction of Gh to be rational (or integral) leads to a relatively
straightforward reduction of the continuous convex version of Œ±(¬∑) to a multiple thresholds version.
7
In fact, our formulation can be extended to deal with real-valued Gh and Œ∫n in the multiple thresh-
olds version; however the resulting FÃÑ is no longer guaranteed to be submodular. It is possible that a
different assumption than the one imposed in Condition 4.5 is required to prove more general results.
F G Multiple Thresholds Convex Threshold Curve
Rational Rational |H|Œ±1DFDNG
‚àèN
i=1(Œ∫i ‚àí Œ∫i‚àí1) |H|Œ±1DFD
N
G
Real Rational |H|Œ±‚Ä≤1DNG
‚àèN
i=1 (Œ∫i ‚àí Œ∫i‚àí1) |H|Œ±
‚Ä≤
1D
N
G
Figure 6: Summarizing FÃÑmax. When Fh are real-valued, we show FÃÑ ‚Ä≤max instead.
Our analysis appears to be overly conservative for many settings. For instance, all the approaches we
evaluated empirically achieved much better performance than their worst-case guarantees. It would
be interesting to identify ways to constrain the problem and develop tighter theoretical guarantees.
7 Other Related Work
Submodular optimization is an important problem that arises across many settings, including sensor
placements [16, 15], summarization [26, 17, 23], inferring latent influence networks [8], diversified
recommender systems [5, 4, 25, 21], and multiple solution prediction [1, 3, 22, 19]. However, the
majority of previous work has focused on offline submodular optimization whereby the submodular
function to be optimized is fixed a priori (i.e., does not vary depending on feedback).
There are two typical ways that a submodular optimization problem can be made interactive. The
first is in online submodular optimization, where an unknown submodular function must be re-
optimized repeatedly over many sessions in an online or repeated-games fashion [20, 25, 21]. In
this setting, feedback is typically provided only at the conclusion of a session, and so adapting from
feedback is performed between sessions. In other words, each session consists of a non-interactive
submodular optimization problem, and the technical challenge stems from the fact that the submod-
ular function is unknown a priori and must be learned from feedback provided post optimization in
each session ‚Äì this setting is often referred to as inter-session interactive optimization.
The other way to make submodular optimization interactive, which we consider in this paper, is to
make feedback available immediately after each action taken. In this way, one can simultaneously
learn about and optimize for the unknown submodular function within a single optimization session
‚Äì this setting is often referred to as intra-session interactive optimization. One can also consider
settings that allow for both intra-session and inter-session interactive optimization.
Perhaps the most well-studied application of intra-session interactive submodular optimization is
active learning [10, 7, 11, 9, 2, 14, 13], where the goal is to quickly reduce the hypothesis class
to some target residual uncertainty for planning or decision making. Many instances of noisy and
approximate active learning can be formulated as an interactive submodular set cover problem [9].
A related setting is adaptive submodularity [7, 2, 6, 13], which is a probabilistic setting that essen-
tially requires that the conditional expectation over the hypothesis set of submodular functions is
itself a submodular function. In contrast, we require that the hypothesis class be pointwise submod-
ular (i.e., each hypothesis corresponds to a different submodular utility function). Although neither
adaptive submodularity nor pointwise submodularity is a strict generalization of the other (cf. [7, 9]),
in practice it can often be easier to model application settings using pointwise submodularity.
The ‚Äúflipped‚Äù problem is to maximize utility with a bounded budget, which is commonly known as
the budgeted submodular maximization problem [18]. Interactive budgeted maximization has been
analyzed rigorously for adaptive submodular problems [7], but it remains a challenge to develop
provably near-optimal interactive algorithms for pointwise submodular utility functions.
8 Conclusions
We introduced smooth interactive submodular set cover, a smoothed generalization of previous ISSC
frameworks. Smooth ISSC allows for the target threshold to vary based on the plausibility of the
hypothesis. Smooth ISSC also introduces an approximate threshold solution concept that can be
applied to real-valued functions, which also applies to basic submodular set cover with real-valued
objectives. We developed the first provably near-optimal algorithm for this setting.
8
References
[1] Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse m-best
solutions in markov random fields. In European Conference on Computer Vision (ECCV), 2012.
[2] Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular
optimization. In International Conference on Machine Learning (ICML), 2013.
[3] Debadeepta Dey, Tommy Liu, Martial Hebert, and J. Andrew Bagnell. Contextual sequence prediction
via submodular function optimization. In Robotics: Science and Systems Conference (RSS), 2012.
[4] Khalid El-Arini and Carlos Guestrin. Beyond keyword search: discovering relevant scientific literature.
In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2011.
[5] Khalid El-Arini, Gaurav Veda, Dafna Shahaf, and Carlos Guestrin. Turning down the noise in the blogo-
sphere. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2009.
[6] Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, and S. Muthukrishnan. Adaptive sub-
modular maximization in bandit setting. In Neural Information Processing Systems (NIPS), 2013.
[7] Daniel Golovin and Andreas Krause. Adaptive submodularity: A new approach to active learning and
stochastic optimization. In Conference on Learning Theory (COLT), 2010.
[8] Manuel Gomez Rodriguez, Jure Leskovec, and Andreas Krause. Inferring networks of diffusion and
influence. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2010.
[9] Andrew Guillory. Active Learning and Submodular Functions. PhD thesis, University of Washington,
2012.
[10] Andrew Guillory and Jeff Bilmes. Interactive submodular set cover. In International Conference on
Machine Learning (ICML), 2010.
[11] Andrew Guillory and Jeff Bilmes. Simultaneous learning and covering with adversarial noise. In Inter-
national Conference on Machine Learning (ICML), 2011.
[12] Steve Hanneke. The complexity of interactive machine learning. Master‚Äôs thesis, Carnegie Mellon Uni-
versity, 2007.
[13] Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew Bagnell, and Siddhartha Srini-
vasa. Near optimal bayesian active learning for decision making. In Conference on Artificial Intelligence
and Statistics (AISTATS), 2014.
[14] Shervin Javdani, Matthew Klingensmith, J. Andrew Bagnell, Nancy Pollard, and Siddhartha Srinivasa.
Efficient touch based localization through submodularity. In IEEE International Conference on Robotics
and Automation (ICRA), 2013.
[15] Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian processes.
In International Conference on Machine Learning (ICML), 2005.
[16] Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen, and Natalie
Glance. Cost-effective outbreak detection in networks. In ACM Conference on Knowledge Discovery and
Data Mining (KDD), 2007.
[17] Hui Lin and Jeff Bilmes. Learning mixtures of submodular shells with application to document summa-
rization. In Conference on Uncertainty in Artificial Intelligence (UAI), 2012.
[18] George Nemhauser, Laurence Wolsey, and Marshall Fisher. An analysis of approximations for maximiz-
ing submodular set functions. Mathematical Programming, 14(1):265‚Äì294, 1978.
[19] Adarsh Prasad, Stefanie Jegelka, and Dhruv Batra. Submodular meets structured: Finding diverse subsets
in exponentially-large structured item sets. In Neural Information Processing Systems (NIPS), 2014.
[20] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multi-armed
bandits. In International Conference on Machine Learning (ICML), 2008.
[21] Karthik Raman, Pannaga Shivaswamy, and Thorsten Joachims. Online learning to diversify from implicit
feedback. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2012.
[22] Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, and J. Andrew Bagnell. Learning policies for
contextual submodular prediction. In International Conference on Machine Learning (ICML), 2013.
[23] Sebastian Tschiatschek, Rishabh Iyer, Haochen Wei, and Jeff Bilmes. Learning mixtures of submodular
functions for image collection summarization. In Neural Information Processing Systems (NIPS), 2014.
[24] Laurence A Wolsey. An analysis of the greedy algorithm for the submodular set covering problem.
Combinatorica, 2(4):385‚Äì393, 1982.
[25] Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversified retrieval.
In Neural Information Processing Systems (NIPS), 2011.
[26] Yisong Yue and Thorsten Joachims. Predicting diverse subsets using structural svms. In International
Conference on Machine Learning (ICML), 2008.
9
