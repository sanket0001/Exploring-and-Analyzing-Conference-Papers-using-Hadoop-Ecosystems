


Paper ID = 5842
Title = Bandit Smooth Convex Optimization:
Improving the Bias-Variance Tradeoff
Ofer Dekel
Microsoft Research
Redmond, WA
oferd@microsoft.com
Ronen Eldan
Weizmann Institute
Rehovot, Israel
roneneldan@gmail.com
Tomer Koren
Technion
Haifa, Israel
tomerk@technion.ac.il
Abstract
Bandit convex optimization is one of the fundamental problems in the field of
online learning. The best algorithm for the general bandit convex optimiza-
tion problem guarantees a regret of eO(T 5/6), while the best known lower bound
is ⌦(T 1/2). Many attempts have been made to bridge the huge gap between these
bounds. A particularly interesting special case of this problem assumes that the
loss functions are smooth. In this case, the best known algorithm guarantees a re-
gret of eO(T 2/3). We present an efficient algorithm for the bandit smooth convex
optimization problem that guarantees a regret of eO(T 5/8). Our result rules out
an ⌦(T 2/3) lower bound and takes a significant step towards the resolution of this
open problem.
1 Introduction
Bandit convex optimization [11, 5] is the following online learning problem. First, an adversary
privately chooses a sequence of bounded and convex loss functions f1, . . . , fT defined over a con-
vex domain K in d-dimensional Euclidean space. Then, a randomized decision maker iteratively
chooses a sequence of points x1, . . . , xT , where each xt 2 K. On iteration t, after choosing the
point x
t
, the decision maker incurs a loss of f
t
(x
t
) and receives bandit feedback: he observes the
value of his loss but he does not receive any other information about the function f
t
. The decision
maker uses the feedback to make better choices on subsequent rounds. His goal is to minimize re-
gret, which is the difference between his loss and the loss incurred by the best fixed point in K. If
the regret grows sublinearly with T , it indicates that the decision maker’s performance improves as
the length of the sequence increases, and therefore we say that he is learning.
Finding an optimal algorithm for bandit convex optimization is an elusive open problem. The first
algorithm for this problem was presented in Flaxman et al. [11] and guarantees a regret of R(T ) =
eO(T 5/6) for any sequence of loss functions (here and throughout, the asymptotic eO notation hides
a polynomial dependence on the dimension d as well as logarithmic factors). Despite the ongoing
effort to improve on this rate, it remains the state of the art. On the other hand, Dani et al. [9]
proves that for any algorithm there exists a worst-case sequence of loss functions for which R(T ) =
⌦(T 1/2), and the gap between the upper and lower bounds is huge.
While no progress has been made on the general form of the problem, some progress has been made
in interesting special cases. Specifically, if the bounded convex loss functions are also assumed to be
Lipschitz, Flaxman et al. [11] improves their regret guarantee to R(T ) = eO(T 3/4). If the loss func-
tions are smooth (namely, their gradients are Lipschitz), Saha and Tewari [15] present an algorithm
with a guaranteed regret of eO(T 2/3). Similarly, if the loss functions are bounded, Lipschitz, and
strongly convex, the guaranteed regret is eO(T 2/3) [3]. If even stronger assumptions are made, an
optimal regret rate of e⇥(T 1/2) can be guaranteed; namely, when the loss functions are both smooth
1
and strongly-convex [12], when they are Lipschitz and linear [2], and when Lipschitz loss functions
are not generated adversarially but drawn i.i.d. from a fixed and unknown distribution [4].
Recently, Bubeck et al. [8] made progress that did not rely on additional assumptions, such as
Lipschitz, smoothness, or strong convexity, but instead considered the general problem in the one-
dimensional case. That result proves that there exists an algorithm with optimal e⇥(T 1/2) regret
for arbitrary univariate convex functions f
t
: [0, 1] 7! [0, 1]. Subsequently, and after the current
paper was written, Bubeck and Eldan [7] generalized this result to bandit convex optimization in
general Euclidean spaces (albeit requiring a Lipschitz assumption). However, the proofs in both
papers are non-constructive and do not give any hint on how to construct a concrete algorithm, nor
any indication that an efficient algorithm exists.
The current state of the bandit convex optimization problem has given rise to two competing con-
jectures. Some believe that there exists an efficient algorithm that matches the current lower bound.
Meanwhile, others are trying to prove larger lower bounds, in the spirit of [10], even under the as-
sumption that the loss functions are smooth; if the ⌦(T 1/2) lower bound is loose, a natural guess
of the true regret rate would be e⇥(T 2/3).1 In this paper, we take an important step towards the
resolution of this problem by presenting an algorithm that guarantees a regret of e⇥(T 5/8) against
any sequence of bounded, convex, smooth loss functions. Compare this result to the previous state-
of-the-art result of e⇥(T 2/3) (noting that 2/3 = 0.666... and 5/8 = 0.625). This result rules out
the possibility of proving a lower bound of ⌦(T 2/3) with smooth functions. While there remains
a sizable gap with the T 1/2 lower bound, our result brings us closer to finding the elusive optimal
algorithm for bandit convex optimization, at least in the case of smooth functions.
Our algorithm is a variation on the algorithms presented in [11, 1, 15], with one new idea. These
algorithms all follow the same template: on each round, the algorithm computes an estimate of
rf
t
(x
t
), the gradient of the current loss function at the current point, by applying a random pertur-
bation to x
t
. The sequence of gradient estimates is then plugged into a first-order online optimization
technique. The technical challenge in the analysis of these algorithms is to bound the bias and the
variance of these gradient estimates. Our idea is take a window of consecutive gradient estimates
and average them, producing a new gradient estimate with lower variance and higher bias. Overall,
the new bias-variance tradeoff works in our favor and allows us to improve the regret upper-bound.
Averaging uncorrelated random vectors to reduce variance is a well-known technique, but applying
it in the context of bandit convex optimization algorithm is easier said than done and requires us to
overcome a number of technical difficulties. For example, the gradient estimates in our window are
taken at different points, which introduces a new type of bias. Another example is the difficulty that
arrises when the sequence x
s
, . . . , x
t
travels adjacent to the boundary of the convex set K (imagine
transitioning from one face of a hypercube to another); the random perturbation applied to x
s
and
x
t
could be supported on orthogonal directions, yet we average the resulting gradient estimates
and expect to get a meaningful low-variance gradient estimate. While the basic idea is simple, our
non-trivial technical analysis is not, and may be of independent interest.
2 Preliminaries
We begin by defining smooth bandit convex optimization more formally, and recalling several basic
results from previous work on the problem (Flaxman et al. [11], Abernethy et al. [2], Saha and Tewari
[15]) that we use in our analysis. We also review the necessary background on self-concordant
barrier functions.
2.1 Smooth Bandit Convex Optimization
In the bandit convex optimization problem, an adversary first chooses a sequence of convex functions
f1, . . . , fT : K 7! [0, 1], where K is a closed and convex domain in Rd. Then, on each round
t = 1, . . . , T , a randomized decision maker has to choose a point x
t
2 K, and after committing
to his decision he incurs a loss of f
t
(x
t
), and observes this loss as feedback. The decision maker’s
expected loss (where expectation is taken with respect to his random choices) is E[
P
T
t=1ft(xt)] and
1In fact, we are aware of at least two separate research groups that invested time trying to prove such
an ⌦(T 2/3) lower bound.
2
his regret is
R(T ) = E
"
TX
t=1
f
t
(x
t
)
#
  min
x2K
TX
t=1
f
t
(x) .
Throughout, we use the notation E
t
[·] to indicate expectations conditioned on all randomness up to
and including round t  1.
We make the following assumptions. First, we assume that each of the functions f1, . . . , fT is L-
Lipschitz with respect to the Euclidean norm k·k2, namely that |ft(x)  ft(y)|  Lkx  yk2 for all
x, y 2 K. We further assume that f
t
is H-smooth with respect to k·k2, which is to say that
8 x, y 2 K , krf
t
(x) rf
t
(y)k2  H kx  yk2 .
In particular, this implies that f
t
is continuously differentiable over K. Finally, we assume that the
Euclidean diameter of the decision domain K is bounded by D > 0.
2.2 First Order Algorithms with Estimated Gradients
The online convex optimization problem becomes much easier in the full information setting, where
the decision maker’s feedback includes the vector g
t
= rf
t
(x
t
), the gradient (or subgradient) of
f
t
at the point x
t
. In this setting, the decision maker can use a first-order online algorithm, such as
the projected online gradient descent algorithm [17] or dual averaging [13] (sometimes known as
follow the regularized leader [16]), and guarantee a regret of O(T 1/2). The dual averaging approach
sets x
t
to be the solution to the following optimization problem,
x
t
= argmin
x2K
(
x ·
t 1X
s=1
↵
s,t
g
s
+R(x)
)
, (1)
where R is a suitably chosen regularizer, and for all t = 1, . . . , T and s = 1, . . . , t we define a
non-negative weight ↵
s,t
. Typically, all of the weights (↵
s,t
) are set to a constant value ⌘, called the
learning rate parameter.
However, since we are not in the full information setting and the decision maker does not observe g
t
,
the algorithms mentioned above cannot be used directly. The key observation of Flaxman et al. [11],
which is later reused in all of the follow-up work, is that g
t
can be estimated by randomly perturbing
the point x
t
. Specifically, on round t, the algorithm chooses the point
y
t
= x
t
+  A
t
u
t
, (2)
instead of the original point x
t
, where   > 0 is a parameter that controls the magnitude of the
perturbation, A
t
is a positive definite d ⇥ d matrix, and u
t
is drawn from the uniform distribution
on the unit sphere. In Flaxman et al. [11], A
t
is simply set to the identity matrix whereas in Saha
and Tewari [15], A
t
is more carefully tailored to the point x
t
(see details below). In any case, care
should be taken to ensure that the perturbed point y
t
remains in the convex set K. The observed
value f
t
(y
t
) is then used to compute the gradient estimate
ĝ
t
=
d
 
f
t
(y
t
)A 1
t
u
t
, (3)
and this estimate is fed to the first-order optimization algorithm. While ĝ
t
is not an unbiased es-
timator of rf
t
(x
t
), it is an unbiased estimator for the gradient of a different function, ˆf
t
, defined
by
ˆf
t
(x) = E
t
[f
t
(x+  A
t
v)] , (4)
where v 2 Rd is uniformly drawn from the unit ball. The function ˆf
t
(x) is a smoothed version of
f
t
, which plays a key role in our analysis and in many of the previous results on this topic. The main
property of ˆf
t
is summarized in the following lemma.
Lemma 1 (Flaxman et al. [11], Saha and Tewari [15, Lemma 5]). For any differentiable function
f : Rd 7! R, positive definite matrix A, x 2 Rd, and   2 (0, 1], define ĝ = (d/ )f(x+ Au)·A 1u,
where u is uniform on the unit sphere. Also, let ˆf(x) = E[f(x +  Av)] where v is uniform on the
unit ball. Then E[ĝ] = rˆf(x).
The difference between rf
t
(x
t
) and rˆf
t
(x
t
) is the bias of the gradient estimator ĝ
t
. The analysis
in Flaxman et al. [11], Abernethy et al. [2], Saha and Tewari [15] focuses on bounding the bias and
the variance of ĝ
t
and their effect on the first-order optimization algorithm.
3
2.3 Self-Concordant Barriers
Following [2, 1, 15], our algorithm and analysis rely on the properties of self-concordant barrier
functions. Intuitively, a barrier is a function defined on the interior of the convex body K, which is
rather flat in most of the interior of K and explodes to1 as we approach its boundary. Addition-
ally, a self-concordant barrier has some technical properties that are useful in our setting. Before
giving the formal definition of a self-concordant barrier, we define the local norm defined by a
self-concordant barrier.
Definition 2 (Local Norm Induced by a Self-Concordant Barrier [14]). Let R : int(K) 7! R be a
self-concordant barrier. The local norm induced by R at the point x 2 int(K) is denoted by kzk
x
and defined as kzk
x
=
p
zTr2R(x)z. Its dual norm is kzk
x,⇤ =
p
zT(r2R(x)) 1z.
In words, the local norm at x is the Mahalanobis norm defined by the Hessian of R at the point x,
namely, r2R(x). We now give a formal definition of a self-concordant barrier.
Definition 3 (Self-Concordant Barrier [14]). Let K ✓ Rd be a convex body. A function R :
int(K) 7! R is a #-self-concordant barrier for K if (i) R is three times continuously differentiable,
(ii) R(x)!1 as x! @K, and (iii) for all x 2 int(K) and y 2 Rd, R satisfies
|r3R(x)[y, y, y]|  2kyk3
x
and |rR(x) · y| 
p
#kyk
x
.
This definition is given for completeness, and is not directly used in our analysis. Instead, we rely
on some useful properties of self-concordant barriers. First and foremost, there exists a O(d)-self-
concordant barrier for any convex body [14, 6]. Efficiently-computable self-concordant barriers
are only known for specific classes of convex bodies, such as polytopes, yet we make the standard
assumption that we have an efficiently computable #-self-concordant barrier for the set K.
Another key feature of a self-concordant barrier is the set of Dikin ellipsoids that it defines. The
Dikin ellipsoid at x 2 int(K) is simply the unit ball with respect to the local norm at x. A key
feature of the Dikin ellipsoid is that it is entirely contained in the convex body K, for any x [see 14,
Theorem 2.1.1]. Another technical property of a self-concordant barriers is that its Hessian changes
slowly with respect to its local norm.
Theorem 4 (Nesterov and Nemirovskii [14, Theorem 2.1.1]). Let K be a convex body with self-
concordant barrier R. For any x 2 int(K) and z 2 Rd such that kzk
x
< 1, it holds that
(1  kzk
x
)
2r2R(x)   r2R(x+ z)   (1  kzk
x
)
 2r2R(x) .
While the self-concordant barrier explodes to infinity at the boundary of K, it is quite flat at points
that are far from the boundary. To make this statement formal, we define an operation that mul-
tiplicatively shrinks the set K toward the minimizer of R (called the analytic center of K). Let
y = argminR(x) and assume without loss of generality that R(y) = 0. For any ✏ 2 (0, 1) let K
y,✏
denote the set {y+(1  ✏)(x  y) : x 2 K}. The next theorem states that the barrier is flat in K
y,✏
and explodes to1 in the thin shell between K
y,✏
and K.
Theorem 5 (Nesterov and Nemirovskii [14, Propositions 2.3.2-3]). Let K be a convex body with
#-self-concordant barrier R, let y = argminR(x), and assume that R(y) = 0. For any ✏ 2 (0, 1],
it holds that
8 x 2 K
y,✏
R(x)  # log 1
✏
.
Our assumptions on the loss functions, as the Lipschitz assumption or the smoothness assumption,
are stated in terms of the standard Euclidean norm (which we denote by k · k2). Therefore, we will
need to relate the Euclidean norm to the local norms defined by the self-concordant barrier. This is
accomplished by the following lemma (whose proof appears in the supplementary material).
Lemma 6. Let K be a convex body with self-concordant barrier R and let D be the (Euclidean)
diameter of K. For any x 2 K, it holds that D 1kzk
x,⇤  kzk2  Dkzkx for all z 2 Rd.
2.4 Self-Concordant Barrier as a Regularizer
Looking back at the dual averaging strategy defined in Eq. (1), we can now fill in some of the details
that were left unspecified: [1, 15] set the regularization R in Eq. (1) to be a #-self-concordant barrier
for the set K. We use the following useful lemma from Abernethy and Rakhlin [1] in our analysis.
4
Algorithm 1: Bandit Smooth Convex Optimization
Parameters: perturbation parameter   2 (0, 1], dual averaging weights (↵
s,t
), self-concordant
barrier R : int(K) 7! R
Initialize: y1 2 K arbitrarily
for t = 1, . . . , T
A
t
 (r2R(x
t
))
 1/2
draw u
t
uniformly from the unit sphere
y
t
 x
t
+  A
t
u
t
choose y
t
, receive feedback f
t
(y
t
)
ĝ
t
 (d/ )f
t
(y
t
) ·A 1
t
u
t
x
t+1  argminx2K{x ·
P
t
s=1 ↵s,tĝs +R(x)}
Lemma 7 (Abernethy and Rakhlin [1]). Let K be a convex body with #-self-concordant barrier
R, let g1, . . . , gT be vectors in Rd, and let ⌘ > 0 be such that ⌘kgtkxt,⇤  14 for all t. Define
x
t
= argmin
x2K{x ·
P
t 1
s=1 ⌘gs +R(x)}. Then,
(i) for all t it holds that kx
t
  x
t+1kxt  2⌘kgtkxt,⇤;
(ii) for any x? 2 K it holds that
P
T
t=1 gt · (xt   x?) 
1
⌘
R(x?) + 2⌘
P
T
t=1 kgtk2xt,⇤.
Algorithms for bandit convex optimization that use a self-concordant regularizer also use the same
self-concordant barrier to obtain gradient estimates. Namely, these algorithms perturb the dual av-
eraging solution x
t
as in Eq. (3), with the perturbation matrix A
t
set to (r2R(x
t
))
 1/2, the root of
the inverse Hessian of R at the point x
t
. In other words, the distribution of y
t
is supported on the
Dikin ellipsoid centered at x
t
, scaled by  . Since   2 (0, 1], this form of perturbation guarantees
that y
t
2 K. Moreover, if y
t
is generated in this way and used to construct the gradient estimator
ĝ
t
, then the local norm of ĝ
t
is bounded, as specified in the following lemma.
Lemma 8 (Saha and Tewari [15, Lemma 5]). Let K ✓ Rd be a convex body with self-concordant
barrier R. For any differentiable function f : K 7! [0, 1],   2 (0, 1], and x 2 int(K), define
ĝ = (d/ )f(y) · A 1u, where A = (r2R(x)) 1/2, y = x+  Au, and u is drawn uniformly from
the unit sphere. Then kĝk
x
 d/ .
3 Main Result
Our algorithm for the bandit smooth convex optimization problem is a variant of the algorithm in
Saha and Tewari [15], and appears in Algorithm 1. Following Abernethy and Rakhlin [1], Saha and
Tewari [15], we use a self-concordant function as the dual averaging regularizer and we use its Dikin
ellipsoids to perturb the points x
t
. The difference between our algorithm and previous ones is the
introduction of dual averaging weights (↵
s,t
), for t = 1, . . . , T and s = 1, . . . , t, which allow us to
vary the weight of each gradient in the dual averaging objective function.
In addition to the parameters  , ⌘, and ✏, we introduce a new buffering parameter k, which takes
non-negative integer values. We set the dual averaging weights in Algorithm 1 to be
↵
s,t
=
(
⌘ if s  t  k
t s+1
k+1 ⌘ if s > t  k ,
(5)
where ⌘ > 0 is a global learning rate parameter. This choice of (↵
s,t
) effectively decreases the
influence of the feedback received on the most recent k rounds. If k = 0, all of the (↵
s,t
) become
equal to ⌘ and Algorithm 1 reduces to the algorithm in Saha and Tewari [15]. The surprising result
is that there exists a different setting of k > 0 that gives a better regret bound.
We introduce a slight abuse of notation, which helps us simplify the presentation of our regret bound.
We will eventually achieve the desired regret bound by setting the parameters ⌘,  , and k to be some
functions of T . Therefore, from now on, we treat the notation ⌘,  , and k as an abbreviation for the
functional forms ⌘(T ),  (T ), and k(T ) respectively. The benefit is that we can now use asymptotic
notation (e.g., O(⌘k)) to sweep meaningless low-order terms under the rug.
5
We prove the following regret bound for this algorithm.
Theorem 9. Let f1, . . . , fT be a sequence of loss functions where each ft : K 7! [0, 1] is dif-
ferentiable, convex, H-smooth and L-Lipschitz, and where K ✓ Rd is a convex body of diameter
D > 0 with #-self-concordant barrier R. For any  , ⌘ 2 (0, 1] and k 2 {0, 1, . . . , T} assume that
Algorithm 1 is run with these parameters and with the weights defined in Eq. (5) (using k and ⌘) to
generate the sequences x1, . . . , xT and y1, . . . , yT . If 12k⌘d    and for any ✏ 2 (0, 1) it holds that
R(T )  HD2 2T +
# log 1
✏
⌘
+
64d2⌘T
 2(k + 1)
+
12(HD2 +DL)d⌘
p
kT
 
+O
✓
T ✏
 
+ T⌘k
◆
.
Specifically, if we set   = d1/4T 3/16, ⌘ = d 1/2T 5/8, k = d1/2T 1/8, and ✏ = T 100, we get
that R(T ) = O(
p
d T 5/8 log T ).
Note that if we set k = 0 in our theorem, we recover the eO(T 2/3) bound in Saha and Tewari [15] up
to a small numerical constant (namely, the dependence on L, H , D, #, d, and T is the same).
4 Analysis
Using the notation x? = argmin
x2K
P
T
t=1 ft(x), the decision maker’s regret becomes R(T ) =
E
⇥P
T
t=1 ft(yt)   ft(x?)
⇤
. Following Flaxman et al. [11], Saha and Tewari [15], we rewrite the
regret as
R(T ) = E
"
TX
t=1
f
t
(y
t
)  ˆf
t
(x
t
)
#
| {z }
a
+ E
"
TX
t=1
ˆf
t
(x
t
)  ˆf
t
(x?)
#
| {z }
b
+ E
"
TX
t=1
ˆf
t
(x?)  f
t
(x?)
#
| {z }
c
. (6)
This decomposition essentially adds a layer of hallucination to the analysis: we pretend that the
loss functions are ˆf1, . . . , ˆfT instead of f1, . . . , fT and we also pretend that we chose the points
x1, . . . , xT rather than y1, . . . , yT . We then analyze the regret in this pretend world (this regret is
the expression in Eq. (6b)). Finally, we tie our analysis back to the real world by bounding the
difference between that which we analyzed and the regret of the actual problem (this difference is
the sum of Eq. (6a) and Eq. (6c)). The advantage of our pretend world over the real world is that we
have unbiased gradient estimates ĝ1, . . . , ĝT that can plug into the dual averaging algorithm.
The algorithm in Saha and Tewari [15] sets all of the dual averaging weights (↵
s,t
) equal to the
constant learning rate ⌘ > 0. It decomposes the regret as in Eq. (6) and their main technical result is
the following bound for the individual terms:
Theorem 10 (Saha and Tewari [15]). Let f1, . . . , fT be a sequence of loss functions where each
f
t
: K 7! [0, 1] is differentiable, convex, H-smooth and L-Lipschitz, and where K ✓ Rd is a convex
body of diameter D > 0 and #-self-concordant barrier R. Assume that Algorithm 1 is run with
perturbation parameter   2 (0, 1] and generates the sequences x1, . . . , xT and y1, . . . , yT . Then
for any ✏ 2 (0, 1) it holds that (6a) + (6c)  (HD2 2 + ✏L)T . If, additionally, the dual averaging
weights (↵
s,t
) are all set to the constant learning rate ⌘ then (6b)  # log(1/✏)⌘ 1 + d2  2⌘T .
The analysis in Saha and Tewari [15] goes on to obtain a regret bound of eO(T 2/3) by choosing
optimal values for the parameters ⌘,   and ✏ and plugging those values into Theorem 10. Our
analysis uses the first part of Theorem 10 to bound (6a) + (6c) and shows that our careful choice of
the dual averaging weights (↵
s,t
) results in the following improved bound on (6b).
We begin our analysis by defining a moving average of the functions ˆf1, . . . , ˆfT , as follows:
8 t = 1, . . . , T , ¯f
t
(x) =
1
k + 1
kX
i=0
ˆf
t i(x) , (7)
where, for soundness, we let ¯f
s
⌘ 0 for s  0. Also, define a moving average of gradient estimates:
8 t = 1, . . . , T , ḡ
t
=
1
k + 1
kX
i=0
ĝ
t i ,
6
again, with ĝ
s
= 0 for s  0. In Section 4 below, we show how each ḡ
t
can be used as a biased
estimate ofr¯f
t
(x
t
). Also note that the choice of the dual averaging weights (↵
s,t
) in Eq. (5) is such
that
P
t
s=1 ↵s,tĝs = ⌘
P
t
s=1 ḡs for all t. Therefore, the last step in Algorithm 1 basically performs
dual averaging with the gradient estimates ḡ1, . . . , ḡT uniformly weighted by ⌘.
We use the functions ¯f
t
to rewrite Eq. (6b) as
E
"
TX
t=1
ˆf
t
(x
t
)  ¯f
t
(x
t
)
#
| {z }
a
+ E
"
TX
t=1
¯f
t
(x
t
)  ¯f
t
(x?)
#
| {z }
b
+ E
"
TX
t=1
¯f
t
(x?)  ˆf
t
(x?)
#
| {z }
c
. (8)
This decomposition essentially adds yet another layer of hallucination to the analysis: we pretend
that the loss functions are ¯f1, . . . , ¯fT instead of ˆf1, . . . , ˆfT (which are themselves pretend loss
functions, as described above). Eq. (8b) is the regret in our new pretend scenario, while Eq. (8a) +
Eq. (8c) is the difference between this regret and the regret in Eq. (6b).
The following lemma bounds each of the terms in Eq. (8) separately, and summarizes the main
technical contribution of our paper.
Lemma 11. Under the conditions of Theorem 9, for any ✏ 2 (0, 1) it holds that (8c)  0 ,
(8a)  12DLd⌘
p
kT
 
+ O
 
(1 + ⌘)k
 
, and
(8b) 
# log 1
✏
⌘
+
64d2⌘T
 2(k + 1)
+
12HD2d⌘
p
kT
 
+ O
✓
T ✏
 
+ T⌘k
◆
.
4.1 Proof Sketch of Lemma 11
As mentioned above, the basic intuition of our technique is quite simple: average the gradients to
decrease their variance. Yet, applying this idea in the analysis is tricky. We begin by describing the
main source of difficulty in proving Lemma 11.
Recall that our strategy is to pretend that the loss functions are ¯f1, . . . , ¯fT and to use the random
vector ḡ
t
as a biased estimator of r¯f
t
(x
t
). Naturally, one of our goals is to show that this bias
is small. Recall that each ĝ
s
is an unbiased estimator of rˆf
s
(x
s
) (conditioned on the history up
to round t). Specifically, note that each vector in the sequence ĝ
t k, . . . , ĝt is a gradient estimate
at a different point. Yet, we average these vectors and claim that they accurately estimate r¯f
t
at
the current point, x
t
. Luckily, ˆf
t
is H-smooth, so rˆf
t i(xt i) should not be much different than
rˆf
t i(xt), provided that we show that xt i and xt are close to each other in Euclidean distance.
To show that x
t i and xt are close, we exploit the stability of the dual averaging algorithm. Particu-
larly, the first claim in Lemma 7 states that kx
s
 x
s+1kxs is controlled by kḡskxs,⇤ for all s, so now
we need to show that kḡ
s
k
xs,⇤ is small. However, ḡt is the average of k+1 gradient estimates taken
at different points; each ĝ
t i is designed to have a small norm with respect to its own local norm
k · k
xt i,⇤; for all we know, it may be very large with respect to the current local norm k · kxt,⇤. So
now we need to show that the local norms at x
t i and xt are similar. We could prove this if we knew
that x
t i and xt are close to each other—which is exactly what we set out to prove in the beginning.
This chicken-and-egg situation complicates our analysis considerably.
Another non-trivial component of our proof is the variance reduction analysis. The motivation
to average ĝ
t k, . . . , ĝt is to generate new gradient estimates with a smaller variance. While the
random vectors ĝ1, . . . , ĝT are not independent, we show that their randomness is uncorrelated.
Therefore, the variance of ḡ
t
is k + 1 times smaller than the variance of each ĝ
t
. However, to make
this argument formal, we again require the local norms at x
t i and xt to be similar.
To make things more complicated, there is the recurring need to move back and forth between local
norms and the Euclidean norm, since the latter is used in the definition of Lipschitz and smoothness.
All of this has to do with bounding Eq. (8b), the regret with respect to the pretend loss functions
¯f1, . . . , ¯fT - an additional bias term appears in the analysis of Eq. (8a).
We conclude the paper by stating our main lemmas and sketching the proof Lemma 11. The full
technical proofs are all deferred to the supplementary material and replaced with some high level
commentary.
7
To break the chicken-and-egg situation described above, we begin with a crude bound on kḡ
t
k
xt,⇤,
which does not benefit at all from the averaging operation. We simultaneously prove that the local
norms at x
t i and xt are similar.
Lemma 12. If the parameters k, ⌘, and   are chosen such that 12k⌘d    then for all t,
(i) kḡ
t
k
xt,⇤  2d/ ;
(ii) for any 0  i  k such that t  i   1 it holds that 12kzkxt i,⇤  kzkxt,⇤  2kzkxt i,⇤.
Lemma 12 itself has a chicken-and-egg aspect, which we resolve using an inductive proof technique.
Armed with the knowledge that the local norms at x
t i and xt are similar, we go on to prove the
more refined bound on E
t
⇥
kḡ
t
k2
xt,⇤
⇤
, which does benefit from averaging.
Lemma 13. If the parameters k, ⌘, and   are chosen such that 12k⌘d    then
E
t
⇥
kḡ
t
k2
xt,⇤
⇤
 2D2L2 + 32d
2
 2(k + 1)
.
The proof constructs a martingale difference sequence and uses the fact that its increments are un-
correlated. Compare the above to Lemma 8, which proves that kĝ
t ik2
xt i,⇤  d
2/ 2 and note the
extra k + 1 in our denominator—all of our hard work was aimed at getting this factor.
Next, we set out to bound the expected Euclidean distance between x
t i and xt. This bound is later
needed to exploit the L-Lipschitz and H-smooth assumptions. The crude bound on kḡ
s
k
xs,⇤ from
Lemma 12 is enough to satisfy the conditions of Lemma 7, which then tells us that E[kx
s
 x
s+1kxs ]
is controlled by E[kḡ
s
k
xs,⇤]. The latter enjoys the improved bound due to Lemma 13. Integrating
the resulting bound over time, we obtain the following lemma.
Lemma 14. If the parameters k, ⌘, and   are chosen such that 12k⌘d    then for all t and any
0  i  k such that t  i   1, we have
E[kx
t i   xtk2] 
12Dd⌘
p
k
 
+ O(⌘k) .
Notice that x
t i and xt may be k rounds apart, but the bound scales only with
p
k. Again, this is
the work of the averaging technique.
Finally, we have all the tools in place to prove our main result, Lemma 11.
Proof sketch. The first term, Eq. (8a), is bounded by rewriting ¯f
t
(x
t
) =
1
k+1
P
k
i=0
ˆf
t i(xt) and
then proving that ˆf
t i(xt) is not very far from ˆf t(xt). This follows from the fact that ˆf t is L-
Lipschitz and from Lemma 14. To bound the second term, Eq. (8b), we use the convexity of each ¯f
t
to write
E
"
TX
t=1
¯f
t
(x
t
)  ¯f
t
(x?)
#
 E
"
TX
t=1
r¯f
t
(x
t
) · (x
t
  x?)
#
.
We relate the right-hand side above to
E
"
TX
t=1
ḡ
t
· (x
t
  x?)
#
,
using the fact that ˆf
t
is H-smooth and again using Lemma 14. Then, we upper bound the above
using Lemma 7, Theorem 5, and Lemma 13. ⇤
Acknowledgments
We thank Jian Ding for several critical contributions during the early stages of this research. Parts
of this work were done while the second and third authors were at Microsoft Research, the support
of which is gratefully acknowledged.
8
References
[1] J. Abernethy and A. Rakhlin. Beating the adaptive bandit with high probability. In Information
Theory and Applications Workshop, 2009, pages 280–289. IEEE, 2009.
[2] J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for
bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory
(COLT), 2008.
[3] A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning The-
ory (COLT), 2010.
[4] A. Agarwal, D. P. Foster, D. Hsu, S. M. Kakade, and A. Rakhlin. Stochastic convex optimiza-
tion with bandit feedback. In Advances in Neural Information Processing Systems (NIPS),
2011.
[5] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.
[6] S. Bubeck and R. Eldan. The entropic barrier: a simple and optimal universal self-concordant
barrier. arXiv preprint arXiv:1412.1587, 2015.
[7] S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex opti-
mization. arXiv preprint arXiv:1507.06580, 2015.
[8] S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:
p
T regret in one
dimension. In In Proceedings of the 28st Annual Conference on Learning Theory (COLT),
2015.
[9] V. Dani, T. Hayes, and S. M. Kakade. The price of bandit information for online optimization.
In Advances in Neural Information Processing Systems (NIPS), 2008.
[10] O. Dekel, J. Ding, T. Koren, and Y. Peres. Bandits with switching costs: T 2/3 regret. In
Proceedings of the 46th Annual Symposium on the Theory of Computing, 2014.
[11] A. D. Flaxman, A. Kalai, and H. B. McMahan. Online convex optimization in the bandit
setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-
SIAM symposium on Discrete algorithms, pages 385–394. Society for Industrial and Applied
Mathematics, 2005.
[12] E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in
Neural Information Processing Systems (NIPS), 2014.
[13] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical program-
ming, 120(1):221–259, 2009.
[14] Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex programming,
volume 13. SIAM, 1994.
[15] A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with
bandit feedback. In International Conference on Artificial Intelligence and Statistics (AISTAT),
pages 636–642, 2011.
[16] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends
in Machine Learning, 4(2):107–194, 2011.
[17] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML’03), pages
928–936, 2003.
9
