


Paper ID = 5719
Title = On the Limitation of Spectral Methods:
From the Gaussian Hidden Clique Problem to
Rank-One Perturbations of Gaussian Tensors
Andrea Montanari
Department of Electrical Engineering and Department of Statistics. Stanford University.
montanari@stanford.edu
Daniel Reichman
Department of Cognitive and Brain Sciences, University of California, Berkeley, CA
daniel.reichman@gmail.com
Ofer Zeitouni
Faculty of Mathematics, Weizmann Institute, Rehovot 76100, Israel
and Courant Institute, New York University
ofer.zeitouni@weizmann.ac.il
Abstract
We consider the following detection problem: given a realization of a symmetric
matrix X of dimension n, distinguish between the hypothesis that all upper tri-
angular variables are i.i.d. Gaussians variables with mean 0 and variance 1 and
the hypothesis that there is a planted principal submatrix B of dimension L for
which all upper triangular variables are i.i.d. Gaussians with mean 1 and variance
1, whereas all other upper triangular elements of X not in B are i.i.d. Gaussians
variables with mean 0 and variance 1. We refer to this as the ‚ÄòGaussian hidden
clique problem‚Äô. When L = (1 + )
‚àö
n ( > 0), it is possible to solve this de-
tection problem with probability 1 ‚àí on(1) by computing the spectrum of X and
considering the largest eigenvalue of X. We prove that when L < (1 ‚àí )
‚àö
n no
algorithm that examines only the eigenvalues of X can detect the existence of a
hidden Gaussian clique, with error probability vanishing as n ‚Üí ‚àû. The result
above is an immediate consequence of a more general result on rank-one pertur-
bations of k-dimensional Gaussian tensors. In this context we establish a lower
bound on the critical signal-to-noise ratio below which a rank-one signal cannot
be detected.
1 Introduction
Consider the following detection problem. One is given a symmetric matrix X = X(n) of dimen-
sion n, such that the
(
n
2
)
+ n entries (Xi,j)i‚â§j are mutually independent random variables. Given
(a realization of) X one would like to distinguish between the hypothesis that all random variables
Xi,j have the same distribution F0 to the hypothesis where there is a set U ‚äÜ [n], with L := |U |,
so that all random variables in the submatrix XU := (Xs,t : s, t ‚àà U) have a distribution F1 that is
different from the distribution of all other elements in X which are still distributed as F0. We refer
to XU as the hidden submatrix.
1
The same problem was recently studied in [1, 8] and, for the asymmetric case (where no symmetry
assumption is imposed on the independent entries of X), in [6, 18, 20]. Detection problems with
similar flavor (such as the hidden clique problem) have been studied over the years in several fields
including computer science, physics and statistics. We refer to Section 5 for further discussion
of the related literature. An intriguing outcome of these works is that, while the two hypothesis are
statistically distinguishable as soon as L ‚â• C log n (for C a sufficiently large constant) [7], practical
algorithms require significantly larger L. In this paper we study the class of spectral (or eigenvalue-
based) tests detecting the hidden submatrix. Our proof technique naturally allow to consider two
further generalizations of this problem that are of independent interests. We briefly summarize our
results below.
The Gaussian hidden clique problem. This is a special case of the above hypothesis testing setting,
whereby F0 = N(0, 1) and F1 = N(1, 1) (entries on the diagonal are defined slightly differently in
order to simplify calculations). Here and below N(m,œÉ2) denote the Gaussian distribution of mean
m and variance œÉ2. Equivalently, let Z be a random matrix from the Gaussian Orthogonal Ensemble
(GOE) i.e. Zij ‚àº N(0, 1/n) independently for i < j, and Zii ‚àº N(0, 2/n). Then, under hypothesis
H1,L we have X = n‚àí1/21U1TU + Z (1U being the indicator vector of U ), and under hypothesis
H0, X = Z (the factor n in the normalization is for technical convenience). The Gaussian hidden
clique problem can be thought of as the following clustering problem: there are n elements and the
entry (i, j) measures the similarity between elements i and j. The hidden submatrix corresponds to
a cluster of similar elements, and our goal is to determine given the matrix whether there is a large
cluster of similar elements or alternatively, whether all similarities are essentially random (Gaussian)
noise.
Our focus in this work is on the following restricted hypothesis testing question. Let Œª1 ‚â• Œª2 ‚â•
¬∑ ¬∑ ¬∑ ‚â• Œªn be the ordered eigenvalues of X. Is there a test that depends only on Œª1, . . . , Œªn and
that distinguishes H0 from H1,L ‚Äòreliably,‚Äô i.e. with error probability converging to 0 as n ‚Üí ‚àû?
Notice that the eigenvalues distribution does not depend on U as long as this is independent from
the noise Z. We can therefore think of U as fixed for this question. Historically, the first polynomial
time algorithm for detecting a planted clique of size O(
‚àö
n) in a random graph [2] relied on spectral
methods (see Section 5 for more details). This is one reason for our interest in spectral tests for the
Gaussian hidden clique problem.
If L ‚â• (1 + Œµ)
‚àö
n then [11] implies that a simple test checking whether Œª1 ‚â• 2 + Œ¥ for some
Œ¥ = Œ¥(Œµ) > 0 is reliable for the Gaussian hidden clique problem. We prove that this result is tight,
in the sense that no spectral test is reliable for L ‚â§ (1‚àí Œµ)
‚àö
n.
Rank-one matrices in Gaussian noise. Our proof technique builds on a simple observation. Since
the noise Z is invariant under orthogonal transformations1, the above question is equivalent to the
following testing problem. For Œ≤ ‚àà R‚â•0, and v ‚àà Rn, ‚Äñv‚Äñ2 = 1 a uniformly random unit vector,
test H0: X = Z versus H1, X = Œ≤vvT +Z. (The correspondence between the two problems yields
Œ≤ = L/
‚àö
n.)
Again, this problem (and a closely related asymmetric version [22]) has been studied in the literature,
and it follows from [11] that a reliable test exists for Œ≤ ‚â• 1 + Œµ. We provide a simple proof (based
on the second moment method) that no test is reliable for Œ≤ < 1‚àí Œµ.
Rank-one tensors in Gaussian noise. It turns that the same proof applies to an even more general
problem: detecting a rank-one signal in a noisy tensor. We carry out our analysis in this more
general setting for two reasons. First, we think that this clarifies the what aspects of the model are
important for our proof technique to apply. Second, the problem estimating tensors from noisy data
has attracted significant interest recently within the machine learning community [15, 21].
More precisely, we consider a noisy tensor X ‚àà
‚äók Rn, of the form X = Œ≤ v‚äók + Z, where Z is
Gaussian noise, and v is a random unit vector. We consider the problem of testing this hypothesis
against H0: X = Z. We establish a threshold Œ≤2ndk such that no test can be reliable for Œ≤ < Œ≤
2nd
k
(in particular Œ≤2nd2 = 1). Two differences are worth remarking for k ‚â• 3 with respect to the more
familiar matrix case k = 2. First, we do not expect the second moment bound Œ≤2ndk to be tight,
i.e. a reliable test to exist for all Œ≤ > Œ≤2ndk . On the other hand, we can show that it is tight up to
1By this we mean that, for any orthogonal matrix R ‚àà O(n), independent of Z, RZRT is distributed as Z.
2
a universal (k and n independent) constant. Second, below Œ≤2ndk the problem is more difficult than
the matrix version below Œ≤2nd2 = 1: not only no reliable test exists but, asymptotically, any test
behaves asymptotically as random guessing. For more details on our results regarding noisy tensors,
see Theorem 3.
2 Main result for spectral detection
Let Z be a GOE matrix as defined in the previous section. Equivalently if G is an (asymmetric)
matrix with i.i.d. entries Gi,j ‚àº N(0, 1),
Z =
1‚àö
2n
(
G + GT
)
. (1)
For a deterministic sequence of vectors v(n), ‚Äñv(n)‚Äñ2 = 1, we consider the two hypotheses{
H0 : X = Z ,
H1,Œ≤ : X = Œ≤vv
T + Z .
(2)
A special example is provided by the Gaussian hidden clique problem in which case Œ≤ = L/
‚àö
n and
v = 1U/
‚àö
L for some set U ‚äÜ [n], |U | = L,{
H0 : X = Z ,
H1,L : X =
1‚àö
n
1U1
T
U + Z .
(3)
Observe that the distribution of eigenvalues of X, under either alternative, is invariant to the choice
of the vector v (or subset U ), as long as the norm of v is kept fixed. Therefore, any successful
algorithm that examines only the eigenvalues, will distinguish between H0 and H1,Œ≤ but not give
any information on the vector v (or subset U , in the case of H1,L).
We let Q0 = Q0(n) (respectively, Q1 = Q1(n)) denote the distribution of the eigenvalues of X
under H0 (respectively H1 = H1,Œ≤ or H1,L).
A spectral statistical test for distinguishing between H0 and H1 (or simply a spectral test) is a
measurable map Tn : (Œª1, . . . , Œªn) 7‚Üí {0, 1}. To formulate precisely what we mean by the word
distinguish, we introduce the following notion.
Definition 1. For each n ‚àà N, let P0,n, P1,n be two probability measures on the same measure
space (‚Ñ¶n,Fn). We say that the sequence (P1,n) is contiguous with respect to (P0,n) if, for any
sequence of events An ‚àà Fn,
lim
n‚Üí‚àû
P0,n(An) = 0 ‚áí lim
n‚Üí‚àû
P1,n(An) = 0 . (4)
Note that contiguity is not in general a symmetric relation.
In the context of the spectral statistical tests described above, the sequences An in Definition 1
(with Pn = Q0(n) and Qn = Q1(n)) can be put in correspondence with spectral statistical tests
Tn by taking An = {(Œª1, . . . , Œªn) : Tn(Œª1, . . . , Œªn) = 0}. We will thus say that H1 is spectrally
contiguous with respect to H0 if Qn is contiguous with respect to Pn.
Our main result on the Gaussian hidden clique problem is the following.
Theorem 1. For any sequence L = L(n) satisfying lim supn‚Üí‚àû L(n)/
‚àö
n < 1, the hypotheses
H1,L are spectrally contiguous with respect to H0.
2.1 Contiguity and integrability
Contiguity is related to a notion of uniform absolute continuity of measures. Recall that a probability
measure ¬µ on a measure space is absolutely continuous with respect to another probability measure
ŒΩ if for every measurable set A, ŒΩ(A) = 0 implies that ¬µ(A) = 0, in which case there exists a
ŒΩ-integrable, non-negative function f ‚â° d¬µdŒΩ (the Radon-Nikodym derivative of ¬µ with respect to ŒΩ),
so that ¬µ(A) =
‚à´
A
f dŒΩ for every measurable set A. We then have the following known useful fact:
3
Lemma 2. Within the setting of Definition 1, assume that P1,n is absolutely continuous with respect
to P0,n, and denote by Œõn ‚â° dP1,ndP0,n its Radon-Nikodym derivative.
(a) If lim supn‚Üí‚àû E0,n(Œõ2n) <‚àû, then (P1,n) is contiguous with respect to (P0,n).
(b) If limn‚Üí‚àû E0,n(Œõ2n) = 1, then limn‚Üí‚àû ‚ÄñP0,n ‚àí P1,n‚ÄñTV = 0, where ‚Äñ ¬∑ ‚ÄñTV denotes the total
variation distance, i.e.
‚ÄñP0,n ‚àí P1,n‚ÄñTV ‚â° sup
A
|P0,n(A)‚àí P1,n(A)‚Äñ.
2.2 Method and structure of the paper
Consider problem (2). We use the fact that the law of the eigenvalues under both H0 and H1,Œ≤ are
invariant under conjugations by a orthogonal matrix. Once we conjugate matrices sampled under the
hypothesis H1,Œ≤ by an independent orthogonal matrix sampled according to the Haar distribution,
we get a matrix distributed as
X = Œ≤vvT + Z , (5)
where u is uniform on the n-dimensional sphere, and Z is a GOE matrix (with off-diagonal entries
of variance 1/n). Letting P1,n denote the law of Œ≤uuT + Z and P0,n denote the law of Z, we show
that P1,n is contiguous with respect to P0,n, which implies that the law of eigenvalues Q1(n) is
contiguous with respect to Q0(n).
To show the contiguity, we consider a more general setup, of independent interest, of Gaussian
tensors of order k, and in that setup show that the Radon-Nikodym derivative Œõn,L =
dP1,n
dP0,n is
uniformly square integrable under P0,n; an application of Lemma 2 then quickly yields Theorem 1.
The structure of the paper is as follows. In the next section, we define formally the detection problem
for a symmetric tensor of order k ‚â• 2. We show the existence of a threshold under which detection
is not possible (Theorem 3), and show how Theorem 1 follows from this. Section 4 is devoted to
the proof of Theorem 3, and concludes with some additional remarks and consequences of Theorem
3. Finally, Section 5 is devoted to a description of the relation between the Gaussian hidden clique
problem and hidden clique problem in computer science, and related literature.
3 A symmetric tensor model and a reduction
Exploiting rotational invariance, we will reduce the spectral detection problem to a detection prob-
lem involving a standard detection problem between random matrices. Since the latter generalizes
to a tensor setup, we first introduce a general Gaussian hypothesis testing for k-tensors, which is
of independent interest. We then explain how the spectral detection problem reduces to the special
case of k = 2.
3.1 Preliminaries and notation
We use lower-case boldface for vectors (e.g. u, v) and upper-case boldface for matrices and
tensors (e.g. X,Z). The ordinary scalar product and `p norm over vectors are denoted by
„Äàu,v„Äâ =
‚àën
i=1 uivi, and ‚Äñv‚Äñp. We write Sn‚àí1 for the unit sphere in n dimensions
Sn‚àí1 ‚â°
{
x ‚àà Rn : ‚Äñx‚Äñ2 = 1
}
. (6)
Given X ‚àà
‚äók Rn a real k-th order tensor, we let {Xi1,...,ik}i1,...,ik denote its coordinates. The
outer product of two tensors is X ‚äóY, and, for v ‚àà Rn, we define v‚äók = v ‚äó ¬∑ ¬∑ ¬∑ ‚äó v ‚àà
‚äók Rn
as the k-th outer power of v. We define the inner product of two tensors X,Y ‚àà
‚äók Rn as
„ÄàX,Y„Äâ =
‚àë
i1,¬∑¬∑¬∑ ,ik‚àà[n]
Xi1,¬∑¬∑¬∑ ,ikYi1,¬∑¬∑¬∑ ,ik . (7)
4
We define the Frobenius (Euclidean) norm of a tensor X by ‚ÄñX‚ÄñF =
‚àö
„ÄàX,X„Äâ, and its operator
norm by
‚ÄñX‚Äñop ‚â° max{„ÄàX,u1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó uk„Äâ : ‚àÄi ‚àà [k] , ‚Äñui‚Äñ2 ‚â§ 1}. (8)
It is easy to check that this is indeed a norm. For the special case k = 2, it reduces to the ordinary
`2 matrix operator norm (equivalently, to the largest singular value of X).
For a permutation œÄ ‚àà Sk, we will denote by XœÄ the tensor with permuted indices XœÄi1,¬∑¬∑¬∑ ,ik =
XœÄ(i1),¬∑¬∑¬∑ ,œÄ(ik). We call the tensor X symmetric if, for any permutation œÄ ‚àà Sk, XœÄ = X. It is
proved [23] that, for symmetric tensors, we have the equivalent representation
‚ÄñX‚Äñop ‚â° max{|„ÄàX,u‚äók„Äâ| : ‚Äñu‚Äñ2 ‚â§ 1}. (9)
We define R ‚â° R ‚à™‚àû with the usual conventions of arithmetic operations.
3.2 The symmetric tensor model and main result
We denote by G ‚àà
‚äók Rn a tensor with independent and identically distributed entries Gi1,¬∑¬∑¬∑ ,ik ‚àº
N(0, 1) (note that this tensor is not symmetric).
We define the symmetric standard normal noise tensor Z ‚àà
‚äók Rn by
Z =
1
k!
‚àö
2
n
‚àë
œÄ‚ààSk
GœÄ . (10)
Note that the subset of entries with unequal indices form an i.i.d. collection {Zi1,i2,...,ik}i1<¬∑¬∑¬∑<ik ‚àº
N(0, 2/(n(k!))).
With this normalization, we have, for any symmetric tensor A ‚àà
‚äók Rn
E
{
e„ÄàA,Z„Äâ
}
= exp
{ 1
n
‚ÄñA‚Äñ2F
}
. (11)
We will also use the fact that Z is invariant in distribution under conjugation by orthogonal transfor-
mations, that is, that for any orthogonal matrix U ‚àà O(n), {Zi1,...,ik} has the same distribution as
{
‚àë
j1,...,jk
(‚àèk
`=1 Ui`,j`
)
¬∑ Zj1,...,jk}.
Given a parameter Œ≤ ‚àà R‚â•0, we consider the following model for a random symmetric tensor X:
X ‚â° Œ≤ v‚äók + Z , (12)
with Z a standard normal tensor, and v uniformly distributed over the unit sphere Sn‚àí1. In the case
k = 2 this is the standard rank-one deformation of a GOE matrix.
We let PŒ≤ = P(k)Œ≤ denote the law of X under model (12).
Theorem 3. For k ‚â• 2, let
Œ≤2ndk ‚â° inf
q‚àà(0,1)
‚àö
‚àí 1
qk
log(1‚àí q2) . (13)
Assume Œ≤ < Œ≤2ndk . Then, for any k ‚â• 3, we have
lim
n‚Üí‚àû
‚à•‚à•PŒ≤ ‚àí P0‚à•‚à•TV = 0 . (14)
Further, for k = 2 and Œ≤ < Œ≤2ndk = 1, PŒ≤ is contiguous with respect to P0.
A few remarks are in order, following Theorem 3.
First, it is not difficult to derive the asymptotic Œ≤2ndk =
‚àö
log(k/2) + ok(1) for large k.
5
Second, for k = 2 we get using log(1‚àí q2) ‚â§ ‚àíq2, that Œ≤2ndk = 1. Recall that for k = 2 and Œ≤ > 1,
it is known that the largest eigenvalue of X, Œª1(X) converges almost surely to (Œ≤ + 1/Œ≤) [11]. As
a consequence ‚ÄñP0 ‚àí PŒ≤‚ÄñTV ‚Üí 1 for all Œ≤ > 1: the second moment bound is tight.
For k ‚â• 3, it follows by the triangle inequality that ‚ÄñX‚Äñop ‚â• Œ≤ ‚àí ‚ÄñZ‚Äñop, and further
lim supn‚Üí‚àû ‚ÄñZ‚Äñop ‚â§ ¬µk almost surely as n ‚Üí ‚àû [19, 5] for some bounded ¬µk. It follows that
‚ÄñP0 ‚àí PŒ≤‚ÄñTV ‚Üí 1 for all Œ≤ > 2¬µk [21]. Hence, the second moment bound is off by a k-dependent
factor. For large k, 2¬µk =
‚àö
2 log k +Ok(1) and hence the factor is indeed bounded in k.
Behavior below the threshold. Let us stress an important qualitative difference between k = 2 and
k ‚â• 3, for Œ≤ < Œ≤2ndk . For k ‚â• 3, the two models are indistinguishable and any test is essentially as
good as random guessing. Formally, for any measurable function T : ‚äókRn ‚Üí {0, 1}, we have
lim
n‚Üí‚àû
[
P0(T (X) = 1) + PŒ≤(T (X) = 0)
]
= 1 . (15)
For k = 2, our result implies that, for Œ≤ < 1, ‚ÄñP0 ‚àí PŒ≤‚ÄñTV is bounded away from 1. On the other
hand, it is easy to see that it is bounded away from 0 as well, i.e.
0 < lim inf
n‚Üí‚àû
‚ÄñP0 ‚àí PŒ≤‚ÄñTV ‚â§ lim sup
n‚Üí‚àû
‚ÄñP0 ‚àí PŒ≤‚ÄñTV < 1 . (16)
Indeed, consider for instance the statistics S = Tr(X). Under P0, S ‚àº N(0, 2), while under PŒ≤ ,
S ‚àº N(Œ≤, 2). Hence
lim inf
n‚Üí‚àû
‚ÄñP0 ‚àí PŒ≤‚ÄñTV ‚â• ‚ÄñN(0, 1)‚àí N(Œ≤/
‚àö
2, 1)‚ÄñTV = 1‚àí 2Œ¶
(
‚àí Œ≤
2
‚àö
2
)
> 0 (17)
(Here Œ¶(x) =
‚à´ x
‚àí‚àû e
‚àíz2/2dz/
‚àö
2œÄ is the Gaussian distribution function.) The same phenomenon
for rectangular matrices (k = 2) is discussed in detail in [22].
3.3 Reduction of spectral detection to the symmetric tensor model, k = 2
Recall that in the setup of Theorem 1, Q0,n is the law of the eigenvalues of X under H0 and Q1,n
is the law of the eigenvalues of X under H1,L. Then Q1,n is invariant by conjugation of orthogonal
matrices. Therefore, the detection problem is not changed if we replace X = n‚àí1/21U1TU + Z by
XÃÇ ‚â° RXRT = 1‚àö
n
R1U (R1U )
T + RZRT , (18)
where R ‚àà O(n) is an orthogonal matrix sampled according to the Haar measure. A direct calcula-
tion yields
XÃÇ = Œ≤vvT + ZÃÉ, (19)
where v is uniform on the n dimensional sphere, Œ≤ = L/
‚àö
n, and ZÃÉ is a GOE matrix (with off-
diagonal entries of variance 1/n). Furthermore, v and ZÃÉ are independent of one another.
Let P1,n be the law of XÃÇ. Note that P1,n = P(k=2)Œ≤ with Œ≤ = L/
‚àö
n. We can relate the detection
problem of H0 vs. H1,L to the detection problem of P0,n vs. P1,n as follows.
Lemma 4. (a) If P1,n is contiguous with respect to P0,n then H1,L is spectrally contiguous with
respect to H0.
(b) We have
‚ÄñQ0,n ‚àíQ1,n‚ÄñTV ‚â§ ‚ÄñP0,n ‚àí P1,n‚ÄñTV.
In view of Lemma 4, Theorem 1 is an immediate consequence of Theorem 3.
4 Proof of Theorem 3
The proof uses the following large deviations lemma, which follows, for instance, from [9, Proposi-
tion 2.3].
6
Lemma 5. Let v a uniformly random vector on the unit sphere Sn‚àí1 and let „Äàv, e1„Äâ be its first
coordinate. Then, for any interval [a, b] with ‚àí1 ‚â§ a < b ‚â§ 1
lim
n‚Üí‚àû
1
n
logP(„Äàv, e1„Äâ ‚àà [a, b]) = max
{1
2
log(1‚àí q2) : q ‚àà [a, b]
}
. (20)
Proof of Theorem 3. We denote by Œõ the Radon-Nikodym derivative of PŒ≤ with respect to P0. By
definition E0Œõ = 1. It is easy to derive the following formula
Œõ =
‚à´
exp
{
‚àí nŒ≤
2
4
+
nŒ≤
2
„ÄàX,v‚äók„Äâ
}
¬µn(dv) . (21)
where ¬µn is the uniform measure on Sn‚àí1. Squaring and using (11), we get
E0Œõ2 = e‚àínŒ≤
2/2
‚à´
E0 exp
{nŒ≤
2
„ÄàX,v1‚äók + v2‚äók„Äâ
}
¬µn(dv1)¬µn(dv2)
= e‚àínŒ≤
2/2
‚à´
exp
{nŒ≤2
4
‚à•‚à•v1‚äók + v2‚äók‚à•‚à•2F}¬µn(dv1)¬µn(dv2)
=
‚à´
exp
{nŒ≤2
2
„Äàv1,v2„Äâk
}
¬µn(dv1)¬µn(dv2)
=
‚à´
exp
{nŒ≤2
2
„Äàv, e1„Äâk
}
¬µn(dv) , (22)
where in the first step we used (11) and in the last step, we used rotational invariance.
Let FŒ≤ : [‚àí1, 1]‚Üí R be defined by
FŒ≤(q) ‚â°
Œ≤2qk
2
+
1
2
log(1‚àí q2) . (23)
Using Lemma 5 and Varadhan‚Äôs lemma, for any ‚àí1 ‚â§ a < b ‚â§ 1,‚à´
exp
{nŒ≤2
2
„Äàv, e1„Äâk
}
I(„Äàv, e1„Äâ ‚àà [a, b])¬µn(dv) = exp
{
n max
q‚àà[a,b]
FŒ≤(q) + o(n)
}
. (24)
It follows from the definition of Œ≤2ndk that max|q|‚â•Œµ FŒ≤(q) < 0 for any Œµ > 0. Hence
E0Œõ2 ‚â§
‚à´
exp
{nŒ≤2
2
„Äàv, e1„Äâk
}
I(|„Äàv, e1„Äâ| ‚â§ Œµ)¬µn(dv) + e‚àíc(Œµ)n , (25)
for some c(Œµ) > 0 and all n large enough. Next notice that, under ¬µn, „Äàv, e1„Äâ
d
= G/(G2 +Zn‚àí1)
1/2
whereG ‚àº N(0, 1) and Zn‚àí1 is a œá2 with n‚àí1 degrees of freedom independent ofG. Then, letting
Zn ‚â° G2 + Zn‚àí1 (a œá2 with n degrees of freedom)
E0Œõ2 ‚â§ E
{
exp
(nŒ≤2
2
|G|k
Z
k/2
n
)
I(|G/Z1/2n | ‚â§ Œµ)
}
+ e‚àíc(Œµ)n
‚â§ E
{
exp
(nŒ≤2
2
|G|k
Z
k/2
n
)
I(|G/Z1/2n | ‚â§ Œµ) I(Zn‚àí1 ‚â• n(1‚àí Œ¥))
}
+ enŒ≤
2Œµk/2P
{
Zn‚àí1 ‚â§ n(1‚àí Œ¥)
}
+ e‚àíc(Œµ)n
‚â§ E
{
exp
( n1‚àí(k/2)Œ≤2
2(1‚àí Œ¥)k/2
|G|k
)
I(|G|2 ‚â§ 2Œµn)
}
+ enŒ≤
2Œµk/2P
{
Zn‚àí1 ‚â§ n(1‚àí Œ¥)
}
+ e‚àíc(Œµ)n
=
2‚àö
2œÄ
‚à´ 2Œµn
0
eC(Œ≤,Œ¥)n
1‚àík/2xk‚àíx2/2dx+ enŒ≤
2Œµk/2P
{
Zn‚àí1 ‚â§ n(1‚àí Œ¥)
}
+ e‚àíc(Œµ)n , (26)
where C(Œ≤, Œ¥) = Œ≤2/(2(1‚àí Œ¥)k/2). Now, for any Œ¥ > 0, we can (and will) choose Œµ small enough
so that both enŒ≤
2Œµk/2P
{
Zn‚àí1 ‚â§ n(1 ‚àí Œ¥)
}
‚Üí 0 exponentially fast (by tail bounds on œá2 random
variables) and, if k ‚â• 3, the argument of the exponent in the integral in the right hand side of (26)
7
is bounded above by ‚àíx2/4, which is possible since the argument vanishes at x‚àó = 2C(Œ≤, Œ¥)n1/2.
Hence, for any Œ¥ > 0, and all n large enough, we have
E0Œõ2 ‚â§
2‚àö
2œÄ
‚à´ 2Œµn
0
eC(Œ≤,Œ¥)n
1‚àík/2xk‚àíx2/2dx+ e‚àíc(Œ¥)n , (27)
for some c(Œ¥) > 0.
Now, for k ‚â• 3 the integrand in (27) is dominated by e‚àíx2/4 and converges pointwise (as n‚Üí‚àû)
to 1. Therefore, since E0Œõ2 ‚â• (E0Œõ)2 = 1,
k ‚â• 3 : lim
n‚Üí‚àû
E0Œõ2 = 1 . (28)
For k = 2, the argument is independent of n and can be integrated immediately, yielding (after
taking the limit Œ¥ ‚Üí 0)
k = 2 : lim sup
n‚Üí‚àû
E0Œõ2 ‚â§
1‚àö
1‚àí Œ≤2
. (29)
(Indeed, the above calculation implies that the limit exists and is given by the right-hand side.)
The proof is completed by invoking Lemma 2.
5 Related work
In the classical G(n, 1/2) planted clique problem, the computational problem is to find the planted
clique (of cardinality k) in polynomial time, where we assume the location of the planted clique is
hidden and is not part of the input. There are several algorithms that recover the planted clique in
polynomial time when k = C
‚àö
n where C > 0 is a constant independent of n [2, 8, 10]. Despite
significant effort, no polynomial time algorithm for this problem is known when k = o(
‚àö
n). In the
decision version of the planted clique problem, one seeks an efficient algorithm that distinguishes
between a random graph distributed as G(n, 1/2) or a random graph containing a planted clique of
size k ‚â• (2 + Œ¥) log n (for Œ¥ > 0; the natural threshold for the problem is the size of the largest
clique in a random sample of G(n, 1/2), which is asymptotic to 2 log n [14]). No polynomial time
algorithm is known for this decision problem if k = o(
‚àö
n).
As another example, consider the following setting introduced by [4] (see also [1]): one is given
a realization of a n-dimensional Gaussian vector x := (x1, ..,xn) with i.i.d. entries. The goal is
to distinguish between the following two hypotheses. Under the first hypothesis, all entries in x
are i.i.d. standard normals. Under the second hypothesis, one is given a family of subsets C :=
{S1, ..., Sm} such that for every 1 ‚â§ k ‚â§ m,Sk ‚äÜ {1, ..., n} and there exists an i ‚àà {1, . . . ,m}
such that, for any Œ± ‚àà Si, xŒ± is a Gaussian random variable with mean ¬µ > 0 and unit variance
whereas for every Œ± /‚àà Si, xŒ± is standard normal. (The second hypothesis does not specify the
index i, only its existence). The main question is how large ¬µ must be such that one can reliably
distinguish between these two hypotheses. In [4], Œ± are vertices in certain undirected graphs and the
family C is a set of pre-specified paths in these graphs.
The Gaussian hidden clique problem is related to various applications in statistics and computational
biology [6, 18]. That detection is statistically possible when L  log n was established in [1]. In
terms of polynomial time detection, [8] show that detection is possible when L = Œò(
‚àö
n) for the
symmetric cases. As noted, no polynomial time algorithm is known for the Gaussian hidden clique
problem when k = o(
‚àö
n). In [1, 20] it was hypothesized that the Gaussian hidden clique problem
should be difficult when L
‚àö
n.
The closest results to ours are the ones of [22]. In the language of the present paper, these authors
consider a rectangular matrix of the form X = Œªv1vT2 + Z ‚àà Rn1√ón2 whereby Z has i.i.d. entries
Zij ‚àº N(0, 1/n1), v1 is deterministic of unit norm, and v2 has entries which are i.i.d. N(0, 1/n1),
independent of Z. They consider the problem of testing this distribution against Œª = 0. Setting
c = limn‚Üí‚àû
n1
n2
, it is proved in [22] that the distribution of the singular values of X under the
null and the alternative are mutually contiguous if Œª <
‚àö
c and not mutually contiguous if Œª >
‚àö
c.
While [22] derive some more refined results, their proofs rely on advanced tools from random matrix
theory [13], while our proof is simpler, and generalizable to other settings (e.g. tensors).
8
References
[1] L. Addario-Berry, N. Broutin, L. Devroye, G. Lugosi. On combinatorial testing problems. Annals of
Statistics 38(5) (2011), 3063‚Äì3092.
[2] N. Alon, M. Krivelevich and B. Sudakov. Finding a large hidden clique in a random graph. Random
Structures and Algorithms 13 (1998), 457‚Äì466.
[3] G. W. Anderson, A. Guionnet and O. Zeitouni. An introduction to random matrices. Cambridge Univer-
sity Press (2010).
[4] E. Arias-Castro, E. J., CandeÃÄs, H. Helgason and O. Zeitouni. Searching for a trail of evidence in a maze.
Annals of Statistics 36 (2008), 1726‚Äì1757.
[5] A. Auffinger, G. Ben Arous, and J. Cerny. Random matrices and complexity of spin glasses. Communi-
cations on Pure and Applied Mathematics 66(2) (2013), 165‚Äì201.
[6] S. Balakrishnan, M. Kolar, A. Rinaldo, A. Singh, and L. Wasserman. Statistical and computational
tradeoffs in biclustering. NIPS Workshop on Computational Trade-offs in Statistical Learning (2011).
[7] S. Bhamidi, P.S. Dey, and A.B. Nobel. Energy landscape for large average submatrix detection problems
in Gaussian random matrices. arXiv:1211.2284.
[8] Y. Deshpande and A. Montanari. Finding hidden cliques of size
‚àö
N/e in nearly linear time. Foundations
of Computational Mathematics (2014), 1‚Äì60
[9] A. Dembo and O. Zeitouni. Matrix optimization under random external fields. arXiv:1409.4606
[10] U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semi-random graph.
Random Struct. Algorithms 162(2) (1999), 195‚Äì208.
[11] D. FeÃÅral and S. PeÃÅcheÃÅ. The largest eigenvalue of rank one deformation of large Wigner matrices. Comm.
Math. Phys. 272 (2007), 185‚Äì228.
[12] Z. FuÃàredi and J. KomloÃÅs, The eigenvalues of random symmetric matrices. Combinatorica 1 (1981),
233‚Äì241.
[13] A. Guionnet and M. Maida. A Fourier view on R-transform and related asymptotics of spherical integrals.
Journal of Functional Analysis 222 (2005), 435‚Äì490.
[14] G. R. Grimmett and C. J. H. McDiarmid. On colouring random graphs. Math. proc. Cambridge Philos.
Soc. 77 (1975), 313‚Äì324.
[15] D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. Journal
of Computer and System Sciences 78.5 (2012): 1460-1480.
[16] M. Jerrum. Large cliques elude the Metropolis process. Random Struct. Algorithms 3(4) (1992), 347‚Äì360.
[17] A. Knowles and J. Yin, The isotropic semicircle law and deformation of Wigner matrices. Communica-
tions on Pure and Applied Mathematics 66(11) (2013), 1663‚Äì1749.
[18] M. Kolar, S. Balakrishnan, A. Rinaldo, and A. Singh. Minimax localization of structural information in
large noisy matrices. Neural Information Processing Systems (NIPS), (2011), 909‚Äì917.
[19] M. Talagrand. Free energy of the spherical mean field model. Probability theory and related fields 134(3)
(2006), 339‚Äì382.
[20] Z Ma and Y Wu. Computational barriers in minimax submatrix detection. arXiv:1309.5914.
[21] A. Montanari and E. Richard. A Statistical Model for Tensor PCA. Neural Information Processing
Systems (NIPS) (2014), 2897‚Äì2905.
[22] A. Onatski, M. J. Moreira, M. Hallin, et al. Asymptotic power of sphericity tests for high-dimensional
data. The Annals of Statistics 41(3) (2013), 1204‚Äì1231.
[23] W. C. Waterhouse. The absolute-value estimate for symmetric multilinear forms. Linear Algebra and its
Applications 128 (1990), 97‚Äì105.
9
