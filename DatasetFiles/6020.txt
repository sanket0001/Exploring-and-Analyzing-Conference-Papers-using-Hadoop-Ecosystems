


Paper ID = 6020
Title = Mixing Time Estimation in Reversible Markov
Chains from a Single Sample Path
Daniel Hsu
Columbia University
djhsu@cs.columbia.edu
Aryeh Kontorovich
Ben-Gurion University
karyeh@cs.bgu.ac.il
Csaba Szepesvári
University of Alberta
szepesva@cs.ualberta.ca
Abstract
This article provides the first procedure for computing a fully data-dependent in-
terval that traps the mixing time tmix of a finite reversible ergodic Markov chain at
a prescribed confidence level. The interval is computed from a single finite-length
sample path from the Markov chain, and does not require the knowledge of any
parameters of the chain. This stands in contrast to previous approaches, which ei-
ther only provide point estimates, or require a reset mechanism, or additional prior
knowledge. The interval is constructed around the relaxation time trelax, which is
strongly related to the mixing time, and the width of the interval converges to zero
roughly at a
√
n rate, where n is the length of the sample path. Upper and lower
bounds are given on the number of samples required to achieve constant-factor
multiplicative accuracy. The lower bounds indicate that, unless further restric-
tions are placed on the chain, no procedure can achieve this accuracy level before
seeing each state at least Ω(trelax) times on the average. Finally, future directions
of research are identified.
1 Introduction
This work tackles the challenge of constructing fully empirical bounds on the mixing time of
Markov chains based on a single sample path. Let (Xt)t=1,2,... be an irreducible, aperiodic time-
homogeneous Markov chain on a finite state space [d] := {1, 2, . . . , d} with transition matrix P .
Under this assumption, the chain converges to its unique stationary distribution π = (πi)
d
i=1 regard-
less of the initial state distribution q:
lim
t→∞
Prq (Xt = i) = lim
t→∞
(qP t)i = πi for each i ∈ [d].
The mixing time tmix of the Markov chain is the number of time steps required for the chain to be
within a fixed threshold of its stationary distribution:
tmix := min
{
t ∈ N : sup
q
max
A⊂[d]
|Prq (Xt ∈ A)− π(A)| ≤ 1/4
}
. (1)
Here, π(A) =
∑
i∈A πi is the probability assigned to set A by π, and the supremum is over all
possible initial distributions q. The problem studied in this work is the construction of a non-trivial
confidence interval Cn = Cn(X1, X2, . . . , Xn, δ) ⊂ [0,∞], based only on the observed sample
path (X1, X2, . . . , Xn) and δ ∈ (0, 1), that succeeds with probability 1− δ in trapping the value of
the mixing time tmix.
This problem is motivated by the numerous scientific applications and machine learning tasks in
which the quantity of interest is the mean π(f) =
∑
i πif(i) for some function f of the states
of a Markov chain. This is the setting of the celebrated Markov Chain Monte Carlo (MCMC)
paradigm [1], but the problem also arises in performance prediction involving time-correlated data,
as is common in reinforcement learning [2]. Observable bounds on mixing times are useful in the
1
design and diagnostics of these methods; they yield effective approaches to assessing the estimation
quality, even when a priori knowledge of the mixing time or correlation structure is unavailable.
Main results. We develop the first procedure for constructing non-trivial and fully empirical con-
fidence intervals for Markov mixing time. Consider a reversible ergodic Markov chain on d states
with absolute spectral gap γ⋆ and stationary distribution minorized by π⋆. As is well-known [3,
Theorems 12.3 and 12.4],
(trelax − 1) ln 2 ≤ tmix ≤ trelax ln
4
π⋆
(2)
where trelax := 1/γ⋆ is the relaxation time. Hence, it suffices to estimate γ⋆ and π⋆. Our main
results are summarized as follows.
1. In Section 3.1, we show that in some problems n = Ω((d log d)/γ⋆ + 1/π⋆) observations
are necessary for any procedure to guarantee constant multiplicative accuracy in estimating
γ⋆ (Theorems 1 and 2). Essentially, in some problems every state may need to be visited
about log(d)/γ⋆ times, on average, before an accurate estimate of the mixing time can be
provided, regardless of the actual estimation procedure used.
2. In Section 3.2, we give a point-estimator for γ⋆, and prove in Theorem 3 that it achieves
multiplicative accuracy from a single sample path of length Õ(1/(π⋆γ
3
⋆)).
1 We also pro-
vide a point-estimator for π⋆ that requires a sample path of length Õ(1/(π⋆γ⋆)). This
establishes the feasibility of estimating the mixing time in this setting. However, the valid
confidence intervals suggested by Theorem 3 depend on the unknown quantities π⋆ and
γ⋆. We also discuss the importance of reversibility, and some possible extensions to non-
reversible chains.
3. In Section 4, the construction of valid fully empirical confidence intervals for π⋆ and γ⋆
are considered. First, the difficulty of the task is explained, i.e., why the standard approach
of turning the finite time confidence intervals of Theorem 3 into a fully empirical one fails.
Combining several results from perturbation theory in a novel fashion we propose a new
procedure and prove that it avoids slow convergence (Theorem 4). We also explain how
to combine the empirical confidence intervals from Algorithm 1 with the non-empirical
bounds from Theorem 3 to produce valid empirical confidence intervals. We prove in
Theorem 5 that the width of these new intervals converge to zero asymptotically at least as
fast as those from either Theorem 3 and Theorem 4.
Related work. There is a vast statistical literature on estimation in Markov chains. For instance, it
is known that under the assumptions on (Xt)t from above, the law of large numbers guarantees that
the sample mean πn(f) :=
1
n
∑n
t=1 f(Xt) converges almost surely to π(f) [4], while the central
limit theorem tells us that as n → ∞, the distribution of the deviation √n(πn(f) − π(f)) will be
normal with mean zero and asymptotic variance limn→∞ nVar (πn(f)) [5].
Although these asymptotic results help us understand the limiting behavior of the sample mean
over a Markov chain, they say little about the finite-time non-asymptotic behavior, which is often
needed for the prudent evaluation of a method or even its algorithmic design [6–13]. To address
this need, numerous works have developed Chernoff-type bounds on Pr(|πn(f)−π(f)| > ǫ), thus
providing valuable tools for non-asymptotic probabilistic analysis [6, 14–16]. These probability
bounds are larger than corresponding bounds for independent and identically distributed (iid) data
due to the temporal dependence; intuitively, for the Markov chain to yield a fresh draw Xt′ that
behaves as if it was independent of Xt, one must wait Θ(tmix) time steps. Note that the bounds
generally depend on distribution-specific properties of the Markov chain (e.g., P , tmix, γ⋆), which
are often unknown a priori in practice. Consequently, much effort has been put towards estimating
these unknown quantities, especially in the context of MCMC diagnostics, in order to provide data-
dependent assessments of estimation accuracy [e.g., 11, 12, 17–19]. However, these approaches
generally only provide asymptotic guarantees, and hence fall short of our goal of empirical bounds
that are valid with any finite-length sample path.
Learning with dependent data is another main motivation to our work. Many results from statisti-
cal learning and empirical process theory have been extended to sufficiently fast mixing, dependent
1The Õ(·) notation suppresses logarithmic factors.
2
data [e.g., 20–26], providing learnability assurances (e.g., generalization error bounds). These re-
sults are often given in terms of mixing coefficients, which can be consistently estimated in some
cases [27]. However, the convergence rates of the estimates from [27], which are needed to derive
confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a
Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence
our main results provide a way to make them fully empirical, at least in the limited setting we study.
It is possible to eliminate many of the difficulties presented above when allowed more flexible access
to the Markov chain. For example, given a sampling oracle that generates independent transitions
from any given state (akin to a “reset” device), the mixing time becomes an efficiently testable
property in the sense studied in [28, 29]. On the other hand, when one only has a circuit-based
description of the transition probabilities of a Markov chain over an exponentially-large state space,
there are complexity-theoretic barriers for many MCMC diagnostic problems [30].
2 Preliminaries
2.1 Notations
We denote the set of positive integers by N, and the set of the first d positive integers {1, 2, . . . , d}
by [d]. The non-negative part of a real number x is [x]+ := max{0, x}, and ⌈x⌉+ := max{0, ⌈x⌉}.
We use ln(·) for natural logarithm, and log(·) for logarithm with an arbitrary constant base. Bold-
face symbols are used for vectors and matrices (e.g., v, M ), and their entries are referenced by
subindexing (e.g., vi,Mi,j). For a vector v, ‖v‖ denotes its Euclidean norm; for a matrixM , ‖M‖
denotes its spectral norm. We use Diag(v) to denote the diagonal matrix whose (i, i)-th entry is vi.
The probability simplex is denoted by ∆d−1 = {p ∈ [0, 1]d : ∑di=1 pi = 1}, and we regard vectors
in ∆d−1 as row vectors.
2.2 Setting
Let P ∈ (∆d−1)d ⊂ [0, 1]d×d be a d × d row-stochastic matrix for an ergodic (i.e., irreducible
and aperiodic) Markov chain. This implies there is a unique stationary distribution π ∈ ∆d−1 with
πi > 0 for all i ∈ [d] [3, Corollary 1.17]. We also assume that P is reversible (with respect to π):
πiPi,j = πjPj,i, i, j ∈ [d]. (3)
The minimum stationary probability is denoted by π⋆ := mini∈[d] πi.
Define the matrices
M := Diag(π)P and L := Diag(π)−1/2M Diag(π)−1/2 .
The (i, j)th entry of the matrix Mi,j contains the doublet probabilities associated with P : Mi,j =
πiPi,j is the probability of seeing state i followed by state j when the chain is started from its
stationary distribution. The matrix M is symmetric on account of the reversibility of P , and hence
it follows that L is also symmetric. (We will strongly exploit the symmetry in our results.) Further,
L = Diag(π)1/2P Diag(π)−1/2, hence L and P are similar and thus their eigenvalue systems are
identical. Ergodicity and reversibility imply that the eigenvalues of L are contained in the interval
(−1, 1], and that 1 is an eigenvalue of L with multiplicity 1 [3, Lemmas 12.1 and 12.2]. Denote and
order the eigenvalues of L as
1 = λ1 > λ2 ≥ · · · ≥ λd > −1.
Let λ⋆ := max{λ2, |λd|}, and define the (absolute) spectral gap to be γ⋆ := 1−λ⋆, which is strictly
positive on account of ergodicity.
Let (Xt)t∈N be a Markov chain whose transition probabilities are governed by P . For each t ∈ N,
let π(t) ∈ ∆d−1 denote the marginal distribution of Xt, so
π(t+1) = π(t)P , t ∈ N.
Note that the initial distribution π(1) is arbitrary, and need not be the stationary distribution π.
The goal is to estimate π⋆ and γ⋆ from the length n sample path (Xt)t∈[n], and also to construct fully
empirical confidence intervals that π⋆ and γ⋆ with high probability; in particular, the construction
3
of the intervals should not depend on any unobservable quantities, including π⋆ and γ⋆ themselves.
As mentioned in the introduction, it is well-known that the mixing time of the Markov chain tmix
(defined in Eq. 1) is bounded in terms of π⋆ and γ⋆, as shown in Eq. (2). Moreover, convergence
rates for empirical processes on Markov chain sequences are also often given in terms of mixing
coefficients that can ultimately be bounded in terms of π⋆ and γ⋆ (as we will show in the proof of
our first result). Therefore, valid confidence intervals for π⋆ and γ⋆ can be used to make these rates
fully observable.
3 Point estimation
In this section, we present lower and upper bounds on achievable rates for estimating the spectral
gap as a function of the length of the sample path n.
3.1 Lower bounds
The purpose of this section is to show lower bounds on the number of observations necessary to
achieve a fixed multiplicative (or even just additive) accuracy in estimating the spectral gap γ⋆. By
Eq. (2), the multiplicative accuracy lower bound for γ⋆ gives the same lower bound for estimating
the mixing time. Our first result holds even for two state Markov chains and shows that a sequence
length of Ω(1/π⋆) is necessary to achieve even a constant additive accuracy in estimating γ⋆.
Theorem 1. Pick any π̄ ∈ (0, 1/4). Consider any estimator γ̂⋆ that takes as input a random
sample path of length n ≤ 1/(4π̄) from a Markov chain starting from any desired initial state
distribution. There exists a two-state ergodic and reversible Markov chain distribution with spectral
gap γ⋆ ≥ 1/2 and minimum stationary probability π⋆ ≥ π̄ such that
Pr [|γ̂⋆ − γ⋆| ≥ 1/8] ≥ 3/8.
Next, considering d state chains, we show that a sequence of length Ω(d log(d)/γ⋆) is required to
estimate γ⋆ up to a constant multiplicative accuracy. Essentially, the sequence may have to visit all
d states at least log(d)/γ⋆ times each, on average. This holds even if π⋆ is within a factor of two of
the largest possible value of 1/d that it can take, i.e., when π is nearly uniform.
Theorem 2. There is an absolute constant c > 0 such that the following holds. Pick any positive
integer d ≥ 3 and any γ̄ ∈ (0, 1/2). Consider any estimator γ̂⋆ that takes as input a random sample
path of length n < cd log(d)/γ̄ from a d-state reversible Markov chain starting from any desired
initial state distribution. There is an ergodic and reversible Markov chain distribution with spectral
gap γ⋆ ∈ [γ̄, 2γ̄] and minimum stationary probability π⋆ ≥ 1/(2d) such that
Pr [|γ̂⋆ − γ⋆| ≥ γ̄/2] ≥ 1/4.
The proofs of Theorems 1 and 2 are given in Appendix A.2
3.2 A plug-in based point estimator and its accuracy
Let us now consider the problem of estimating γ⋆. For this, we construct a natural plug-in estimator.
Along the way, we also provide an estimator for the minimum stationary probability, allowing one
to use the bounds from Eq. (2) to trap the mixing time.
Define the random matrix M̂ ∈ [0, 1]d×d and random vector π̂ ∈ ∆d−1 by
M̂i,j :=
|{t ∈ [n− 1] : (Xt, Xt+1) = (i, j)}|
n− 1 , i, j ∈ [d] ,
π̂i :=
|{t ∈ [n] : Xt = i}|
n
, i ∈ [d] .
Furthermore, define
Sym(L̂) :=
1
2
(L̂+ L̂
⊤
)
2A full version of this paper, with appendices, is available on arXiv [31].
4
to be the symmetrized version of the (possibly non-symmetric) matrix
L̂ := Diag(π̂)−1/2M̂ Diag(π̂)−1/2.
Let λ̂1 ≥ λ̂2 ≥ · · · ≥ λ̂d be the eigenvalues of Sym(L̂). Our estimator of the minimum sta-
tionary probability π⋆ is π̂⋆ := mini∈[d] π̂i, and our estimator of the spectral gap γ⋆ is γ̂⋆ :=
1−max{λ̂2, |λ̂d|}.
These estimators have the following accuracy guarantees:
Theorem 3. There exists an absolute constant C > 0 such that the following holds. Assume the
estimators π̂⋆ and γ̂⋆ described above are formed from a sample path of length n from an ergodic and
reversible Markov chain. Let γ⋆ > 0 denote the spectral gap and π⋆ > 0 the minimum stationary
probability. For any δ ∈ (0, 1), with probability at least 1− δ,
|π̂⋆ − π⋆| ≤ C


√
π⋆ log
d
π⋆δ
γ⋆n
+
log dπ⋆δ
γ⋆n

 (4)
and
|γ̂⋆ − γ⋆| ≤ C


√
log dδ · log nπ⋆δ
π⋆γ⋆n
+
log 1γ⋆
γ⋆n

 . (5)
Theorem 3 implies that the sequence lengths required to estimate π⋆ and γ⋆ to within constant
multiplicative factors are, respectively, Õ
(
1
π⋆γ⋆
)
and Õ
(
1
π⋆γ3⋆
)
. By Eq. (2), the second of these is
also a bound on the required sequence length to estimate tmix.
The proof of Theorem 3 is based on analyzing the convergence of the sample averages M̂ and π̂
to their expectation, and then using perturbation bounds for eigenvalues to derive a bound on the
error of γ̂⋆. However, since these averages are formed using a single sample path from a (possibly)
non-stationary Markov chain, we cannot use standard large deviation bounds; moreover applying
Chernoff-type bounds for Markov chains to each entry of M̂ would result in a significantly worse
sequence length requirement, roughly a factor of d larger. Instead, we adapt probability tail bounds
for sums of independent random matrices [32] to our non-iid setting by directly applying a blocking
technique of [33] as described in the article of [20]. Due to ergodicity, the convergence rate can be
bounded without any dependence on the initial state distribution π(1). The proof of Theorem 3 is
given in Appendix B.
Note that because the eigenvalues of L are the same as that of the transition probability matrix P ,
we could have instead opted to estimate P , say, using simple frequency estimates obtained from
the sample path, and then computing the second largest eigenvalue of this empirical estimate P̂ .
In fact, this approach is a way to extend to non-reversible chains, as we would no longer rely on
the symmetry of M or L. The difficulty with this approach is that P lacks the structure required
by certain strong eigenvalue perturbation results. One could instead invoke the Ostrowski-Elsner
theorem [cf. Theorem 1.4 on Page 170 of 34], which bounds the matching distance between the
eigenvalues of a matrix A and its perturbation A+E by O(‖E‖1/d). Since ‖P̂ − P ‖ is expected
to be of size O(n−1/2), this approach will give a confidence interval for γ⋆ whose width shrinks
at a rate of O(n−1/(2d))—an exponential slow-down compared to the rate from Theorem 3. As
demonstrated through an example from [34], the dependence on the d-th root of the norm of the
perturbation cannot be avoided in general. Our approach based on estimating a symmetric matrix
affords us the use of perturbation results that exploit more structure.
Returning to the question of obtaining a fully empirical confidence interval for γ⋆ and π⋆, we notice
that, unfortunately, Theorem 3 falls short of being directly suitable for this, at least without further
assumptions. This is because the deviation terms themselves depend inversely both on γ⋆ and π⋆,
and hence can never rule out 0 (or an arbitrarily small positive value) as a possibility for γ⋆ or π⋆.
3
In effect, the fact that the Markov chain could be slow mixing and the long-term frequency of some
3Using Theorem 3, it is possible to trap γ⋆ in the union of two empirical confidence intervals—one around
γ̂⋆ and the other around zero, both of which shrink in width as the sequence length increases.
5
Algorithm 1 Empirical confidence intervals
Input: Sample path (X1, X2, . . . , Xn), confidence parameter δ ∈ (0, 1).
1: Compute state visit counts and smoothed transition probability estimates:
Ni := |{t ∈ [n− 1] : Xt = i}| , i ∈ [d];
Ni,j := |{t ∈ [n− 1] : (Xt, Xt+1) = (i, j)}| , P̂i,j :=
Ni,j + 1/d
Ni + 1
, (i, j) ∈ [d]2.
2: Let Â# be the group inverse of Â := I − P̂ .
3: Let π̂ ∈ ∆d−1 be the unique stationary distribution for P̂ .
4: Compute eigenvalues λ̂1≥λ̂2≥ · · · ≥λ̂d of Sym(L̂), where L̂ := Diag(π̂)1/2P̂ Diag(π̂)−1/2.
5: Spectral gap estimate:
γ̂⋆ := 1−max{λ̂2, |λ̂d|}.
6: Empirical bounds for |P̂i,j−Pi,j | for (i, j) ∈ [d]2: c := 1.01, τn,δ := inf{t ≥ 0 : 2d2(1 +
⌈logc 2nt ⌉+)e−t ≤ δ},
and B̂i,j :=


√
cτn,δ
2Ni
+
√√√√cτn,δ
2Ni
+
√
2cP̂i,j(1− P̂i,j)τn,δ
Ni
+
(5/3)τn,δ + |P̂i,j − 1/d|
Ni


2
.
7: Relative sensitivity of π:
κ̂ :=
1
2
max
{
Â#j,j −min
{
Â#i,j : i ∈ [d]
}
: j ∈ [d]
}
.
8: Empirical bounds for maxi∈[d] |π̂i − πi| and max
⋃
i∈[d]{|
√
πi/π̂i − 1|, |
√
π̂i/πi − 1|}:
b̂ := κ̂max
{
B̂i,j : (i, j) ∈ [d]2
}
, ρ̂ :=
1
2
max
⋃
i∈[d]
{
b̂
π̂i
,
b̂
[π̂i − b̂]+
}
.
9: Empirical bounds for |γ̂⋆ − γ⋆|:
ŵ := 2ρ̂+ ρ̂2 + (1 + 2ρ̂+ ρ̂2)
( ∑
(i,j)∈[d]2
π̂i
π̂j
B̂2i,j
)1/2
.
states could be small makes it difficult to be confident in the estimates provided by γ̂⋆ and π̂⋆. This
suggests that in order to obtain fully empirical confidence intervals, we need an estimator that is not
subject to such effects—we pursue this in Section 4. Theorem 3 thus primarily serves as a point
of comparison for what is achievable in terms of estimation accuracy when one does not need to
provide empirical confidence bounds.
4 Fully empirical confidence intervals
In this section, we address the shortcoming of Theorem 3 and give fully empirical confidence in-
tervals for the stationary probabilities and the spectral gap γ⋆. The main idea is to use the Markov
property to eliminate the dependence of the confidence intervals on the unknown quantities (includ-
ing π⋆ and γ⋆). Specifically, we estimate the transition probabilities from the sample path using
simple frequency estimates: as a consequence of the Markov property, for each state, the frequency
estimates converge at a rate that depends only on the number of visits to the state, and in particular
the rate (given the visit count of the state) is independent of the mixing time of the chain.
6
As discussed in Section 3, it is possible to form a confidence interval for γ⋆ based on the eigen-
values of an estimated transition probability matrix by appealing to the Ostrowski-Elsner theorem.
However, as explained earlier, this would lead to a slow O(n−1/(2d)) rate. We avoid this slow rate
by using an estimate of the symmetric matrix L, so that we can use a stronger perturbation result
(namely Weyl’s inequality, as in the proof of Theorem 3) available for symmetric matrices.
To form an estimate of L based on an estimate of the transition probabilities, one possibility is
to estimate π using a frequency-based estimate for π as was done in Section 3, and appeal to
the relation L = Diag(π)1/2P Diag(π)−1/2 to form a plug-in estimate. However, as noted in
Section 3.2, confidence intervals for the entries of π formed this way may depend on the mixing
time. Indeed, such an estimate of π does not exploit the Markov property.
We adopt a different strategy for estimating π, which leads to our construction of empirical confi-
dence intervals, detailed in Algorithm 1. We form the matrix P̂ using smoothed frequency estimates
of P (Step 1), then compute the so-called group inverse Â# of Â = I − P̂ (Step 2), followed by
finding the unique stationary distribution π̂ of P̂ (Step 3), this way decoupling the bound on the
accuracy of π̂ from the mixing time. The group inverse Â# of Â is uniquely defined; and if P̂
defines an ergodic chain (which is the case here due to the use of the smoothed estimates), Â# can
be computed at the cost of inverting an (d−1)×(d−1) matrix [35, Theorem 5.2].4 Further, once
given Â#, the unique stationary distribution π̂ of P̂ can be read out from the last row of Â# [35,
Theorem 5.3]. The group inverse is also be used to compute the sensitivity of π. Based on π̂ and P̂ ,
we construct the plug-in estimate L̂ of L, and use the eigenvalues of its symmetrization to form the
estimate γ̂⋆ of the spectral gap (Steps 4 and 5). In the remaining steps, we use perturbation analyses
to relate π̂ and π, viewing P as the perturbation of P̂ ; and also to relate γ̂⋆ and γ⋆, viewing L as a
perturbation of Sym(L̂). Both analyses give error bounds entirely in terms of observable quantities
(e.g., κ̂), tracing back to empirical error bounds for the smoothed frequency estimates of P .
The most computationally expensive step in Algorithm 1 is the computation of the group inverse
Â#, which, as noted reduces to matrix inversion. Thus, with a standard implementation of matrix
inversion, the algorithm’s time complexity is O(n+ d3), while its space complexity is O(d2).
To state our main theorem concerning Algorithm 1, we first define κ to be analogous to κ̂ from
Step 7, with Â# replaced by the group inverse A# ofA := I − P . The result is as follows.
Theorem 4. Suppose Algorithm 1 is given as input a sample path of length n from an ergodic and
reversible Markov chain and confidence parameter δ ∈ (0, 1). Let γ⋆ > 0 denote the spectral gap,
π the unique stationary distribution, and π⋆ > 0 the minimum stationary probability. Then, on an
event of probability at least 1− δ,
πi ∈ [π̂i − b̂, π̂i + b̂] for all i ∈ [d], and γ⋆ ∈ [γ̂⋆ − ŵ, γ̂⋆ + ŵ].
Moreover, b̂ and ŵ almost surely satisfy (as n → ∞)
b̂ = O
(
max
(i,j)∈[d]2
κ
√
Pi,j log log n
πin
)
, ŵ = O
(
κ
π⋆
√
log log n
π⋆n
+
√
d log log n
π⋆n
)
.5
The proof of Theorem 4 is given in Appendix C. As mentioned above, the obstacle encountered in
Theorem 3 is avoided by exploiting the Markov property. We establish fully observable upper and
lower bounds on the entries ofP that converge at a
√
n/ log log n rate using standard martingale tail
inequalities; this justifies the validity of the bounds from Step 6. Properties of the group inverse [35,
36] and eigenvalue perturbation theory [34] are used to validate the empirical bounds on πi and γ⋆
developed in the remaining steps of the algorithm.
The first part of Theorem 4 provides valid empirical confidence intervals for each πi and for γ⋆,
which are simultaneously valid at confidence level δ. The second part of Theorem 4 shows that the
4 The group inverse of a square matrix A, a special case of the Drazin inverse, is the unique matrix A#
satisfyingAA#A = A,A#AA# = A# andA#A = AA#.
5In Theorems 4 and 5, our use of big-O notation is as follows. For a random sequence (Yn)n and a (non-
random) positive sequence (εθ,n)n parameterized by θ, we say “Yn = O(εθ,n) holds almost surely as n → ∞”
if there is some universal constant C > 0 such that for all θ, lim supn→∞ Yn/εθ,n ≤ C holds almost surely.
7
width of the intervals decrease as the sequence length increases. We show in Appendix C.5 that
κ ≤ d/γ⋆, and hence b̂ = O
(
max(i,j)∈[d]2
d
γ⋆
√
Pi,j log logn
πin
)
, ŵ = O
(
d
π⋆γ⋆
√
log logn
π⋆n
)
.
It is easy to combine Theorems 3 and 4 to yield intervals whose widths shrink at least as fast
as both the non-empirical intervals from Theorem 3 and the empirical intervals from Theorem 4.
Specifically, determine lower bounds on π⋆ and γ⋆ using Algorithm 1, π⋆ ≥ mini∈[d][π̂i − b̂]+ ,
γ⋆ ≥ [γ̂⋆ − ŵ]+; then plug-in these lower bounds for π⋆ and γ⋆ in the deviation bounds in Eq. (5)
from Theorem 3. This yields a new interval centered around the estimate of γ⋆ from Theorem 3,
and it no longer depends on unknown quantities. The interval is a valid 1 − 2δ probability confi-
dence interval for γ⋆, and for sufficiently large n, the width shrinks at the rate given in Eq. (5). We
can similarly construct an empirical confidence interval for π⋆ using Eq. (4), which is valid on the
same 1− 2δ probability event.6 Finally, we can take the intersection of these new intervals with the
corresponding intervals from Algorithm 1. This is summarized in the following theorem, which we
prove in Appendix D.
Theorem 5. The following holds under the same conditions as Theorem 4. For any δ ∈ (0, 1),
the confidence intervals Û and V̂ described above for π⋆ and γ⋆, respectively, satisfy π⋆ ∈ Û and
γ⋆ ∈ V̂ with probability at least 1 − 2δ. Furthermore, the widths of these intervals almost surely
satisfy (as n → ∞) |Û | = O
(√
π⋆ log
d
π⋆δ
γ⋆n
)
, |V̂ | = O
(
min
{√
log d
δ
·log(n)
π⋆γ⋆n
, ŵ
})
, where ŵ is
the width from Algorithm 1.
5 Discussion
The construction used in Theorem 5 applies more generally: Given a confidence interval of the
form In = In(γ⋆, π⋆, δ) for some confidence level δ and a fully empirical confidence set En(δ)
for (γ⋆, π⋆) for the same level, I
′
n = En(δ) ∩ ∪(γ,π)∈En(δ)In(γ, π, δ) is a valid fully empirical 2δ-
level confidence interval whose asymptotic width matches that of In up to lower order terms under
reasonable assumptions on En and In. In particular, this suggests that future work should focus on
closing the gap between the lower and upper bounds on the accuracy of point-estimation. Another
interesting direction is to reduce the computation cost: The current cubic cost in the number of states
can be too high even when the number of states is only moderately large.
Perhaps more important, however, is to extend our results to large state space Markov chains: In
most practical applications the state space is continuous or is exponentially large in some natural pa-
rameters. As follows from our lower bounds, without further assumptions, the problem of fully data
dependent estimation of the mixing time is intractable for information theoretical reasons. Interest-
ing directions for future work thus must consider Markov chains with specific structure. Parametric
classes of Markov chains, including but not limited to Markov chains with factored transition kernels
with a few factors, are a promising candidate for such future investigations. The results presented
here are a first step in the ambitious research agenda outlined above, and we hope that they will
serve as a point of departure for further insights in the area of fully empirical estimation of Markov
chain parameters based on a single sample path.
References
[1] J. S. Liu. Monte Carlo Strategies in Scientific Computing. Springer Series in Statistics. Springer-Verlag,
2001.
[2] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (Adaptive Computation and
Machine Learning). A Bradford Book, 1998.
[3] D. Levin, Y. Peres, and E. Wilmer. Markov Chains and Mixing Times. AMS, 2008.
[4] S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Springer, 1993.
[5] C. Kipnis and S. R. S. Varadhan. Central limit theorem for additive functionals of reversible markov
processes and applications to simple exclusions. Comm. Math. Phys., 104(1):1–19, 1986.
6For the π⋆ interval, we only plug-in lower bounds on π⋆ and γ⋆ only where these quantities appear as 1/π⋆
and 1/γ⋆ in Eq. (4). It is then possible to “solve” for observable bounds on π⋆. See Appendix D for details.
8
[6] I. Kontoyiannis, L. A. Lastras-Montaño, and S. P. Meyn. Exponential bounds and stopping rules for
MCMC and general Markov chains. In VALUETOOLS, page 45, 2006.
[7] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, pages 65–72, 2006.
[8] V. Mnih, Cs. Szepesvári, and J.-Y. Audibert. Empirical Bernstein stopping. In ICML, pages 672–679,
2008.
[9] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample-variance penalization. In COLT, 2009.
[10] L. Li, M. L. Littman, T. J. Walsh, and A. L. Strehl. Knows what it knows: a framework for self-aware
learning. Machine Learning, 82(3):399–443, 2011.
[11] J. M. Flegal and G. L. Jones. Implementing MCMC: estimating with confidence. In Handbook of Markov
chain Monte Carlo, pages 175–197. Chapman & Hall/CRC, 2011.
[12] B. M. Gyori and D. Paulin. Non-asymptotic confidence intervals for MCMC in practice. arXiv:1212.2016,
2014.
[13] A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit feed-
back. In ICML, 2015.
[14] D. Gillman. A Chernoff bound for random walks on expander graphs. SIAM Journal on Computing,
27(4):1203–1220, 1998.
[15] C. A. León and F. Perron. Optimal Hoeffding bounds for discrete reversible Markov chains. Annals of
Applied Probability, pages 958–970, 2004.
[16] D. Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral methods.
Electronic Journal of Probability, 20:1–32, 2015.
[17] S. T. Garren and R. L. Smith. Estimating the second largest eigenvalue of a Markov transition matrix.
Bernoulli, 6:215–242, 2000.
[18] G. L. Jones and J. P. Hobert. Honest exploration of intractable probability distributions via markov chain
monte carlo. Statist. Sci., 16(4):312–334, 11 2001.
[19] Y. Atchadé. Markov Chain Monte Carlo confidence intervals. Bernoulli, 2015. (to appear).
[20] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of
Probability, 22(1):94–116, January 1994.
[21] R. L. Karandikar and M. Vidyasagar. Rates of uniform convergence of empirical means with mixing
processes. Statistics and Probability Letters, 58(3):297–307, 2002.
[22] D. Gamarnik. Extension of the PAC framework to finite and countable Markov chains. IEEE Transactions
on Information Theory, 49(1):338–345, 2003.
[23] M. Mohri and A. Rostamizadeh. Stability bounds for non-iid processes. In NIPS, 2008.
[24] M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In NIPS, 2009.
[25] I. Steinwart and A. Christmann. Fast learning from non-i.i.d. observations. In NIPS, 2009.
[26] I. Steinwart, D. Hush, and C. Scovel. Learning from dependent observations. Journal of Multivariate
Analysis, 100(1):175–194, 2009.
[27] D. McDonald, C. Shalizi, and M. Schervish. Estimating beta-mixing coefficients. In AISTATS, pages
516–524, 2011.
[28] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In
FOCS, pages 259–269. IEEE, 2000.
[29] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing closeness of discrete distributions.
Journal of the ACM (JACM), 60(1):4:2–4:25, 2013.
[30] N. Bhatnagar, A. Bogdanov, and E. Mossel. The computational complexity of estimating MCMC conver-
gence time. In RANDOM, pages 424–435. Springer, 2011.
[31] D. Hsu, A. Kontorovich, and C. Szepesvári. Mixing time estimation in reversible Markov chains from a
single sample path. CoRR, abs/1506.02903, 2015.
[32] J. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine
Learning, 2015.
[33] S. Bernstein. Sur l’extension du theoreme limite du calcul des probabilites aux sommes de quantites
dependantes. Mathematische Annalen, 97:1–59, 1927.
[34] G. W. Stewart and J. Sun. Matrix perturbation theory. Academic Press, Boston, 1990.
[35] C. D. Meyer Jr. The role of the group generalized inverse in the theory of finite Markov chains. SIAM
Review, 17(3):443–464, 1975.
[36] G. Cho and C. Meyer. Comparison of perturbation bounds for the stationary distribution of a Markov
chain. Linear Algebra and its Applications, 335:137–150, 2001.
9
