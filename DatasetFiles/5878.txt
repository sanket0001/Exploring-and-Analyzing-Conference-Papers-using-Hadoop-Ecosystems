


Paper ID = 5878
Title = Human Memory Search as Initial-Visit Emitting
Random Walk
Kwang-Sung Jun∗, Xiaojin Zhu†, Timothy Rogers‡
∗Wisconsin Institute for Discovery, †Department of Computer Sciences, ‡Department of Psychology
University of Wisconsin-Madison
kjun@discovery.wisc.edu, jerryzhu@cs.wisc.edu, ttrogers@wisc.edu
Zhuoran Yang
Department of Mathematical Sciences
Tsinghua University
yzr11@mails.tsinghua.edu.cn
Ming Yuan
Department of Statistics
University of Wisconsin-Madison
myuan@stat.wisc.edu
Abstract
Imagine a random walk that outputs a state only when visiting it for the first time.
The observed output is therefore a repeat-censored version of the underlying walk,
and consists of a permutation of the states or a prefix of it. We call this model
initial-visit emitting random walk (INVITE). Prior work has shown that the ran-
dom walks with such a repeat-censoring mechanism explain well human behavior
in memory search tasks, which is of great interest in both the study of human
cognition and various clinical applications. However, parameter estimation in IN-
VITE is challenging, because naive likelihood computation by marginalizing over
infinitely many hidden random walk trajectories is intractable. In this paper, we
propose the first efficient maximum likelihood estimate (MLE) for INVITE by de-
composing the censored output into a series of absorbing random walks. We also
prove theoretical properties of the MLE including identifiability and consistency.
We show that INVITE outperforms several existing methods on real-world human
response data from memory search tasks.
1 Human Memory Search as a Random Walk
A key goal for cognitive science has been to understand the mental structures and processes that
underlie human semantic memory search. Semantic fluency has provided the central paradigm for
this work: given a category label as a cue (e.g. animals, vehicles, etc.) participants must generate as
many example words as possible in 60 seconds without repetition. The task is useful because, while
exceedingly easy to administer, it yields rich information about human semantic memory. Partici-
pants do not generate responses in random order but produce “bursts” of related items, beginning
with the highly frequent and prototypical, then moving to subclusters of related items. This ordinal
structure sheds light on associative structures in memory: retrieval of a given item promotes retrieval
of a related item, and so on, so that the temporal proximity of items in generated lists reflects the
degree to which the two items are related in memory [14, 5]. The task also places demands on other
important cognitive contributors to memory search: for instance, participants must retain a men-
tal trace of previously-generated items and use it to refrain from repetition, so that the task draws
upon working memory and cognitive control in addition to semantic processes. For these reasons
the task is a central tool in all commonly-used metrics for diagnosing cognitive dysfunction (see
e.g. [6]). Performance is generally sensitive to a variety of neurological disorders [19], but different
syndromes also give rise to different patterns of impairment, making it useful for diagnosis [17]. For
these reasons the task has been widely employed both in basic science and applied health research.
Nevertheless, the representations and processes that support category fluency remain poorly under-
stood. Beyond the general observation that responses tend to be clustered by semantic relatedness,
1
it is not clear what ordinal structure in produced responses reveals about the structure of human
semantic memory, in either healthy or disordered populations. In the past few years researchers in
cognitive science have begun to fill this gap by considering how search models from other domains
of science might explain patterns of responses observed in fluency tasks [12, 13, 15]. We review
related works in Section 4.
In the current work we build on these advances by considering, not how search might operate on a
pre-specified semantic representation, but rather how the representation itself can be learned from
data (i.e., human-produced semantic fluency lists) given a specified model of the list-generation
process. Specifically, we model search as a random walk on a set of states (e.g. words) where the
transition probability indicates the strength of association in memory, and with the further constraint
that node labels are only generated when the node is first visited. Thus, repeated visits are censored
in the output. We refer to this generative process as the initial-visit emitting (INVITE) random walk.
The repeat-censoring mechanism of INVITE was first employed in Abbott et al. [1]. However, their
work did not provide a tractable method to compute the likelihood nor to estimate the transition
matrix from the fluency responses. The problem of estimating the underlying Markov chain from
the lists so produced is nontrivial because once the first two items in a list have been produced there
may exist infinitely many pathways that lead to production of the next item. For instance, consider
the produced sequence “dog” → “cat” → “goat” where the underlying graph is fully connected.
Suppose a random walk visits “dog” then “cat”. The walk can then visit “dog” and “cat” arbitrarily
many times before visiting “goat”; there exist infinitely many walks that outputs the given sequence.
How can the transition probabilities of the underlying random walk be learned?
A solution to this problem would represent a significant advance from prior works that estimate
parameters from a separate source such as a standard text corpus [13]. First, one reason for ver-
bal fluency’s enduring appeal has been that the task appears to reveal important semantic structure
that may not be discoverable by other means. It is not clear that methods for estimating semantic
structure based on another corpus do a very good job at modelling the structure of human semantic
representations generally [10], or that they would reveal the same structures that govern behavior
specifically in this widely-used fluency task. Second, the representational structures employed can
vary depending upon the fluency category. For instance, the probability of producing “chicken”
after “goat” will differ depending on whether the task involves listing “animals”, “mammals”, or
“farm animals”. Simply estimating a single structure from the same corpus will not capture these
task-based effects. Third, special populations, including neurological patients and developing chil-
dren, may generate lists from quite different underlying mental representations, which cannot be
independently estimated from a standard corpus.
In this work, we make two important contributions on the INVITE random walk. First, we propose
a tractable way to compute the INVITE likelihood. Our key insight in computing the likelihood is
to turn INVITE into a series of absorbing random walks. This formulation allows us to leverage the
fundamental matrix [7] and compute the likelihood in polynomial time. Second, we show that the
MLE of INVITE is consistent, which is non-trivial given that the convergence of the log likelihood
function is not uniform. We formally define INVITE and present the two main contributions as
well as an efficient optimization method to estimate the parameters in Section 2. In Section 3, we
apply INVITE to both toy data and real-world fluency data. On toy data our experiments empirically
confirm the consistency result. On actual human responses from verbal fluency INVITE outperforms
off-the-shelf baselines. The results suggest that INVITE may provide a useful tool for investigating
human cognitive functions.
2 The INVITE Random Walk
INVITE is a probabilistic model with the following generative story. Consider a random walk on
a set of n states S with an initial distribution π > 0 (entry-wise) and an arbitrary transition matrix
P where Pij is the probability of jumping from state i to j. A surfer starts from a random initial
state drawn from π. She outputs a state if it is the first time she visits that state. Upon arriving
at an already visited state, however, she does not output the state. The random walk continues
indefinitely. Therefore, the output consists of states in the order of their first-visit; the underlying
entire walk trajectory is hidden. We further assume that the time step of each output is unobserved.
For example, consider the random walk over four states in Figure 1(a). If the underlying random
walk takes the trajectory (1, 2, 1, 3, 1, 2, 1, 4, 1, . . .), the observation is (1, 2, 3, 4).
2
1
2
3 4
1 13
1
3
1
31
1
2 1
3 4
1 1
1
1
T
W1
T
W1
2 1
3 4
1
1
1
1 0 0.5 1
−1.52
−1.5
−1.48
−1.46
−1.44
λ
Lo
g 
lik
el
ih
oo
d
(a) (b) (c) (d)
Figure 1: (a-c) Example Markov chains (d) Example nonconvexity of the INVITE log likelihood
We say that the observation produced by INVITE is a censored list since non-initial visits are cen-
sored. It is easy to see that a censored list is a permutation of the n states or a prefix thereof (more
on this later). We denote a censored list by a = (a1, a2, . . . , aM ) where M ≤ n. A censored list
is not Markovian since the probability of a transition in censored list depends on the whole history
rather than just the current state. It is worth noting that INVITE is distinct from Broder’s algorithm
for generating random spanning trees [4], or the self-avoiding random walk [9], or cascade models
of infection. We discuss the technical difference to related works in Section 4.
We characterize the type of output INVITE is capable of producing, given that the underlying un-
censored random walk continues indefinitely. A state s is said to be transient if a random walk
starting from s has nonzero probability of not returning to itself in finite time and recurrent if such
probability is zero. A set of states A is closed if a walk cannot exit A; i.e., if i ∈ A and j 6∈ A,
then a random walk from i cannot reach j. A set of states B is irreducible if there exists a path
between every pair of states in B; i.e., if i, j ∈ B, then a random walk from i can reach j. Define
[M ] = {1, 2, . . . ,M}. We use a1:M as a shorthand for a1, . . . , aM . Theorem 1 states that a finite
state Markov chain can be uniquely decomposed into disjoint sets, and Theorem 2 states what a
censored list should look like. All proofs are in the supplementary material.
Theorem 1. [8] If the state space S is finite, then S can be written as a disjoint union T ∪W1 ∪
. . . ∪WK , where T is a set of transient states that is possibly empty and each Wk, k ∈ [K], is a
nonempty closed irreducible set of recurrent states.
Theorem 2. Consider a Markov chain P with the decomposition S = T ∪W1 ∪ . . . ∪WK as in
Theorem 1. A censored list a = (a1:M ) generated by INVITE on P has zero or more transient states,
followed by all states in one and only one closed irreducible set. That is, ∃` ∈ [M ] s.t. {a1:`−1} ⊆ T
and {a`:M} = Wk for some k ∈ [K].
As an example, when the graph is fully connected INVITE is capable of producing all n! permuta-
tions of the n states as the censored lists. As another example, in Figure 1 (b) and (c), both chains
have two transient states T = {1, 2} and two recurrent states W1 = {3, 4}. (b) has no path that
visits both 1 and 2, and thus every censored list must be a prefix of a permutation. However, (c) has
a path that visits both 1 and 2, thus can generate (1,2,3,4), a full permutation.
In general, each INVITE run generates a permutation of n states, or a prefix of a permutation. Let
Sym(n) be the symmetric group on [n]. Then, the data space D of censored lists is D ≡ {(a1:k) |
a ∈ Sym(n), k ∈ [n]}.
2.1 Computing the INVITE likelihood
Learning and inference under the INVITE model is challenging due to its likelihood function. A
naive method to compute the probability of a censored list a given π and P is to sum over all un-
censored random walk trajectories x which produces a: P(a;π,P) =
∑
x produces a P(x;π,P).
This naive computation is intractable since the summation can be over an infinite number of tra-
jectories x’s that might have produced the censored list a. For example, consider the censored list
a = (1, 2, 3, 4) generated from Figure 1(a). There are infinite uncensored trajectories to produce a
by visiting states 1 and 2 arbitrarily many times before visiting state 3, and later state 4.
The likelihood of π and P on a censored list a is
P(a;π,P) =
{
πa1
∏M−1
k=1 P(ak+1 | a1:k; P) if a cannot be extended
0 otherwise.
(1)
3
Note we assign zero probability to a censored list that is not completed yet, since the underly-
ing random walk must run forever. We say a censored list a is valid (invalid) under π and P if
P(a;π,P) > 0 (= 0).
We first review the fundamental matrix in the absorbing random walk. A state that transits to itself
with probability 1 is called an absorbing state. Given a Markov chain P with absorbing states, we
can rearrange the states into P′ =
(
Q R
0 I
)
, where Q is the transition between the nonabsorbing
states, R is the transition from the nonabsorbing states to absorbing states, and the rest trivially
represent the absorbing states. Theorem 3 presents the fundamental matrix, the essential tool for the
tractable computation of the INVITE likelihood.
Theorem 3. [7] The fundamental matrix of the Markov chain P′ is N = (I − Q)−1. Nij is
the expected number of times that a chain visits state j before absorption when starting from i.
Furthermore, define B = (I−Q)−1R. Then, Bik is the probability of a chain starting from i being
absorbed by k. In other words, Bi· is the absorption distribution of a chain starting from i.
As a tractable way to compute the likelihood, we propose a novel formulation that turns an INVITE
random walk into a series of absorbing random walks. Although INVITE itself is not an absorbing
random walk, each segment that produces the next item in the censored list can be modeled as one.
That is, for each k = 1 . . .M − 1 consider the segment of the uncensored random walk starting
from the previous output ak until the next output ak+1. For this segment, we construct an absorbing
random walk by keeping a1:k nonabsorbing and turning the rest into the absorbing states. A random
walk starting from ak is eventually absorbed by a state in S \ {a1:k}. The probability of being
absorbed by ak+1 is exactly the probability of outputting ak+1 after outputting a1:k in INVITE.
Formally, we construct an absorbing random walk P(k):
P(k) =
(
Q(k) R(k)
0 I
)
, (2)
where the states are ordered as a1:M . Corollary 1 summarizes our computation of the INVITE
likelihood.
Corollary 1. The k-th step INVITE likelihood for k ∈ [M − 1] is
P(ak+1 | a1:k,P) =
{
[(I−Q(k))−1R(k)]k1 if (I−Q(k))−1 exists
0 otherwise
(3)
Suppose we observe m independent realizations of INVITE: Dm ={(
a
(1)
1 , ..., a
(1)
M1
)
, ...,
(
a
(m)
1 , ..., a
(m)
Mm
)}
, where Mi is the length of the i-th censored list.
Then, the INVITE log likelihood is `(π,P;Dm) =
∑m
i=1 logP(a(i);π,P).
2.2 Consistency of the MLE
Identifiability is an essential property for a model to be consistent. Theorem 4 shows that allowing
self-transitions in P cause INVITE to be unidentifiable. Then, Theorem 5 presents a remedy. The
proof for both theorems are presented in our supplementary material. Let diag(q) be a diagonal
matrix whose i-th diagonal entry is qi.
Theorem 4. Let P be an n × n transition matrix without any self-transition (Pii = 0,∀i), and
q ∈ [0, 1)n. Define P′ = diag(q) + (I− diag(q))P, a scaled transition matrix with self-transition
probabilities q. Then, P(a;π,P) = P(a;π,P′), for every censored list a.
For example, consider a censored list a = (1, j) where j 6= 1. Using the fundamental matrix,
P(a2 | a1; P) = (1− P11)−1P1j = (
∑
j′ 6=1 P1j′)
−1P1j = (
∑
j′ 6=1 cP1j′)
−1cP1j ,∀c. This implies
that multiplying a constant c to P1j for all j 6= 1 and renormalizing the first row P1· to sum to 1
does not change the likelihood.
Theorem 5. Assume the initial distribution π > 0 elementwise. In the space of transition matrices
P without self-transitions, INVITE is identifiable.
Let ∆n−1 = {p ∈ Rn | pi ≥ 0,∀i,
∑
i pi = 1} be the probability simplex. For brevity, we pack
the parameters of INVITE into one vector θ as follows: θ ∈ Θ = {(π>,P1·, . . . ,Pn·)> | π,Pi· ∈
∆n−1, Pii = 0,∀i}. Let θ∗ = (π∗>,P∗1·, . . . ,P∗n·)> ∈ Θ be the true model. Given a set of m
censored lists Dm generated from θ∗, the average log likelihood function and its pointwise limit are
4
Q̂m(θ) =
1
m
m∑
i=1
logP(a(i));θ) and Q∗(θ) =
∑
a∈D
P(a;θ∗) logP(a;θ). (4)
For brevity, we assume that the true model θ∗ is strongly connected; the analysis can be easily
extended to remove it. Under the Assumption A1, Theorem 6 states the consistency result.
Assumption A1. Let θ∗ = (π∗>,P∗1·, . . . ,P∗n·)> ∈ Θ be the true model. π∗ has no zero entries.
Furthermore, P∗ is strongly connected.
Theorem 6. Assume A1. The MLE of INVITE θ̂m ≡ maxθ∈Θ Q̂m(θ) is consistent.
We provide a sketch here. The proof relies on Lemma 6 and Lemma 2 that are presented in our
supplementary material. Since Θ is compact, the sequence {θ̂m} has a convergent subsequence
{θ̂mj}. Let θ′ = limj→∞ θ̂mj . Since Q̂mj (θ∗) ≤ Q̂mj (θ̂mj ),
Q∗(θ∗) = lim
j→∞
Q̂mj (θ∗) ≤ lim
j→∞
Q̂mj (θ̂mj ) = Q∗(θ′),
where the last equality is due to Lemma 6. By Lemma 2, θ∗ is the unique maximizer of Q∗,
which implies θ′ = θ∗. Note that the subsequence was chosen arbitrarily. Since every convergent
subsequence converges to θ∗, θ̂m converges to θ∗.
2.3 Parameter Estimation via Regularized Maximum Likelihood
We present a regularized MLE (RegMLE) of INVITE. We first extend the censored lists that we
consider. Now we allow the underlying walk to terminate after finite steps because in real-world
applications the observed censored lists are often truncated. That is, the underlying random walk
can be stopped before exhausting every state the walk could visit. For example, in verbal fluency,
participants have limited time to produce a list. Consequently, we use the prefix likelihood
L(a;π,P) = πa1
M−1∏
k=1
P(ak+1 | a1:k; P). (5)
We find the RegMLE by maximizing the prefix log likelihood plus a regularization term on π,P.
Note that, π and P can be separately optimized. For π, we place a Dirichlet prior and find the
maximum a posteriori (MAP) estimator π̂ by π̂j ∝
∑m
i=1 1a(i)1 =j
+ Cπ,∀j.
Directly computing the RegMLE of P requires solving a constrained optimization problem, because
the transition matrix P must be row stochastic. We re-parametrize P which leads to a more conve-
nient unconstrained optimization problem. Let β ∈ Rn×n. We exponentiate β and row-normalize it
to derive P: Pij = eβij/
∑n
j′=1 e
βij′ ,∀i, j. We fix the diagonal entries of β to−∞ to disallow self-
transitions. We place squared `2 norm regularizer on β to prevent overfitting. The unconstrained
optimization problem is:
min
β
−
∑m
i=1
∑Mi−1
k=1 logP(a
(i)
k+1 | a
(i)
1:k;β) +
1
2Cβ
∑
i 6=j β
2
ij , (6)
where Cβ > 0 is a regularization parameter. We provide the derivative of the prefix log likelihood
w.r.t. β in our supplementary material. We point out that the objective function of (6) is not convex
in β in general. Let n = 5 and suppose we observe two censored lists (5, 4, 3, 1, 2) and (3, 4, 5, 1, 2).
We found with random starts two different local optima β(1) and β(2) of (6). We plot the prefix log
likelihood of (1 − λ)β(1) + λβ(2), where λ ∈ [0, 1] in Figure 1(d). Nonconvexity of this 1D slice
implies nonconvexity of the prefix log likelihood surface in general.
Efficient Optimization using Averaged Stochastic Gradient Descent Given a censored list a of
lengthM , computing the derivative of P(ak+1 | a1:k) w.r.t. β takesO(k3) time for matrix inversion.
There are n2 entries in β, so the time complexity per item is O(k3 +n2). This computation needs to
be done for k = 1, ..., (M − 1) in a list and for m censored lists, which makes the overall time com-
plexityO(mM(M3+n2)). In the worst case,M is as large as n, which makes itO(mn4). Even the
state-of-the-art batch optimization method such as LBFGS takes a very long time to find the solution
for a moderate problem size such as n ≈ 500. For a faster computation of the RegMLE (6), we turn
to averaged stochastic gradient descent (ASGD) [20, 18]. ASGD processes the lists sequentially by
updating the parameters after every list. The per-round objective function for β on the i-th list is
f(a(i);β) ≡ −
Mi−1∑
k=1
logP(a(i)k+1 | a
(i)
1:k;β) +
Cβ
2m
∑
i 6=j
β2ij .
5
The number of lists (m)
102
er
ro
r(
P̂
)
0
1
2
3
Ring, n=25
INVITE
RW
FE
The number of lists (m)
102
er
ro
r(
P̂
)
1
2
3
4
5
Star, n=25
INVITE
RW
FE
The number of lists (m)
102
er
ro
r(
P̂
)
0
1
2
3
Grid, n=25
INVITE
RW
FE
(a) (b) (c)
Figure 2: Toy experiment results where the error is measured with the Frobenius norm.
We randomly initialize β0. At round t, we update the solution βt with βt ← βt−1 − ηt∇f(a(i);β)
and the average estimate βt with βt ← t−1t βt−1 +
1
tβt. Let ηt = γ0(1 + γ0at)
−c. We use
a = Cβ/m and c = 3/4 following [3] and pick γ0 by running the algorithm on a small subsample
of the train set. We run ASGD for a fixed number of epochs and take the final βt as the solution.
3 Experiments
We compare INVITE against two popular estimators of P: naive random walk (RW) and First-Edge
(FE). RW is the regularized MLE of the naive random walk, pretending the censored lists are the
underlying uncensored walk trajectory: P̂ (RW )rc ∝
(∑m
i=1
∑Mi−1
j=1 1(a(i)j =r)∧(a
(i)
j+1=c)
)
+ CRW .
Though simple and popular, RW is a biased estimator due to the model mismatch. FE was proposed
in [2] for graph structure recovery in cascade model. FE uses only the first two items in each
censored list: P̂ (FE)rc ∝
(∑m
i=1 1(a(i)1 =r)∧(a
(i)
2 =c)
)
+CFE . Because the first transition in a censored
list is always the same as the first transition in its underlying trajectory, FE is a consistent estimator
of P (assuming π has no zero entries). In fact, FE is equivalent to the RegMLE of the length
two prefix likelihood of the INVITE model. However, we expect FE to waste information since it
discards the rest of the censored lists. Furthermore, FE cannot estimate the transition probabilities
from an item that does not appear as the first item in the lists, which is common in real-world data.
3.1 Toy Experiments
Here we compare the three estimators INVITE, RW, and FE on toy datasets, where the observations
are indeed generated by an initial-visit emitting random walk. We construct three undirected, un-
weighted graphs of n = 25 nodes each: (i) Ring, a ring graph, (ii) Star, n−1 nodes each connected
to a “hub” node, and (iii) Grid, a 2-dimensional
√
n×
√
n lattice.
The initial distribution π∗ is uniform, and the transition matrix P∗ at each node has an
equal transition probability to its neighbors. For each graph, we generate datasets with m ∈
{10, 20, 40, 80, 160, 320, 640} censored lists. Each censored list has length n. We note that, in
the star graph a censored list contains many apparent transitions between leaf nodes, although such
transitions are not allowed in its underlying uncensored random walk. This will mislead RW. This
effect is less severe in the grid graph and the ring graph.
For each estimator, we perform 5-fold cross validation (CV) for finding the best smoothing param-
eters Cβ, CRW , CFE on the grid 10−2, 10−1.5, . . . , 101, respectively, with which we compute each
estimator. Then, we evaluate the three estimators using the Frobenius norm between P̂ and the true
transition matrix P∗: error(P̂) =
√∑
i,j(P̂ij − P ∗ij)2. Note the error must approach 0 as m in-
creases for consistent estimators. We repeat the same experiment 20 times where each time we draw
a new set of censored lists.
Figure 2 shows how error(P̂) changes as the number of censored lists m increases. The error bars
are 95% confidence bounds. We make three observations: (1) INVITE tends towards 0 error. This
is expected given the consistency of INVITE in Theorem 6. (2) RW is biased. In all three plots, RW
tends towards some positive number, unlike INVITE and FE. This is because RW has the wrong
model on the censored lists. (3) INVITE outperforms FE. On the ring and grid graphs INVITE
dominates FE for every training set size. On the star graph FE is better than INVITE with a small
m, but INVITE eventually achieves lower error. This reflects the fact that, although FE is unbiased,
it discards most of the censored lists and therefore has higher variance compared to INVITE.
6
Animal Food
n 274 452
m 4710 4622
Length
Min. 2 1
Max. 36 47
Mean 18.72 20.73
Median 19 21
Table 1: Statistics of the verbal fluency data.
Model Test set mean neg. loglik.
Animal
INVITE 60.18 (±1.75)
RW 69.16 (±2.00)
FE 72.12 (±2.17)
Food
INVITE 83.62 (±2.32)
RW 94.54 (±2.75)
FE 100.27 (±2.96)
Table 2: Verbal fluency test set log likelihood.
3.2 Verbal Fluency
We now turn to the real-world fluency data where we compare INVITE with the baseline models.
Since we do not have the ground truth parameter π and P, we compare test set log likelihood
of various models. Confirming the empirical performance of INVITE sheds light on using it for
practical applications such as the dignosis and classification of the brain-damaged patient.
Data The data used to assess human memory search consists of two verbal fluency datasets from
the Wisconsin Longitudinal Survey (WLS). The WLS is a longitudinal assessment of many sociode-
mographic and health factors that has been administered to a large cohort of Wisconsin residents
every five years since the 1950s. Verbal fluency for two semantic categories, animals and foods,
was administered in the last two testing rounds (2005 and 2010), yielding a total of 4714 lists for
animals and 4624 lists for foods collected from a total of 5674 participants ranging in age from their
early-60’s to mid-70’s. The raw lists included in the WLS were preprocessed by expanding abbrevi-
ations (“lab”→ “labrador”), removing inflections (“cats”→ “cat”), correcting spelling errors, and
removing response errors like unintelligible items. Though instructed to not repeat, some human
participants did occasionally produce repeated words. We removed the repetitions from the data,
which consist of 4% of the word token responses. Finally, the data exhibits a Zipfian behavior with
many idiosyncratic, low count words. We removed words appearing in less than 10 lists. In total,
the process resulted in removing 5% of the total number of word token responses. The statistics of
the data after preprocessing is summarized in Table 1.
Procedure We randomly subsample 10% of the lists as the test set, and use the rest as the training
set. We perform 5-fold CV on the training set for each estimator to find the best smoothing parameter
Cβ, CRW , CFE ∈ {101, 10.5, 100, 10−.5, 10−1, 10−1.5, 10−2} respectively, where the validation
measure is the prefix log likelihood for INVITE and the standard random walk likelihood for RW.
For the validation measure of FE we use the INVITE prefix log likelihood since FE is equivalent to
the length two prefix likelihood of INVITE. Then, we train the final estimator on the whole training
set using the fitted regularization parameter.
Result The experiment result is summarized in Table 2. For each estimator, we measure the aver-
age per-list negative prefix log likelihood on the test set for INVITE and FE, and the standard random
walk per-list negative log likelihood for RW. The number in the parenthesis is the 95% confidence
interval. Boldfaced numbers mean that the corresponding estimator is the best and the difference
from the others is statistically significant under a two-tailed paired t-test at 95% significance level.
In both animal and food verbal fluency tasks, the result indicates that human-generated fluency lists
are better explained by INVITE than by either RW or FE. Furthermore, RW outperforms FE. We
believe that FE performs poorly despite being consistent because the number of lists is too small
(compared to the number of states) for FE to reach a good estimate.
4 Related Work
Though behavior in semantic fluency tasks has been studied for many years, few computationally ex-
plicit models of the task have been advanced. Influential models in the psychological literature, such
as the widely-known ”clustering and switching” model of Troyer et al. [21], have been articulated
only verbally. Efforts to estimate the structure of semantic memory from fluency lists have mainly
focused on decomposing the structure apparent in distance matrices that reflect the mean inter-item
ordinal distances across many fluency lists [5]—but without an account of the processes that gener-
ate list structure it is not clear how the results of such studies are best interpreted. More recently,
researchers in cognitive science have begun to focus on explicit model of the processes by which
fluency lists are generated. In these works, the structure of semantic memory is first modelled either
as a graph or as a continuous multidimensional space estimated from word co-occurrence statistics
in large corpora of natural language. Researchers then assess whether structure in fluency data can
be understood as resulting from a particular search process operating over the specified semantic
7
structure. Models explored in this vein include simple random walk over a semantic network, with
repeated nodes omitted from the sequence produced [12], the PageRank algorithm employed for net-
work search by Google [13], and foraging algorithms designed to explain the behavior of animals
searching for food [15]. Each example reports aspects of human behavior that are well-explained by
the respective search process, given accompanying assumptions about the nature of the underlying
semantic structure. However, these works do not learn their model directly from the fluency lists,
which is the key difference from our study.
Broder’s algorithm Generate [4] for generating random spanning tree is similar to INVITE’s gen-
erative process. Given an undirected graph, the algorithm runs a random walk and outputs each
transition to an unvisited node. Upon transiting to an already visited node, however, it does not
output the transition. The random walk stops after visiting every node in the graph. In the end, we
observe an ordered list of transitions. For example, in Figure 1(a) if the random walk trajectory is
(2,1,2,1,3,1,4), then the output is (2→1, 1→3, 1→4). Note that if we take the starting node of the
first transition and the arriving nodes of each transition, then the output list reduces to a censored list
generated from INVITE with the same underlying random walk. Despite the similarity, to the best
of our knowledge, the censored list derived from the output of the algorithm Generate has not been
studied, and there has been no parameter estimation task discussed in prior works.
Self-avoiding random walk, or non-self-intersecting random walk, performs random walk while
avoiding already visited node [9]. For example, in Figure 1(a), if a self-avoiding random walk starts
from state 2 then visits 1, then it can only visit states 3 or 4 since 2 is already visited. In not visiting
the same node twice, self-avoiding walk is similar to INVITE. However, a key difference is that
self-avoiding walk cannot produce a transition i→ j if Pij = 0. In contrast, INVITE can appear to
have such “transitions” in the censored list. Such behavior is a core property that allows INVITE to
switch clusters in modeling human memory search.
INVITE resembles cascade models in many aspects [16, 11]. In a cascade model, the information
or disease spreads out from a seed node to the whole graph by infections that occur from an infected
node to its neighbors. [11] formulates a graph learning problem where an observation is a list, or
so-called trace, that contains infected nodes along with their infection time. Although not discussed
in the present paper, it is trivial for INVITE to produce time stamps for each item in its censored
list, too. However, there is a fundamental difference in how the infection occurs. A cascade model
typically allows multiple infected nodes to infect their neighbors in parallel, so that infection can
happen simultaneously in many parts of the graph. On the other hand, INVITE contains a single
surfer that is responsible for all the infection via a random walk. Therefore, infection in INVITE is
necessarily sequential. This results in INVITE exhibiting clustering behaviors in the censored lists,
which is well-known in human memory search tasks [21].
5 Discussion
There are numerous directions to extend INVITE. First, more theoretical investigation is needed. For
example, although we know the MLE of INVITE is consistent, the convergence rate is unknown.
Second, one can improve the INVITE estimate when data is sparse by assuming certain cluster
structures in the transition matrix P, thereby reducing the degrees of freedom. For instance, it is
known that verbal fluency tends to exhibit “runs” of semantically related words. One can assume
a stochastic block model P with parameter sharing at the block level, where the blocks represent
semantic clusters of words. One then estimates the block structure and the shared parameters at the
same time. Third, INVITE can be extended to allow repetitions in a list. The basic idea is as follows.
In the k-th segment we previously used an absorbing random walk to compute P(ak+1 | a1:k), where
a1:k were the nonabsorbing states. For each nonabsorbing state ai, add a “dongle twin” absorbing
state a′i attached only to ai. Allow a small transition probability from ai to a
′
i. If the walk is absorbed
by a′i, we output ai in the censored list, which becomes a repeated item in the censored list. Note
that the likelihood computation in this augmented model is still polynomial. Such a model with
“reluctant repetitions” will be an interesting interpolation between “no repetitions” and “repetitions
as in a standard random walk.”
Acknowledgments
The authors are thankful to the anonymous reviewers for their comments. This work is supported in
part by NSF grants IIS-0953219 and DGE-1545481, NIH Big Data to Knowledge 1U54AI117924-
01, NSF Grant DMS-1265202, and NIH Grant 1U54AI117924-01.
8
References
[1] J. T. Abbott, J. L. Austerweil, and T. L. Griffiths, “Human memory search as a random walk in
a semantic network,” in NIPS, 2012, pp. 3050–3058.
[2] B. D. Abrahao, F. Chierichetti, R. Kleinberg, and A. Panconesi, “Trace complexity of network
inference.” CoRR, vol. abs/1308.2954, 2013.
[3] L. Bottou, “Stochastic gradient tricks,” in Neural Networks, Tricks of the Trade, Reloaded, ser.
Lecture Notes in Computer Science (LNCS 7700), G. Montavon, G. B. Orr, and K.-R. Müller,
Eds. Springer, 2012, pp. 430–445.
[4] A. Z. Broder, “Generating random spanning trees,” in FOCS. IEEE Computer Society, 1989,
pp. 442–447.
[5] A. S. Chan, N. Butters, J. S. Paulsen, D. P. Salmon, M. R. Swenson, and L. T. Maloney, “An
assessment of the semantic network in patients with alzheimer’s disease.” Journal of Cognitive
Neuroscience, vol. 5, no. 2, pp. 254–261, 1993.
[6] J. R. Cockrell and M. F. Folstein, “Mini-mental state examination.” Principles and practice of
geriatric psychiatry, pp. 140–141, 2002.
[7] P. G. Doyle and J. L. Snell, Random Walks and Electric Networks. Washington, DC: Mathe-
matical Association of America, 1984.
[8] R. Durrett, Essentials of stochastic processes, 2nd ed., ser. Springer texts in statistics. New
York: Springer, 2012.
[9] P. Flory, Principles of polymer chemistry. Cornell University Press, 1953.
[10] A. M. Glenberg and S. Mehta, “Optimal foraging in semantic memory,” Italian Journal of
Linguistics, 2009.
[11] M. Gomez Rodriguez, J. Leskovec, and A. Krause, “Inferring networks of diffusion and influ-
ence,” Max-Planck-Gesellschaft. New York, NY, USA: ACM Press, July 2010, pp. 1019–
1028.
[12] J. Goi, G. Arrondo, J. Sepulcre, I. Martincorena, N. V. de Mendizbal, B. Corominas-Murtra,
B. Bejarano, S. Ardanza-Trevijano, H. Peraita, D. P. Wall, and P. Villoslada, “The semantic or-
ganization of the animal category: evidence from semantic verbal fluency and network theory.”
Cognitive Processing, vol. 12, no. 2, pp. 183–196, 2011.
[13] T. L. Griffiths, M. Steyvers, and A. Firl, “Google and the mind: Predicting fluency with pager-
ank,” Psychological Science, vol. 18, no. 12, pp. 1069–1076, 2007.
[14] N. M. Henley, “A psychological study of the semantics of animal terms.” Journal of Verbal
Learning and Verbal Behavior, vol. 8, no. 2, pp. 176–184, Apr. 1969.
[15] T. T. Hills, P. M. Todd, and M. N. Jones, “Optimal foraging in semantic memory,” Psycholog-
ical Review, pp. 431–440, 2012.
[16] D. Kempe, J. Kleinberg, and E. Tardos, “Maximizing the spread of influence through a social
network,” in Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, ser. KDD ’03. New York, NY, USA: ACM, 2003, pp. 137–146.
[17] F. Pasquier, F. Lebert, L. Grymonprez, and H. Petit, “Verbal fluency in dementia of frontal lobe
type and dementia of Alzheimer type.” Journal of Neurology, vol. 58, no. 1, pp. 81–84, 1995.
[18] B. T. Polyak and A. B. Juditsky, “Acceleration of stochastic approximation by averaging,”
SIAM J. Control Optim., vol. 30, no. 4, pp. 838–855, July 1992.
[19] T. T. Rogers, A. Ivanoiu, K. Patterson, and J. R. Hodges, “Semantic memory in Alzheimer’s
disease and the frontotemporal dementias: a longitudinal study of 236 patients.” Neuropsy-
chology, vol. 20, no. 3, pp. 319–335, 2006.
[20] D. Ruppert, “Efficient estimations from a slowly convergent robbins-monro process,” Cornell
University Operations Research and Industrial Engineering, Tech. Rep., 1988.
[21] A. Troyer, M. Moscovitch, G. Winocur, M. Alexander, and D. Stuss, “Clustering and switching
on verbal fluency: The effects of focal fronal- and temporal-lobe lesions,” Neuropsychologia,
vol. 36, no. 6, 1998.
9
