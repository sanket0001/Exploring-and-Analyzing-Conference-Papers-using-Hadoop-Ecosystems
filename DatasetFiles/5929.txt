


Paper ID = 5929
Title = Fast and Memory Optimal Low-Rank Matrix
Approximation
Se-Young Yun
MSR, Cambridge
seyoung.yun@inria.fr
Marc Lelarge ∗
Inria & ENS
marc.lelarge@ens.fr
Alexandre Proutiere †
KTH, EE School / ACL
alepro@kth.se
Abstract
In this paper, we revisit the problem of constructing a near-optimal rank k approx-
imation of a matrix M ∈ [0, 1]m×n under the streaming data model where the
columns of M are revealed sequentially. We present SLA (Streaming Low-rank
Approximation), an algorithm that is asymptotically accurate, when ksk+1(M) =
o(
√
mn) where sk+1(M) is the (k + 1)-th largest singular value of M . This
means that its average mean-square error converges to 0 as m and n grow large
(i.e., ‖M̂ (k)−M (k)‖2F = o(mn) with high probability, where M̂ (k) andM (k) de-
note the output of SLA and the optimal rank k approximation ofM , respectively).
Our algorithm makes one pass on the data if the columns of M are revealed in
a random order, and two passes if the columns of M arrive in an arbitrary order.
To reduce its memory footprint and complexity, SLA uses random sparsification,
and samples each entry of M with a small probability δ. In turn, SLA is memory
optimal as its required memory space scales as k(m+n), the dimension of its out-
put. Furthermore, SLA is computationally efficient as it runs in O(δkmn) time (a
constant number of operations is made for each observed entry of M ), which can
be as small as O(k log(m)4n) for an appropriate choice of δ and if n ≥ m.
1 Introduction
We investigate the problem of constructing, in a memory and computationally efficient man-
ner, an accurate estimate of the optimal rank k approximation M (k) of a large (m × n) matrix
M ∈ [0, 1]m×n. This problem is fundamental in machine learning, and has naturally found nu-
merous applications in computer science. The optimal rank k approximation M (k) minimizes, over
all rank k matrices Z, the Frobenius norm ‖M − Z‖F (and any norm that is invariant under rota-
tion) and can be computed by Singular Value Decomposition (SVD) of M in O(nm2) time (if we
assume that m ≤ n). For massive matrices M (i.e., when m and n are very large), this becomes
unacceptably slow. In addition, storing and manipulating M in memory may become difficult. In
this paper, we design a memory and computationally efficient algorithm, referred to as Streaming
Low-rank Approximation (SLA), that computes a near-optimal rank k approximation M̂ (k). Under
mild assumptions on M , the SLA algorithm is asymptotically accurate in the sense that as m and n
grow large, its average mean-square error converges to 0, i.e., ‖M̂ (k)−M (k)‖2F = o(mn) with high
probability (we interpret M (k) as the signal that we aim to recover form a noisy observation M ).
To reduce its memory footprint and running time, the proposed algorithm combines random sparsi-
fication and the idea of the streaming data model. More precisely, each entry of M is revealed to
the algorithm with probability δ, called the sampling rate. Moreover, SLA observes and treats the
∗Work performed as part of MSR-INRIA joint research centre. M.L. acknowledges the support of the
French Agence Nationale de la Recherche (ANR) under reference ANR-11-JS02-005-01 (GAP project).
†A. Proutiere’s research is supported by the ERC FSA grant, and the SSF ICT-Psi project.
1
columns of M one after the other in a sequential manner. The sequence of observed columns may
be chosen uniformly at random in which case the algorithm requires one pass on M only, or can be
arbitrary in which case the algorithm needs two passes. SLA first stores ` = 1/(δ log(m)) randomly
selected columns, and extracts via spectral decomposition an estimator of parts of the k top right
singular vectors of M . It then completes the estimator of these vectors by receiving and treating the
remain columns sequentially. SLA finally builds, from the estimated top k right singular vectors, the
linear projection onto the subspace generated by these vectors, and deduces an estimator of M (k).
The analysis of the performance of SLA is presented in Theorems 7, and 8. In summary:
when m ≤ n, log
4(m)
m ≤ δ ≤ m
−8/9, with probability 1− kδ, the output M̂ (k) of SLA satisfies:
‖M (k) − M̂ (k)‖2F
mn
= O
(
k2
(
s2k+1(M)
mn
+
log(m)√
δm
))
, (1)
where sk+1(M) is the (k + 1)-th singular value of M . SLA requires O(kn) memory space, and if
δ ≥ log
4(m)
m and k ≤ log
6(m), its time is O(δkmn). To ensure the asymptotic accuracy of SLA, the
upper-bound in (1) needs to converge to 0 which is true as soon as ksk+1(M) = o(
√
mn). In the
case where M is seen as a noisy version of M (k), this condition quantifies the maximum amount of
noise allowed for our algorithm to be asymptotically accurate.
SLA is memory optimal, since any rank k approximation algorithm needs to at least store its output,
i.e., k right and left singular vectors, and hence needs at leastO(kn) memory space. Further observe
that among the class of algorithms sampling each entry ofM at a given rate δ, SLA is computational
optimal, since it runs in O(δkmn) time (it does a constant number of operations per observed entry
if k = O(1)). In turn, to the best of our knowledge, SLA is both faster and more memory efficient
than existing algorithms. SLA is the first memory optimal and asymptotically accurate low rank
approximation algorithm.
The approach used to design SLA can be readily extended to devise memory and computationally
efficient matrix completion algorithms. We present this extension in the supplementary material.
Notations. Throughout the paper, we use the following notations. For any m× n matrix A, we de-
note by A> its transpose, and by A−1 its pseudo-inverse. We denote by s1(A) ≥ · · · ≥ sn∧m(A) ≥
0, the singular values ofA. When matricesA andB have the same number of rows, [A,B] to denote
the matrix whose first columns are those of A followed by those of B. A⊥ denotes an orthonormal
basis of the subspace perpendicular to the linear span of the columns of A. Aj , Ai, and Aij de-
note the j-th column of A, the i-th row of A, and the entry of A on the i-th line and j-th column,
respectively. For h ≤ l, Ah:l (resp. Ah:l) is the matrix obtained by extracting the columns (resp.
lines) h, . . . , l of A. For any ordered set B = {b1, . . . , bp} ⊂ {1, . . . , n}, A(B) refers to the matrix
composed by the ordered set B of columns of A. A(B) is defined similarly (but for lines). For real
numbers a ≤ b, we define |A|ba the matrix with (i, j) entry equal to (|A|ba)ij = min(b,max(a,Aij)).
Finally, for any vector v, ‖v‖ denotes its Euclidean norm, whereas for any matrix A, ‖A‖F denotes
its Frobenius norm, ‖A‖2 its operator norm, and ‖A‖∞ its `∞-norm, i.e., ‖A‖∞ = maxi,j |Aij |.
2 Related Work
Low-rank approximation algorithms have received a lot of attention over the last decade. There are
two types of error estimate for these algorithms: either the error is additive or relative.
To translate our bound (1) in an additive error is easy:
‖M − M̂ (k)‖F ≤ ‖M −M (k)‖F +O
(
k
(
sk+1(M)√
mn
+
log1/2m
(δm)1/4
)
√
mn
)
. (2)
Sparsifying M to speed-up the computation of a low-rank approximation has been proposed in the
literature and the best additive error bounds have been obtained in [AM07]. When the sampling rate
δ satisfies δ ≥ log
4m
m , the authors show that with probability 1− exp(− log
4m),
‖M − M̃ (k)‖F ≤ ‖M −M (k)‖F +O
(
k1/2n1/2
δ1/2
+
k1/4n1/4
δ1/4
‖M (k)‖1/2F
)
. (3)
2
This performance guarantee is derived from Lemma 1.1 and Theorem 1.4 in [AM07]. To
compare (2) and (3), note that our assumptions on the bounded entries of M ensures that:
s2k+1(M)
mn ≤
1
k and ‖M
(k)‖F ≤ ‖M‖F ≤
√
mn. In particular, we see that the worst case
bound for (3) is
(
k1/2√
δm
+ k
1/4
(δm)1/4
)√
nm which is always lower than the worst case bound for (2):
k
(
1
k +
logm√
δm
)1/2√
nm. When k = O(1), our bound is only larger by a logarithmic term in m
compared to [AM07]. However, the algorithm proposed in [AM07] requires to store O(δmn) en-
tries of M whereas SLA needs O(n) memory space. Recall that log4m ≤ δm ≤ m1/9 so that our
algorithm makes a significant improvement on the memory requirement at a low price in the error
guarantee bounds. Although biased sampling algorithms can reduce the error, the algorithm have to
run leverage scores with multiple passes over data [BJS15]. In a recent work, [CW13] proposes a
time efficient algorithm to compute a low-rank approximation of a sparse matrix. Combined with
[AM07], we obtain an algorithm running in time O(δmn) + O(nk2 + k3) but with an increased
additive error term.
We can also compare our result to papers providing an estimate M̃ (k) of the optimal low-rank ap-
proximation of M with a relative error ε, i.e. such that ‖M − M̃ (k)‖F ≤ (1 + ε)‖M −M (k)‖F . To
the best of our knowledge, [CW09] provides the best result in this setting. Theorem 4.4 in [CW09]
shows that provided the rank ofM is at least 2(k+1), their algorithm outputs with probability 1−η
a rank-k matrix M̃ (k) with relative error ε using memory spaceO (k/ε log(1/η)(n+m)) (note that
in [CW09], the authors use as unit of memory a bit whereas we use as unit of memory an entry of the
matrix so we removed a logmn factor in their expression to make fair comparisons). To compare
with our result, we can translate our bound (1) in a relative error, and we need to take:
ε = O
k sk+1(M) + log1/2m(δm)1/4√mn
‖M −M (k)‖F
 .
First note that since M is assumed to be of rank at least 2(k + 1), we have ‖M − M (k)‖F ≥
sk+1(M) > 0 and ε is well-defined. Clearly, for our ε to tend to zero, we need ‖M −M (k)‖F to
be not too small. For the scenario we have in mind, M is a noisy version of the signal M (k) so that
M−M (k) is the noise matrix. When every entry ofM−M (k) is generated independently at random
with a constant variance, ‖M −M (k)‖F = Θ(
√
m+ n) while sk+1(M) = Θ(
√
n). In such a case,
we have ε = o(1) and we improve the memory requirement of [CW09] by a factor ε−1 log(kδ)−1.
[CW09] also considers a model where the full columns of M are revealed one after the other in an
arbitrary order, and proposes a one-pass algorithm to derive the rank-k approximation ofM with the
same memory requirement. In this general setting, our algorithm is required to make two passes on
the data (and only one pass if the order of arrival of the column is random instead of arbitrary). The
running time of the algorithm scales as O(kmnε−1 log(kδ)−1) to project M onto kε−1 log(kδ)−1
dimensional random space. Thus, SLA improves the time again by a factor of ε−1 log(kδ)−1.
We could also think of using sketching and streaming PCA algorithms to estimate M (k). When the
columns arrive sequentially, these algorithms identify the left singular vectors using one-pass on the
matrix and then need a second pass on the data to estimate the right singular vectors. For example,
[Lib13] proposes a sketching algorithm that updates the p most frequent directions as columns are
observed. [GP14] shows that with O(km/ε) memory space (for p = k/ε), this sketching algorithm
finds m × k matrix Û such that ‖M − PÛM‖F ≤ (1 + ε)‖M − M (k)‖F , where PÛ denotes
the projection matrix to the linear span of the columns of Û . The running time of the algorithm
is roughly O(kmnε−1), which is much greater than that of SLA. Note also that to identify such
matrix Û in one pass on M , it is shown in [Woo14] that we have to use Ω(km/ε) memory space.
This result does not contradict the performance analysis of SLA, since the latter needs two passes
on M if the columns of M are observed in an arbitrary manner. Finally, note that the streaming
PCA algorithm proposed in [MCJ13] does not apply to our problem as this paper investigates a very
specific problem: the spiked covariance model where a column is randomly generated in an i.i.d.
manner.
3 Streaming Low-rank Approximation Algorithm
3
Algorithm 1 Streaming Low-rank Approximation (SLA)
Input: M , k, δ, and ` = 1δ log(m)
1. A(B1), A(B2) ← independently sample entries of [M1, . . . ,M`] at rate δ
2. PCA for the first ` columns: Q← SPCA(A(B1), k)
3. Trimming the rows and columns of A(B2):
A(B2) ← set the entries of rows of A(B2) having more than two non-zero entries to 0
A(B2) ← set the entries of the columns of A(B2) having more than 10mδ non-zero entries to 0
4. W ← A(B2)Q 5. V̂ (B1) ← (A(B1))>W 6. Î ← A(B1)V̂ (B1)
Remove A(B1), A(B2), and Q from the memory space
for t = `+ 1 to n do
7. At ← sample entries of Mt at rate δ 8. V̂ t ← (At)>W 9. Î ← Î +AtV̂ t
Remove At from the memory space
end for
10. R̂← find R̂ using the Gram-Schmidt process such that V̂ R̂ is an orthonormal matrix
11. Û ← 1
δ̂
ÎR̂R̂>
Output: M̂ (k) = |Û V̂ >|10
Algorithm 2 Spectral PCA (SPCA)
Input: C ∈ [0, 1]m×`, k
Ω← `× k Gaussian random matrix
Trimming: C̄ ← set the entries of the rows of C with more than 10 non-zero entries to 0
Φ← C̄>C̄ − diag(C̄>C̄)
Power Iteration: QR← QR decomposition of Φd5 log(`)eΩ
Output: Q
In this section, we present the Streaming Low-rank Approximation (SLA) algorithm and analyze
its performance. SLA makes one pass on the matrix M , and is provided with the columns of M
one after the other in a streaming manner. The SVD of M is M = UΣV > where U and V are
(m×m) and (n×n) unitary matrices and Σ is the (m×n) matrix diag(s1(M), . . . sn∧m(M)). We
assume (or impose by design of SLA) that the ` (specified below) first observed columns of M are
chosen uniformly at random among all columns. An extension of SLA to scenarios where columns
are observed in an arbitrary order is presented in §3.5, but this extension requires two passes on M .
To be memory efficient, SLA uses sampling. Each observed entry of M is erased (i.e., set equal to
0) with probability 1 − δ, where δ > 0 is referred to as the sampling rate. The algorithm, whose
pseudo-code is presented in Algorithm 1, proceeds in three steps:
1. In the first step, we observe ` = 1δ log(m) columns of M chosen uniformly at random. These
columns form the matrix M(B) = UΣ(V (B))>, where B denotes the ordered set of the indexes of
the ` first observed columns. M(B) is sampled at rate δ. More precisely, we apply two independent
sampling procedures, where in each of them, every entry of M(B) is sampled at rate δ. The two
resulting independent random matrices A(B1), and A(B2) are stored in memory. A(B1), referred to
as A(B) to simplify the notations, is used in this first step, whereas A(B2) will be used in subsequent
steps. Next through a spectral decomposition of A(B), we derive a (` × k) orthonormal matrix Q
such that the span of its column vectors approximates that of the column vectors of V (B)1:k . The first
step corresponds to Lines 1 and 2 in the pseudo-code of SLA.
2. In the second step, we complete the construction of our estimator of the top k right singular
vectors V1:k of M . Denote by V̂ the k × n matrix formed by these estimated vectors. We first
compute the components of these vectors corresponding to the set of indexes B as V̂ (B) = A>(B1)W
with W = A(B2)Q. Then for t = ` + 1, . . . , n, after receiving the t-th column Mt of M , we set
V̂ t = A>t W , where At is obtained by sampling entries of Mt at rate δ. Hence after one pass on
M , we get V̂ = Ã>W , where Ã = [A(B1), A`+1, . . . , An]. As it turns out, multiplying W by Ã
>
amplifies the useful signal contained in W , and yields an accurate approximation of the span of the
4
top k right singular vectors V1:k of M . The second step is presented in Lines 3, 4, 5, 7 and 8 in SLA
pseudo-code.
3. In the last step, we deduce from V̂ a set of column vectors gathered in matrix Û such that Û>V̂
provides an accurate approximation of M (k). First, using the Gram-Schmidt process, we find R̂
such that V̂ R̂ is an orthonormal matrix and compute Û = 1δAV̂ R̂R̂
> in a streaming manner as in
Step 2. Then, Û V̂ > = 1δAV̂ R̂(V̂ R̂)
> where V̂ R̂(V̂ R̂)> approximates the projection matrix onto
the linear span of the top k right singular vectors of M . Thus, Û V̂ > is close to M (k). This last step
is described in Lines 6, 9, 10 and 11 in SLA pseudo-code.
In the next subsections, we present in more details the rationale behind the three steps of SLA, and
provide a performance analysis of the algorithm.
3.1 Step 1. Estimating right-singular vectors of the first batch of columns
The objective of the first step is to estimate V (B)1:k , those components of the top k right singular
vectors of M whose indexes are in the set B (remember that B is the set of indexes of the ` first
observed columns). This estimator, denoted by Q, is obtained by applying the power method to
extract the top k right singular vector of M(B), as described in Algorithm 2. In the design of this
algorithm and its performance analysis, we face two challenges: (i) we only have access to a sampled
version A(B) of M(B); and (ii) UΣ(V (B))> is not the SVD of M(B) since the column vectors of
V
(B)
1:k are not orthonormal in general (we keep the components of these vectors corresponding to the
set of indexes B). Hence, the top k right singular vectors of M(B) that we extract in Algorithm 2 do
not necessarily correspond to V (B)1:k .
To address (i), in Algorithm 2, we do not directly extract the top k right singular vectors of A(B).
We first remove the rows of A(B) with too many non-zero entries (i.e., too many observed entries
from M(B)), since these rows would perturb the SVD of A(B). Let us denote by Ā the obtained
trimmed matrix. We then form the covariance matrix Ā>Ā, and remove its diagonal entries to
obtain the matrix Φ = Ā>Ā − diag(Ā>Ā). Removing the diagonal entries is needed because of
the sampling procedure. Indeed, the diagonal entries of Ā>Ā scale as δ, whereas its off-diagonal
entries scale as δ2. Hence, when δ is small, the diagonal entries would clearly become dominant in
the spectral decomposition. We finally apply the power method to Φ to obtain Q. In the analysis of
the performance of Algorithm 2, the following lemma will be instrumental, and provides an upper
bound of the gap between Φ and (M(B))>M(B) using the matrix Bernstein inequality (Theorem 6.1
[Tro12]). All proofs are detailed in Appendix.
Lemma 1 If δ ≤ m− 89 , with probability 1 − 1`2 , ‖Φ − δ
2(M(B))
>M(B)‖2 ≤ c1δ
√
m` log(`), for
some constant c1 > 1.
To address (ii), we first establish in Lemma 2 that for an appropriate choice of `, the column vectors
of V (B)1:k are approximately orthonormal. This lemma is of independent interest, and relates the SVD
of a truncated matrix, here M(B), to that of the initial matrix M . More precisely:
Lemma 2 If δ ≤ m−8/9, there exists a `×k matrix V̄ (B) such that its column vectors are orthonor-
mal, and with probability 1 − exp(−m1/7), for all i ≤ k satisfying that s2i (M) ≥ nδ`
√
m` log(`),
‖
√
n
` V
(B)
1:i − V̄
(B)
1:i ‖2 ≤ m−
1
3 .
Note that as suggested by the above lemma, it might be impossible to recover V (B)i when the corre-
sponding singular value si(M) is small (more precisely, when s2i (M) ≤ nδ`
√
m` log(`)). However,
the singular vectors corresponding to such small singular values generate very little error for low-
rank approximation. Thus, we are only interested in singular vectors whose singular values are
above the threshold ( nδ`
√
m` log(`))1/2. Let k′ = max{i : s2i (M) ≥ nδ`
√
m` log(`), i ≤ k}.
Now to analyze the performance of Algorithm 2 when applied to A(B), we decompose Φ as Φ =
δ2`
n V̄
(B)
1:k′ (Σ
1:k′
1:k′)
2(V̄
(B)
1:k′ )
> + Y , where Y = Φ − δ
2`
n V̄
(B)
1:k′ (Σ
1:k′
1:k′)
2(V̄
(B)
1:k′ )
> is a noise matrix. The
5
following lemma quantifies how noise may affect the performance of the power method, i.e., it
provides an upper bound of the gap between Q and V̄ (B)1:k′ as a function of the operator norm of the
noise matrix Y :
Lemma 3 With probability 1 − 1`2 , the output Q of SPCA when applied to A(B) satisfies for all
i ≤ k′: ‖(V̄ (B)1:i )> ·Q⊥‖2 ≤
3‖Y ‖2
δ2 `n si(M)
2 .
In the proof, we analyze the power iteration algorithm from results in [HMT11].
To complete the performance analysis of Algorithm 2, it remains to upper bound ‖Y ‖2. To this aim,
we decompose Y into three terms:
Y =
(
Φ− δ2(M(B))>M(B)
)
+ δ2(M(B))
> (I − U1:k′U>1:k′)M(B)+
δ2
(
(M(B))
>U1:k′U
>
1:k′M(B) −
`
n
V̄
(B)
1:k′ (Σ
1:k′
1:k′)
2(V̄
(B)
1:k′ )
>
)
.
The first term can be controlled using Lemma 1, and the last term is upper bounded using Lemma
2. Finally, the second term corresponds to the error made by ignoring the singular vectors which
are not within the top k′. To estimate this term, we use the matrix Chernoff bound (Theorem 2.2 in
[Tro11]), and prove that:
Lemma 4 With probability 1 − exp(−m1/4), ‖(I − U1:k′U>1:k′)M(B)‖22 ≤ 2δ
√
m` log(`) +
`
ns
2
k+1(M).
In summary, combining the four above lemmas, we can establish that Q accurately estimates V̄ (B)1:k :
Theorem 5 If δ ≤ m−8/9, with probability 1 − 3`2 , the output Q of Algorithm 2 when applied to
A(B) satisfies for all i ≤ k: ‖(V̄
(B)
1:i )
> · Q⊥‖2 ≤
3δ2(s2k+1(M)+2m
2
3 n)+3(2+c1)δ
n
`
√
m` log(`)
δ2s2i (M)
, where
c1 is the constant from Lemma 1.
3.2 Step 2: Estimating the principal right singular vectors of M
In this step, we aim at estimating the top k right singular vectors V1:k, or at least at producing
k vectors whose linear span approximates that of V1:k. Towards this objective, we start from Q
derived in the previous step, and define the (m× k) matrix W = A(B2)Q. W is stored and kept in
memory for the remaining of the algorithm.
It is tempting to directly read from W the top k′ left singular vectors U1:k′ . Indeed, we know that
Q ≈
√
n
` V
(B)
1:k , and E[A(B2)] = δUΣ(V (B))>, and hence E[W ] ≈ δ
√
n
`U1:kΣ
1:k
1:k. However, the
level of the noise in W is too important so as to accurately extract U1:k′ . In turn, W can be written
as δUΣ(V (B))>Q + Z, where Z = (A(B2) − δUΣ(V (B))>)Q partly captures the noise in W . It
is then easy to see that the level of the noise Z satisfies E[‖Z‖2] ≥ E[‖Z‖F /
√
k] = Ω(
√
δm).
Indeed, first observe that Z is of rank k. Then E[‖Z‖2F ] =
∑m
i=1
∑k
j=1 E[Z2ij ] ≈ mkδ: this is
due to the facts that (i) Q and A(B2) − δUΣ(V (B))> are independent (since A(B1) and A(B2) are
independent), (ii) ‖Qj‖22 = 1 for all j ≤ k, and (iii) the entries of A(B2) are independent with
variance Θ(δ(1 − δ)). However, for all j ≤ k′, the j-th singular value of δUΣ(V (B))>Q scales as
O(δ
√
m`) = O(
√
δm
log(m) ), since sj(M) ≤
√
mn and sj(M(B)) ≈
√
`
nsj(M) when j ≤ k
′ from
Lemma 2.
Instead, from W , A(B1) and the subsequent sampled arriving columns At, t > `, we produce
a (n × k) matrix V̂ whose linear span approximates that of V1:k′ . More precisely, we first let
V̂ (B) = A>(B1)W . Then for all t = ` + 1, . . . , n, we define V̂
t = A>t W , where At is obtained
from the t-th observed column of M after sampling each of its entries at rate δ. Multiplying W by
Ã = [A(B1), A`+1, . . . , An] amplifies the useful signal in W , so that V̂ = Ã
>W constitutes a good
approximation of V1:k. To understand why, we can rewrite V̂ as follows:
V̂ = δ2M>M(B)Q+ δM
>(A(B2) − δM(B))Q+ (Ã− δM)
>W.
6
In the above equation, the first term corresponds to the useful signal and the two remaining terms
constitute noise matrices. From Theorem 5, the linear span of columns of Q approximates that of
the columns of V̄ (B) and thus, for j ≤ k′, sj(δ2M>M(B)Q) ≈ δ2s2j (M)
√
`
n ≥ δ
√
mn log(`).
The spectral norms of the noise matrices are bounded using random matrix arguments, and the fact
that (A(B2)− δM(B)) and (Ã− δM) are zero-mean random matrices with independent entries. We
can show (see Lemma 14 given in the supplementary material) using the independence of A(B1)
and A(B2) that with high probability, ‖δM>(A(B2) − δM(B))Q‖2 = O(δ
√
mn). We may also
establish that with high probability, ‖(Ã− δM)>W‖2 = O(δ
√
m(m+ n)). This is a consequence
of a result derived in [AM07] (quoted in Lemma 13 in the supplementary material) stating that with
high probability, ‖Ã − δM‖ = O(
√
δ(m+ n)) and of the fact that due to the trimming process
presented in Line 3 in Algorithm 1, ‖W‖2 = O(
√
δm). In summary, as soon as n scales at least
as m, the noise level becomes negligible, and the span of V̂1:k′ provides an accurate approximation
of that of V1:k′ . The above arguments are made precise and rigorous in the supplementary material.
The following theorem summarizes the accuracy of our estimator of V1:k.
Theorem 6 With log
4(m)
m ≤ δ ≤ m
− 89 for all i ≤ k, there exists a constant c2 such that with
probability 1− kδ, ‖V >i (V̂1:k)⊥‖2 ≤ c2
s2k+1(M)+n log(m)
√
m/δ+m
√
n log(m)/δ
s2i (M)
.
3.3 Step 3: Estimating the principal left singular vectors of M
In the last step, we estimate the principal left singular vectors of M to finally derive an estimator of
M (k), the optimal rank-k approximation of M . The construction of this estimator is based on the
observation that M (k) = U1:kΣ1:k1:kV
>
1:k = MPV1:k , where PV1:k = V1:kV
>
1:k is an (n × n) matrix
representing the projection onto the linear span of the top k right singular vectors V1:k of M . Hence
to estimateM (k), we try to approximate the matrix PV1:k . To this aim, we construct a (k×k) matrix
R̂ so that the column vectors of V̂ R̂ form an orthonormal basis whose span corresponds to that
of the column vectors of V̂ . This construction is achieved using Gram-Schmidt process. We then
approximate PV1:k by PV̂ = V̂ R̂R̂
>V̂ >, and finally our estimator M̂ (k) of M (k) is 1δ ÃPV̂ .
The construction of M̂ (k) can be made in a memory efficient way accommodating for our streaming
model where the columns of M arrive one after the other, as described in the pseudo-code of SLA.
First, after constructing V̂ (B) in Step 2, we build the matrix Î = A(B1)V̂
(B). Then, for t =
` + 1, . . . , n, after constructing the t-th line V̂ t of V̂ , we update Î by adding to it the matrix AtV̂ t,
so that after all columns of M are observed, Î = ÃV̂ . Hence we can build an estimator Û of the
principal left singular vectors of M as Û = 1δ ÎR̂R̂
>, and finally obtain M̂ (k) = |Û V̂ >|10.
To quantify the estimation error of M̂ (k), we decompose M (k) − M̂ (k) as: M (k) − M̂ (k) =
M (k)(I − PV̂ ) + (M (k) − M)PV̂ + (M −
1
δ Ã)PV̂ . The first term of the r.h.s. of the above
equation can be bounded using Theorem 6: for i ≤ k, we have si(M)2‖V >i V̂⊥‖ ≤ z =
c2(s
2
k+1(M) + n log(m)
√
m/δ +m
√
n log(m)/δ), and hence we can conclude that for all i ≤ k,∥∥si(M)UiV >i (I − PV̂ )∥∥2F ≤ z. The second term can be easily bounded observing that the matrix
(M (k) −M)PV̂ is of rank k: ‖(M (k) −M)PV̂ ‖2F ≤ k‖(M (k) −M)PV̂ ‖22 ≤ k‖M (k) −M‖22 =
ksk+1(M)
2. The last term in the r.h.s. can be controlled as in the performance analysis of Step 2, and
observing that ( 1δ Ã−M)PV̂ is of rank k: ‖
(
1
δ Ã−M
)
PV̂ ‖2F ≤ k
∥∥∥ 1δ Ã−M∥∥∥2
2
= O(kδ(m+n)).
It is then easy to remark that for the range of the parameter δ we are interested in, the upper bound z
of the first term dominates the upper bound of the two other terms. Finally, we obtain the following
result (see the supplementary material for a complete proof):
Theorem 7 When log
4(m)
m ≤ δ ≤ m
− 89 , with probability 1 − kδ, the output of the SLA algorithm
satisfies with constant c3:
‖M(k)−[ÛV̂ >]10‖
2
F
mn = c3k
2
(
s2k+1(M)
mn +
log(m)√
δm
+
√
log(m)
δn
)
.
7
Note that if log
4(m)
m ≤ δ ≤ m
− 89 , then log(m)√
δm
= o(1). Hence if n ≥ m, the SLA algorithm provides
an asymptotically accurate estimate of M (k) as soon as sk+1(M)
2
mn = o(1).
3.4 Required Memory and Running Time
Required memory.
Lines 1-6 in SLA pseudo-code. A(B1) and A(B2) have O(δm`) non-zero entries and we need
O(δm` logm) bits to store the id of these entries. Similarly, the memory required to store Φ is
O(δ2m`2 log(`)). Storing Q further requires O(`k) memory. Finally, V̂ (B1) and Î computed in
Line 6 requireO(`k) andO(km) memory space, respectively. Thus, when ` = 1δ logm , this first part
of the algorithm requires O(k(m+ n)) memory.
Lines 7-9. Before we treat the remaining columns, A(B1), A(B2), andQ are removed from the mem-
ory. Using this released memory, when the t-th column arrives, we can store it, compute V̂ t and Î ,
and remove the column to save memory. Therefore, we do not need additional memory to treat the
remaining columns.
Lines 10 and 11. From Î and V̂ , we compute Û . To this aim, the memory required is O(k(m+ n)).
Running time.
From line 1 to 6. The SPCA algorithm requiresO(`k(δ2m`+k) log(`)) floating-point operations to
compute Q. W , V̂ , and Î are inner products, and their computations require O(δkm`) operations.
With ` = 1δ log(m) , the number of operations to treat the first ` columns is O(`k(δ
2m`+ k) log(`) +
kδm`) = O(km) +O(k
2
δ ).
From line 7 to 9. To compute V̂ t and Î when the t-th column arrives, we need O(δkm) operations.
Since there are n− ` remaining columns, the total number of operations is O(δkmn).
Lines 10 and 11 R̂ is computed from V̂ using the Gram-Schmidt process which requires O(k2m)
operations. We then compute ÎR̂R̂> using O(k2m) operations. Hence we conclude that:
In summary, we have shown that:
Theorem 8 The memory required to run the SLA algorithm is O(k(m + n)). Its running time is
O(δkmn+ k
2
δ + k
2m).
Observe that when δ ≥ max( (log(m))
4
m ,
(log(m))2
n ) and k ≤ (log(m))
6, we have δkmn ≥ k2/δ ≥
k2m, and therefore, the running time of SLA is O(δkmn).
3.5 General Streaming Model
SLA is a one-pass low-rank approximation algorithm, but the set of the ` first observed columns
of M needs to be chosen uniformly at random. We can readily extend SLA to deal with scenarios
where the columns of M can be observed in an arbitrary order. This extension requires two passes
on M , but otherwise performs exactly the same operations as SLA. In the first pass, we extract a set
of ` columns chosen uniformly at random, and in the second pass, we deal with all other columns.
To extract ` randomly selected columns in the first pass, we proceed as follows. Assume that when
the t-th column ofM arrives, we have already extracted l columns. Then the t-th column is extracted
with probability `−ln−t+1 . This two-pass version of SLA enjoys the same performance guarantees as
those of SLA.
4 Conclusion
This paper revisited the low rank approximation problem. We proposed a streaming algorithm that
samples the data and produces a near optimal solution with a vanishing mean square error. The
algorithm uses a memory space scaling linearly with the ambient dimension of the matrix, i.e. the
memory required to store the output alone. Its running time scales as the number of sampled entries
of the input matrix. The algorithm is relatively simple, and in particular, does exploit elaborated
techniques (such as sparse embedding techniques) recently developed to reduce the memory re-
quirement and complexity of algorithms addressing various problems in linear algebra.
8
References
[AM07] Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approxi-
mations. Journal of the ACM (JACM), 54(2):9, 2007.
[BJS15] Srinadh Bhojanapalli, Prateek Jain, and Sujay Sanghavi. Tighter low-rank approximation
via sampling the leveraged element. In Proceedings of the Twenty-Sixth Annual ACM-
SIAM Symposium on Discrete Algorithms, pages 902–920. SIAM, 2015.
[CW09] Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming
model. In Proceedings of the forty-first annual ACM symposium on Theory of computing,
pages 205–214. ACM, 2009.
[CW13] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in
input sparsity time. In Proceedings of the forty-fifth annual ACM symposium on Theory
of computing, pages 81–90. ACM, 2013.
[GP14] Mina Ghashami and Jeff M Phillips. Relative errors for deterministic low-rank matrix
approximations. In SODA, pages 707–717. SIAM, 2014.
[HMT11] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with ran-
domness: Probabilistic algorithms for constructing approximate matrix decompositions.
SIAM review, 53(2):217–288, 2011.
[Lib13] Edo Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 581–
588. ACM, 2013.
[MCJ13] Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming
PCA. In Advances in Neural Information Processing Systems, 2013.
[Tro11] Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform.
Advances in Adaptive Data Analysis, 3(01n02):115–126, 2011.
[Tro12] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
Computational Mathematics, 12(4):389–434, 2012.
[Woo14] David Woodruff. Low rank approximation lower bounds in row-update streams. In
Advances in Neural Information Processing Systems, pages 1781–1789, 2014.
9
