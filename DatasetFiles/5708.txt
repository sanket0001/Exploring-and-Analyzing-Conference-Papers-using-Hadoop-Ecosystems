


Paper ID = 5708
Title = A class of network models recoverable by spectral
clustering
Yali Wan
Department of Statistics
University of Washington
Seattle, WA 98195-4322, USA
yaliwan@washington.edu
Marina MeilaÃÜ
Department of Statistics
University of Washington
Seattle, WA 98195-4322, USA
mmp@stat.washington.edu
Abstract
Finding communities in networks is a problem that remains difficult, in spite of the
amount of attention it has recently received. The Stochastic Block-Model (SBM)
is a generative model for graphs with ‚Äúcommunities‚Äù for which, because of its
simplicity, the theoretical understanding has advanced fast in recent years. In par-
ticular, there have been various results showing that simple versions of spectral
clustering using the Normalized Laplacian of the graph can recover the commu-
nities almost perfectly with high probability. Here we show that essentially the
same algorithm used for the SBM and for its extension called Degree-Corrected
SBM, works on a wider class of Block-Models, which we call Preference Frame
Models, with essentially the same guarantees. Moreover, the parametrization we
introduce clearly exhibits the free parameters needed to specify this class of mod-
els, and results in bounds that expose with more clarity the parameters that control
the recovery error in this model class.
1 Introduction
There have been many recent advances in the recovery of communities in networks, under ‚Äúblock-
model‚Äù assumptions [19, 18, 9]. In particular, advances in recovering communities by spectral
clustering algorithms. These have been extended to models including node-specific propensities.
In this paper, we argue that one can further expand the model class for which recovery by spectral
clustering is possible, and describe a model that subsumes a number of existing models, which we
call the PFM. We show that under the PFM model, the communities can be recovered with small
error, w.h.p. Our results correspond to what [6] termed the ‚Äúweak recovery‚Äù regime, in which w.h.p.
the fraction of nodes that are mislabeled is o(1) when n‚Üí‚àû.
2 The Preference Frame Model of graphs with communities
This model embodies the assumption that interactions at the community level (which we will also
call macro level) can be quantified by meaningful parameters. This general assumption underlies
the (p, q) and the related parameterizations of the SBM as well. We define a preference frame to
be a graph with K nodes, one for each community, that encodes the connectivity pattern at the
community level by a (non-symmetric) stochastic matrix R. Formally, given [K] = {1, . . .K}, a
K √óK matrix R (det(R) 6= 0) representing the transition matrix of a reversible Markov chain on
[K], the weighted graph H = ([K], R), with edge set suppR (edges correspond to entries in R not
being 0) is called a K-preference frame. Requiring reversibility is equivalent to requiring that there
is a set of symmetric weights on the edges from which R can be derived ([17]). We note that without
the reversibility assumption, we would be modeling directed graphs, which we will leave for future
1
work. We denote by œÅ the left principal eigenvector of R, satisfying œÅTR = œÅT . W.l.o.g. we can
assume the eigenvalue 1 or R has multiplicity 11 and therefore we call œÅ the stationary distribution
of R.
We say that a deterministic weighted graph G = (V, S) with weight matrix S (and edge set suppS)
admits a K-preference frame H = ([K], R) if and only if there exists a partition C of the nodes V
into K clusters C = {C1, . . . Ck} of sizes n1, . . . , nK , respectively, so that the Markov chain on V
with transition matrix P determined by S satisfies the linear constraints‚àë
j‚ààCm
Pij = Rlm for all i ‚àà Cl, and all cluster indices l,m ‚àà {1, 2, . . . k}. (1)
The matrix P is obtained from S by the standard row-normalization P = D‚àí1S where D =
diag{d1:n}, di =
‚àën
i=1 Sij .
A random graph family over node set V admits a K-preference frameH, and is called a Preference
Frame Model (PFM), if the edges i, j, i < j are sampled independently from Bernoulli distributions
with parameters Sij . It is assumed that the edges obtained are undirected and that Sij ‚â§ 1 for all
pairs i 6= j. We denote a realization from this process by A. Furthermore, let dÃÇi =
‚àë
j‚ààV Aij and
in general, throughout this paper, we will denote computable quantities derived from the observed
A with the same letter as their model counterparts, decorated with the ‚Äúhat‚Äù symbol. Thus, DÃÇ =
diag dÃÇ1:n, PÃÇ = DÃÇ
‚àí1A, and so on.
One question we will study is under what conditions the PFM model can be estimated from a givenA
by a standard spectral clustering algorithms. Evidently, the difficult part in this estimation problem is
recovering the partition C. If this is obtained correctly, the remaining parameters are easily estimated
in a Maximum Likelihood framework.
But another question we elucidate refers to the parametrization itself. It is known that in the SBM
and Degree Corrected-SBM (DC-SBM) [18], in spite of their simplicity, there are dependencies
between the community level ‚Äúintensive‚Äù parameters and the graph level ‚Äúextensive‚Äùparameters, as
we will show below. In the parametrization of the PFM , we can explicitly show which are the free
parameters and which are the dependent ones.
Several network models in wide use admit a preference frame. For example, the SBM(B) model,
which we briefly describe here. This model has parameters the cluster sizes (n1:K) and the con-
nectivity matrix B ‚àà [0, 1]K√óK . For two nodes i, j ‚àà V , the probability of an edge (i, j) is Bkl
iff i ‚àà Ck and j ‚àà Cl. The matrix B needs not be symmetric. When Bkk = p,Bkl = q for
k, l ‚àà [K], k 6= l, the model is denoted SBM(p, q). It is easy to verify that the SBM admits a
preference frame. For instance, in the case of SBM(p, q), we have
di = p(nl ‚àí 1) + q(n‚àí nl) ‚â° dCl , for i ‚àà Cl,
Rl,m =
qnm
dCl
if l 6= m, Rl,l =
p(nl ‚àí 1)
dCl
, for l,m ‚àà {1, 2, . . . , k}.
In the above we have introduced the notation dCl =
‚àë
j‚ààCl di. One particular realization of the
PFM is the Homogeneous K-Preference Frame model (HPFM). In a HPFM, each node i ‚àà V is
characterized by a weight, or propensity to form ties wi. For each pair of communities l,m with
l ‚â§ m and for each i ‚àà Cl, j ‚àà Cm we sample Aij with probability Sij given by
Sij =
Rmlwiwj
œÅl
. (2)
This formulation ensures detail balance in the edge expectations, i.e. Sij = Sji. The HPFM is
virtually equivalent to what is known as the ‚Äúdegree model‚Äù [8] or ‚ÄúDC-SBM‚Äù, up to a reparam-
eterization2. Proposition 1 relates the node weights to the expected node degrees di. We note
that the main result we prove in this paper uses independent sampling of edges only to prove the
concentration of the laplacian matrix. The PFM model can be easily extended to other graph models
1Otherwise the networks obtained would be disconnected.
2Here we follow the customary definition of this model, which does not enforce Sii = 0, even though this
implies a non-zero probability of self-loops.
2
with dependent edges if one could prove concentration and eigenvalue separation. For example,
when R has rational entries, the subgraph induced by each block of A can be represented by a
random d-regular graph with a specified degree.
Proposition 1 In a HPFM di = wi
‚àëK
l=1Rkl
wCl
œÅl
whenever i ‚àà Ck and k ‚àà [K].
Equivalent statements that the expected degrees in each cluster are proportional to the weights exist
in [7, 19] and they are instrumental in analyzing this model. This particular parametrization imme-
diately implies in what case the degrees are globally proportional to the weights. This is, obviously,
the situation when wCl ‚àù œÅl for all l ‚àà [K].
As we see, the node degrees in a HPFM are not directly determined by the propensities wi, but
depend on those by a multiplicative constant that varies with the cluster. This type of interaction
between parameters has been observed in practically all extensions of the Stochastic Block-Model
that we are aware of, making parameter interpretation more difficult. Our following result establishes
what are the free parameters of the PFM and of their subclasses. As it will turn out, these parameters
and their interactions are easily interpretable.
Proposition 2 Let (n1, . . . nK) be a partition of n (assumed to represent the cluster sizes of C =
{C1, . . . CK} a partition of node set V), R a non-singular K √ó K stochastic matrix, œÅ its left
principal eigenvector, and œÄC1 ‚àà [0, 1]n1 , . . . œÄCK ‚àà [0, 1]nK probability distributions over C1:K .
Then, there exists a PFM consistent withH = ([K], R), with clustering C, and whose node degrees
are given by
di = dtotœÅkœÄCk,i, (3)
whenever i ‚àà Ck, where dtot =
‚àë
i‚ààV di is a user parameter which is only restricted above by
Assumption 2.
The proof of this result is constructive, and can be found in the extended version.
The parametrization shows to what extent one can specify independently the degree distribution of a
network model, and the connectivity parameters R. Moreover, it describes the pattern of connection
of a node i as a composition of a macro-level pattern, which gives the total probability of i to
form connections with a cluster l, and the micro-level distribution of connections between i and the
members of Cl. These parameters are meaningful on their own and can be specified or estimated
separately, as they have no hidden dependence on each other or on n,K.
The PFM enjoys a number of other interesting properties. As this paper will show, almost all the
properties that make SBM‚Äôs popular and easy to understand hold also for the much more flexible
PFM. In the remainder of this paper we derive recovery guarantees for the PFM. As an additional
goal, we will show that in the frame we set with the PFM, the recovery conditions become clearer,
more interpretable, and occasionally less restrictive than for other models.
As already mentioned, the PFM includes many models that have been found useful by previous
authors. Yet, the PFM class is much more flexible than those individual models, in the sense that
it allows other unexplored degrees of freedom (or, in other words, achieves the same advantages as
previously studied models with fewer constraints on the data). Note that there is an infinite number
of possible random graphs G with the same parameters (d1:n, n1:k, R) satisfying the constraints (1)
and Proposition 2, yet for reliable community detection we do not need to control S fully, but only
aggregate statistics like
‚àë
j‚ààC Aij .
3 Spectral clustering algorithm and main result
Now, we address the community recovery problem from a random graph (V, A) sampled from
the PFM defined as above. We make the standard assumption that K is known. Our analysis is
3
based on a very common spectral clustering algorithm used in [13] and described also in [14, 21].
Input : Graph (V, A) with |V| = n and A ‚àà {0, 1}n√ón, number of clusters K
Output: Clustering C
1. Compute DÃÇ = diag(dÃÇ1, ¬∑ ¬∑ ¬∑ , dÃÇn) and Laplacian
LÃÇ = DÃÇ‚àí1/2ADÃÇ‚àí1/2 (4)
2. Calculate the K eigenvectors YÃÇ1, ¬∑ ¬∑ ¬∑ , YÃÇK associated with the K eigenvalues |ŒªÃÇ1| ‚â• ¬∑ ¬∑ ¬∑ ‚â• |ŒªÃÇK |
of LÃÇ. Normalize the eigenvectors to unit length. We denote them as the first K eigenvectors in the
following text;
3. Set VÃÇi = DÃÇ‚àí1/2YÃÇi, i = 1, ¬∑ ¬∑ ¬∑ ,K. Form matrix VÃÇ = [VÃÇ1 ¬∑ ¬∑ ¬∑ VÃÇK ];
4. Treating each row of VÃÇ as a point in K dimensions, cluster them by the K-means algorithm to
obtain the clustering CÃÇ.
Algorithm 1: Spectral Clustering
Note that the vectors VÃÇ are the firstK eigenvectors of P . The K-means algorithm is assumed to find
the global optimum. For more details on good initializations for K-means in step 4 see [16].
We quantify the difference between CÃÇ and the true clusterings C by the mis-clustering rate perr,
which is defined as
perr = 1‚àí
1
n
max
œÜ:[K]‚Üí[K]
‚àë
k
|CœÜ(k) ‚à© CÃÇk|. (5)
Theorem 3 (Mis-clustering rate bound for HPFM and PFM) Let the n √ó n matrix S admit a
PFM, and w1:n, R, œÅ, P,A, d1:n have the usual meaning, and let Œª1:n be the eigenvalues of P ,
with |Œªi| ‚â• |Œªi+1|. Let dmin = min d1:n be the minimum expected degree, dÃÇmin = min dÃÇi, and
dmax = maxij nSij . Let Œ≥ ‚â• 1, Œ∑ > 0 be arbitrary numbers. Assume:
Assumption 1 S admits a HPFM model and (2) holds.
Assumption 2 Sij ‚â§ 1
Assumption 3 dÃÇmin ‚â• log(n)
Assumption 4 dmin ‚â• log(n)
Assumption 5 ‚àÉŒ∫ > 0, dmax ‚â§ Œ∫ log n
Assumption 6 grow > 0, where grow is defined in Proposition 4.
Assumption 7 Œª1:K are the eigenvalues of R, and |ŒªK | ‚àí |ŒªK+1| = œÉ > 0.
We also assume that we run Algorithm 1 on S and that K-means finds the optimal solution. Then,
for n sufficiently large, the following statements hold with probability at least 1‚àí e‚àíŒ≥ .
PFM Assumptions 2 - 7 imply
perr ‚â§
Kdtot
ndmingrow
[
C0Œ≥
4
œÉ2 log n
+
4(log n)Œ∑
dÃÇmin
]
(6)
HPFM Assumptions 1 - 6 imply
perr ‚â§
Kdtot
ndmingrow
[
C0Œ≥
4
Œª2K log n
+
4(log n)Œ∑
dÃÇmin
]
(7)
where C0 is a constant depending on Œ∫ and Œ≥.
Note that perr decreases at least as 1/ log(n) when dÃÇmin = dmin = log(n). This is because dÃÇmin
and dmin help with the concentration of L. Using Proposition 4, the distances between rows of V ,
4
i.e, the true centers of the k-means step, are lower bounded by grow/dtot. After plugging in the
assumptions for dmin, dÃÇmin, dmax, we obtain
perr ‚â§
KŒ∫
grow
[
C0Œ≥
4
œÉ2 log n
+
4
(log n)(1‚àíŒ∑)
]
. (8)
When n is small, the first component on the right hand side dominates because of the constant C0,
while the second part dominates when n is very large. This shows that perr decreases almost as
1/ log n. Of the remaining quantities, Œ∫ controls the spread of the degrees di. Notice that ŒªK and
œÉ are eigengaps in HPFM model and PFM model respectively and depend only on the preference
frame, and likewise for grow. The eigengaps ensure the stability of principal spaces and the sepa-
ration from the spurious eigenvalues, as shown in Proposition 6. The term containing (log n)Œ∑ is
designed to control the difference between di and dÃÇi with Œ∑ a small positive constant.
3.1 Proof outline, techniques and main concepts
The proof of Theorem 3 (given in the extended version of the paper) relies on three steps, which
are to be found in most results dealing with spectral clustering. First, concentration bounds of
the empirical Laplacian LÃÇ w.r.t L are obtained. There are various conditions under which these
can be obtained, and ours are most similar to the recent result of [9]. The other tools we use are
Hoeffding bounds and tools from linear algebra. Second, one needs to bound the perturbation of
the eigenvectors Y as a function of the perturbation in L. This is based on the pivotal results of
Davis and Kahan, see e.g [18]. A crucial ingredient in these type of theorems is the size of the
eigengap between the invariant subspace Y and its orthogonal complement. This is a condition that
is model-dependent, and therefore we discuss the techniques we introduce for solving this problem
in the PFM in the next subsection.
The third step is to bound the error of the K-means clustering algorithm. This is done by a counting
argument. The crux of this step is to ensure the separation of theK distinct rows of V . This, again, is
model dependent and we present our result below. The details and proof are in the extended version.
All proofs are for the PFM; to specialize to the HPFM, one replaces œÉ with |ŒªK |
3.2 Cluster separation and bounding the spurious eigenvalues in the PFM
Proposition 4 (Cluster separation) Let V, œÅ, d1:n have the usual meaning and define the cluster
volume dCk =
‚àë
i‚ààCk di, and cmax, cmin as maxk,mink
dCk
nœÅk
. Let i, j ‚àà V be nodes belonging
respectively to clusters k,m with k 6= m. Then,
||Vi: ‚àí Vj:||2 ‚â•
1
dtot
[
1
cmax
(
1
œÅk
+
1
œÅm
)
‚àí 1‚àö
œÅkœÅm
(
1
cmin
‚àí 1
cmax
)]
=
grow
dtot
, (9)
where grow =
[
1
cmax
(
1
œÅk
+ 1œÅm
)
‚àí 1‚àöœÅkœÅm
(
1
cmin
‚àí 1cmax
)]
. Moreover, if the columns of V are
normalized to length 1, the above result holds by replacing dtotcmax,min with max,mink nkœÅk .
In the square brackets, cmax,min depend on the cluster-level degree distribution, while all the other
quantitities depend only of the preference frame. Hence, this expression is invariant with n, and as
long as it is strictly positive, we have that the cluster separation is ‚Ñ¶(1/dtot).
The next theorem is crucial in proving that L has a constant eigengap. We express the eigengap of P
in terms of the preference frameH and the mixing inside each of the clusters Ck. For this, we resort
to generalized stochastic matrices, i.e. rectangular positive matrices with equal row sums, and we
relate their properties to the mixing of Markov chains on bipartite graphs.
These tools are introduced here, for the sake of intuition, toghether with the main spectral result,
while the rest of the proofs are in the extended version.
Given C, for any vector x ‚àà Rn, we denote by xk, k = 1, . . .K, the block of x indexed by elements
of cluster k of C. Similarly, for any square matrix A ‚àà Rn√ón, we denote by Akl = [Aij ]i‚ààk,j‚ààl the
block with rows indexed by i ‚àà k, and columns indexed by j ‚àà l.
5
Denote by œÅ, Œª1:K , ŒΩ1:K ‚àà RK respectively the stationary distribution, eigenvalues3, and eigenvec-
tors of R.
We are interested in block stochastic matrices P for which the eigenvalues of R are the principal
eigenvalues. We call ŒªK+1 . . . Œªn spurious eigenvalues. Theorem 6 below is a sufficient condition
that bounds |ŒªK+1| whenever each of the K2 blocks of P is ‚Äùhomogeneous‚Äù in a sense that will be
defined below.
When we consider the matrix L = D‚àí1/2SD‚àí1/2 partitioned according to C, it will be convenient
to consider the off-diagonal blocks in pairs. This is why the next result describes the properties of
matrices consisting of a pair of off-diagonal blocks.
Proposition 5 (Eigenvalues for the off-diagonal blocks) Let M be the square matrix
M =
[
0 B
A 0
]
(10)
where A ‚àà Rn2√ón1 and B ‚àà Rn1√ón2 , and let x =
[
x1
x2
]
, x1,2 ‚àà Cn1,2 be an eigenvector of M
with eigenvalue Œª. Then
Bx2 = Œªx1 ABx2 = Œª
2x2 (11)
Ax1 = Œªx2 BAx1 = Œª
2x1 (12)
M2 =
[
BA 0
0 AB
]
(13)
Moreover, if M is symmetric, i.e B = AT , then Œª is a singular value of A, x is real, and ‚àíŒª is
also an eigenvalue of M with eigenvector [xT1 ‚àí xT2 ]T . Assuming n2 ‚â§ n1, and that A is full rank,
one can write A = V ŒõUT with V ‚àà Rn2√ón2 , U ‚àà Rn1√ón2 orthogonal matrices, and Œõ a diagonal
matrix of non-zero singular values.
Theorem 6 (Bounding the spurious eigenvalues of L) Let C, L, P,D, S,R, œÅ be defined as above,
and let Œª be an eigenvalue of P . Assume that (1) P is block-stochastic with respect to C; (2) Œª1:K are
the eigenvalues ofR, and |ŒªK | > 0; (3) Œª is not an eigenvalue ofR; (4) denote by Œªkl3 (Œªkk2 ) the third
(second) largest in magnitude eigenvalue of block Mkl (Lkk) and assume that
|Œªkl3 |
Œªmax(Mkl)
‚â§ c < 1
( |Œª
kk
2 |
Œªmax(Lkk)
‚â§ c). Then, the spurious eigenvalues of P are bounded by c times a constant that
depends only on R.
|Œª| ‚â§ c max
k=1:K
Ô£´Ô£≠rkk +‚àë
l 6=k
‚àö
rklrlk
Ô£∂Ô£∏ (14)
Remarks: The factor that multiplies c can be further bounded denoting a = [
‚àö
rkl]
T
l=1:K , b =
[
‚àö
rlk]
T
l=1:K
rkk +
‚àë
l 6=k
‚àö
rklrlk = a
T b ‚â§ ||a||||b|| =
‚àö‚àö‚àö‚àö K‚àë
l=1
rkl
K‚àë
l=1
rlk =
‚àö‚àö‚àö‚àö K‚àë
l=1
rlk (15)
In other words,
|Œª| ‚â§ c
2
max
k=1:K
‚àö‚àö‚àö‚àö K‚àë
l=1
rlk (16)
The maximum column sum of a stochastic matrix is 1 if the matrix is doubly stochastic and larger
than 1 otherwise, and can be as large as
‚àö
K. However, one must remember that the interesting R
matrices have ‚Äúlarge‚Äù eigenvalues. In particular we will be interested in ŒªK > c. It is expected that
under these conditions, the factor depending on R to be close to 1.
3Here too, eigenvalues will always be ordered in decreasing order of their magnitudes, with positive values
preceeding negatives one of the same magnitude. Consequently, for any stochastic matrix, Œª1 = 1 always
6
The second remark is on the condition (3), that all blocks have small spurious eigenvalues. This
condition is not merely a technical convenience. If a block had a large eigenvalue, near 1 or ‚àí1
(times its Œªmax), then that block could itself be broken into two distinct clusters. In other words, the
clustering C would not accurately capture the cluster structure of the matrix P . Hence, condition (3)
amounts to requiring that no other cluster structure is present, in other words that within each block,
the Markov chain induced by P mixes well.
4 Related work
Previous results we used The Laplacian concentration results use a technique introduced recently
by [9], and some of the basic matrix theoretic results are based on [14] which studied the P and L
matrix in the context of spectral clustering. As any of the many works we cite, we are indebted to
the pioneering work on the perturbation of invariant subspaces of Davis and Kahan [18, 19, 20].
4.1 Previous related models
The configuration model for regular random graphs [4, 11] and for graphs with general fixed degrees
[10, 12] is very well known. It can be shown by a simple calculation that the configuration model
also admits a K-preference frame. In the particular case when the diagonal of the R matrix is 0 and
the connections between clusters are given by a bipartite configuration model with fixed degrees,
K-preference frames have been studied by [15] under the name ‚Äúequitable graphs‚Äù; the object there
was to provide a way to calculate the spectrum of the graph.
Since the PFM is itself an extension of the SBM, many other extensions of the latter will bear
resemblance to PFM. Here we review only a subset of these, a series of strong relatively recent
advances, which exploit the spectral properties of the SBM and extend this to handle a large range
of degree distributions [7, 19, 5]. The PFM includes each of these models as a subclass4.
In [7] the authors study a model that coincides (up to some multiplicative constants) with the HPFM.
The paper introduces an elegant algorithm that achieves partial recovery or better, which is based
on the spectral properties of a random Laplacian-like matrix, and does not require knowledge of the
partition size K.
The PFM also coincides with the model of [1] and [8] called the expected degree model w.r.t the
distribution of intra-cluster edges, but not w.r.t the ambient edges, so the HPFM is a subclass of this
model.
A different approach to recovery The papers [5, 18, 9] propose regularizing the normalized Lapla-
cian with respect to the influence of low degrees, by adding the scaled unit matrix œÑI to the incidence
matrix A, and thereby they achieve recovery for much more imbalanced degree distributions than
us. Currently, we do not see an application of this interesting technique to the PFM, as the diagonal
regularization destroys the separation of the intracluster and intercluster transitions, which guaran-
tee the clustering property of the eigenvectors. Therefore, currently we cannot break the n log n
limit into the ultra-sparse regime, although we recognize that this is an important current direction
of research.
Recovery results like ours can be easily extended to weighted, non-random graphs, and in this sense
they are relevant to the spectral clustering of these graphs, when they are assumed to be noisy
versions of a G that admits a PFM.
4.2 An empirical comparison of the recovery conditions
As obtaining general results in comparing the various recovery conditions in the literature would be
a tedious task, here we undertake to do a numerical comparison. While the conclusions drawn from
this are not universal, they illustrate well the stringency of various conditions, as well as the gap
between theory and actual recovery. For this, we construct HPFM models, and verify numerically if
they satisfy the various conditions. We have also clustered random graphs sampled from this model,
with good results (shown in the extended version).
4In particular, the models proposed in [7, 19, 5] are variations of the DC-SBM and thus forms of the
homogeneous PFM.
7
We generate S from the HPFM model with K = 5, n = 5000. Each wi is uniformly generated
from (0.5, 1). n1:K = (500, 1000, 1500, 1000, 1000), grow > 0, Œª1:K = (1, 0.8, 0.6, 0.4, 0.2). The
matrix R is given below; note its last row in which r55 <
‚àë4
l=1 r5l.
R =
Ô£´Ô£¨Ô£¨Ô£¨Ô£≠
.80 .07 .02 .02 .09
.04 .52 .24 .12 .08
.01 .20 .65 .15 .00
.01 .08 .12 .70 .08
.13 .21 .02 .32 .33
Ô£∂Ô£∑Ô£∑Ô£∑Ô£∏ œÅ = (.25, .44, .54, .65, .17). (17)
The conditions we are verifying include besides ours, those obtained by [18], [19], [3] and [5];
since the original S is a perfect case for spectral clustering of weighted graphs, we also verify the
theoretical recovery conditions for spectral clustering in [2] and [16].
Our result Theorem 3 Assumption 1 and 2 automatically hold from the construction of the data.
By simulating the data, We find that dmin = 77.4, dÃÇmin = 63, both of which are bigger than
log n = 8.52. Therefore Assumption 3 and 4 hold. dmax = 509.3, grow = 1.82 > 0, thus Assump-
tion 5 and 6 hold. After running Algorithm 1, the mis-clustering rate is r = 0.0008, which satisfies
the theoretical bound. In conclusion, the dataset fits into both the assumptions and conclusion of
Theorem 3.
Qin and Rohe[18] This paper has an assumption on the lower bound on ŒªK , that is 18‚àö3ŒªK ‚â•‚àö
K(ln(K/)
dmin
, so that the concentration bound holds with probability (1 ‚àí ). We set  = 0.1 and
obtain ŒªK ‚â• 12.3, which is impossible to hold since ŒªK is upper bounded by 15.
Rohe, Chatterjee, Yu[19] Here, one defines œÑn = dminn , and requires œÑ
2
n log n > 2 to ensure the
concentration of L. To meet this assumption, with n = 5000, dmin ‚â• 2422. While in our case
dmin = 77.4. The assumption requires a very dense graph and is not satisfied in this dataset.
Balcan, Borgs Braverman, Chayes[3]Their theorem is based on self-determined community struc-
ture. It requires all the nodes to be more connected within their own cluster. However, in our graph,
1296 out of 5000 nodes have more connections to outside nodes than to nodes in their own cluster.
Ng, Jordan, Weiss[16] require Œª2 < 1 ‚àí Œ¥, where Œ¥ > (2 + 2
‚àö
2),  =
‚àö
K(K ‚àí 1)1 +K22,
1 ‚â• maxi1,i2‚àà{1,¬∑¬∑¬∑ ,K}
‚àë
j‚ààCi1
‚àë
k‚ààCi2
A2jk
dÃÇj dÃÇk
, 2 ‚â• maxi‚àà{1,¬∑¬∑¬∑ ,K}
‚àë
k:k‚ààSi
dÃÇj
(
‚àë
k,l‚ààSi
A2kl
dÃÇkdÃÇl
)1/2.
On the given data, we find that  ‚â• 36.69, and Œ¥ ‚â• 125.28, which is impossible to hold since Œ¥
needs to be smaller than 1.
Chaudhuri, Chung, Tsiatas[5] The recovery theorem of this paper requires di ‚â• 1289 ln(6n/Œ¥),
so that when all the assumptions hold, it recovers the clustering correctly with probability at least
1 ‚àí 6Œ¥. We set Œ¥ = 0.01, and obtain that di = 77.40, 1289 ln(6n/Œ¥) = 212.11. Therefore the
assumption fails as well.
For our method, the hardest condition to satisfy, and the most different from the others, was Assump-
tion 6. We repeated this experiment with the other weights distributions for which this assumption
fails. The assumptions in the related papers continued to be violated. In [Qin and Rohe], we obtain
ŒªK ‚â• 17.32. In [Rohe, Chatterjee, Yu], we still needs dmin ‚â• 2422. In [Balcan, Borgs Braverman,
Chayes], we get 1609 points more connected to the outside nodes of its cluster. In [Balakrishnan,
Xu, Krishnamurthy, Singh], we get œÉ = 0.172 and needs to satisfy œÉ = o(0.3292). In [Ng, Jordan,
Weiss], we obtain Œ¥ ‚â• 175.35. Therefore, the assumptions in these papers are all violated as well.
5 Conclusion
In this paper, we have introduced the preference frame model, which is more flexible and subsumes
many current models including SBM and DC-SBM. It produces state-of-the art recovery rates com-
parable to existing models. To accomplish this, we used a parametrization that is clearer and more
intuitive. The theoretical results are based on the new geometric techniques which control the eigen-
gaps of the matrices with piecewise constant eigenvectors.
We note that the main result theorem 3 uses independent sampling of edges only to prove the concen-
tration of the laplacian matrix. The PFM model can be easily extended to other graph models with
dependent edges if one could prove concentration and eigenvalue separation. For example, when
R has rational entries, the subgraph induced by each block of A can be represented by a random
d-regular graph with a specified degree.
5To make Œª ‚â§ 1 possible, one needs dmin ‚â• 11718.
8
References
[1] Sanjeev Arora, Rong Ge, Sushant Sachdeva, and Grant Schoenebeck. Finding overlapping
communities in social networks: toward a rigorous approach. In Proceedings of the 13th ACM
Conference on Electronic Commerce, pages 37‚Äì54. ACM, 2012.
[2] Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise thresholds
for spectral clustering. In Advances in Neural Information Processing Systems, pages 954‚Äì962,
2011.
[3] Maria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, and Shang-Hua
Teng. Finding endogenously formed communities. arxiv preprint arXiv:1201.4899v2, 2012.
[4] Bela Bollobas. Random Graphs. Cambridge University Press, second edition, 2001.
[5] K. Chaudhuri, F. Chung, and A. Tsiatas. Spectral clustering of graphs with general degrees in
extended planted partition model. Journal of Machine Learning Research, pages 1‚Äì23, 2012.
[6] Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and
submatrix localization with a growing number of clusters and submatrices. arXiv preprint
arXiv:1402.1267, 2014.
[7] Amin Coja-Oghlan and Andre Lanka. Finding planted partitions in random graphs with general
degree distributions. SIAM Journal on Discrete Mathematics, 23:1682‚Äì1714, 2009.
[8] M. O. Jackson. Social and Economic Networks. Princeton University Press, 2008.
[9] Can M. Le and Roman Vershynin. Concentration and regularization of random graphs. 2015.
[10] Brendan McKay. Asymptotics for symmetric 0-1 matrices with prescribed row sums. Ars
Combinatoria, 19A:15‚Äì26, 1985.
[11] Brendan McKay and Nicholas Wormald. Uniform generation of random regular graphs of
moderate degree. Journal of Algorithms, 11:52‚Äì67, 1990.
[12] Brendan McKay and Nicholas Wormald. Asymptotic enumeration by degree sequence of
graphs with degrees o(n1/2. Combinatorica, 11(4):369‚Äì382, 1991.
[13] Marina MeilaÃÜ and Jianbo Shi. Learning segmentation by random walks. In T. K. Leen, T. G.
Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems, vol-
ume 13, pages 873‚Äì879, Cambridge, MA, 2001. MIT Press.
[14] Marina MeilaÃÜ and Jianbo Shi. A random walks view of spectral segmentation. In T. Jaakkola
and T. Richardson, editors, Artificial Intelligence and Statistics AISTATS, 2001.
[15] M.E.J. Newman and Travis Martin. Equitable random graphs. 2014.
[16] Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis and an
algorithm. Advances in neural information processing systems, 2:849‚Äì856, 2002.
[17] J.R. Norris. Markov Chains. Cambridge University Press, 1997.
[18] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic
blockmodel. In Advances in Neural Information Processing Systems, pages 3120‚Äì3128, 2013.
[19] Karl Rohe, Sourav Chatterjee, Bin Yu, et al. Spectral clustering and the high-dimensional
stochastic blockmodel. The Annals of Statistics, 39(4):1878‚Äì1915, 2011.
[20] Gilbert W Stewart, Ji-guang Sun, and Harcourt Brace Jovanovich. Matrix perturbation theory,
volume 175. Academic press New York, 1990.
[21] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395‚Äì
416, 2007.
9
