


Paper ID = 5926
Title = Quartz: Randomized Dual Coordinate Ascent
with Arbitrary Sampling
Zheng Qu
Department of Mathematics
The University of Hong Kong
Hong Kong
zhengqu@maths.hku.hk
Peter Richtárik
School of Mathematics
The University of Edinburgh
EH9 3FD, United Kingdom
peter.richtarik@ed.ac.uk
Tong Zhang
Department of Statistics
Rutgers University
Piscataway, NJ, 08854
tzhang@stat.rutgers.edu
Abstract
We study the problem of minimizing the average of a large number of smooth
convex functions penalized with a strongly convex regularizer. We propose and
analyze a novel primal-dual method (Quartz) which at every iteration samples and
updates a random subset of the dual variables, chosen according to an arbitrary
distribution. In contrast to typical analysis, we directly bound the decrease of
the primal-dual error (in expectation), without the need to first analyze the dual
error. Depending on the choice of the sampling, we obtain efficient serial and
mini-batch variants of the method. In the serial case, our bounds match the best
known bounds for SDCA (both with uniform and importance sampling). With
standard mini-batching, our bounds predict initial data-independent speedup as
well as additional data-driven speedup which depends on spectral and sparsity
properties of the data.
Keywords: empirical risk minimization, dual coordinate ascent, arbitrary sampling, data-driven
speedup.
1 Introduction
In this paper we consider a primal-dual pair of structured convex optimization problems which has
in several variants of varying degrees of generality attracted a lot of attention in the past few years
in the machine learning and optimization communities [4, 22, 20, 23, 21, 27].
Let A1, . . . , An be a collection of d-by-m real matrices and φ1, . . . , φn be 1/γ-smooth convex
functions from Rm to R, where γ > 0. Further, let g : Rd → R ∪ {+∞} be a 1-strongly convex
function and λ > 0 a regularization parameter. We are interested in solving the following primal
problem:
minw=(w1,...,wd)∈Rd
[
P (w)
def
= 1n
∑n
i=1 φi(A
>
i w) + λg(w)
]
. (1)
In the machine learning context, matrices {Ai} are interpreted as examples/samples, w is a (linear)
predictor, function φi is the loss incurred by the predictor on example Ai, g is a regularizer, λ
is a regularization parameter and (1) is the regularized empirical risk minimization problem. In
1
this paper we are especially interested in problems where n is very big (millions, billions), and
much larger than d. This is often the case in big data applications. Stochastic Gradient Descent
(SGD) [18, 11, 25] was designed for solving this type of large-scale optimization problems. In each
iteration SGD computes the gradient of one single randomly chosen function φi and approximates
the gradient using this unbiased but noisy estimation. Because of the variance of the stochastic
estimation, SGD has slow convergence rate O(1/). Recently, many methods achieving fast (linear)
convergence rate O(log(1/)) have been proposed, including SAG [19], SVRG [6], S2GD [8],
SAGA [1], mS2GD [7] and MISO [10], all using different techniques to reduce the variance.
Another approach, such as Stochastic Dual Coordinate Ascent (SDCA) [22], solves (1) by consid-
ering its dual problem that is defined as follows. For each i, let φ∗i : Rm → R be the convex
conjugate of φi, namely, φ∗i (u) = maxs∈Rm s
>u − φi(s) and similarly let g∗ : Rd → R be the
convex conjugate of g. The dual problem of (1) is defined as:
max
α=(α1,...,αn)∈RN=Rnm
[
D(α)
def
= −f(α)− ψ(α)
]
, (2)
where α = (α1, . . . , αn) ∈ RN = Rnm is obtained by stacking dual variables (blocks) αi ∈ Rm,
i = 1, . . . , n, on top of each other and functions f and ψ are defined by
f(α)
def
= λg∗
(
1
λn
∑n
i=1Aiαi
)
; ψ(α)
def
= 1n
∑n
i=1 φ
∗
i (−αi). (3)
SDCA [22] and its proximal extension Prox-SDCA [20] first solve the dual problem (2) by updating
uniformly at random one dual variable at each round and then recover the primal solution by setting
w = ∇g∗(α). Let Li = λmax(A>i Ai). It is known that if we run SDCA for at least
O
((
n+ maxi Liλγ
)
log
((
n+ maxi Liλγ
)
1

))
iterations, then SDCA finds a pair (w,α) such that E[P (w)−D(α)] ≤ . By applying accelerated
randomized coordinate descent on the dual problem, APCG [9] needs at most Õ(n +
√
maxi Li
λγ )
number of iterations to get -accuracy. ASDCA [21] and SPDC [26] are also accelerated and ran-
domized primal-dual methods. Moreover, they can update a mini-batch of dual variables in each
round.
We propose a new algorithm (Algorithm 1), which we call Quartz, for simultaneously solving the
primal (1) and dual (2) problems. On the dual side, at each iteration our method selects and updates
a random subset (sampling) Ŝ ⊆ {1, . . . , n} of the dual variables/blocks. We assume that these sets
are i.i.d. throughout the iterations. However, we do not impose any additional assumptions on the
distribution of Ŝ apart from the necessary requirement that each block i needs to be chosen with
a positive probability: pi
def
= P(i ∈ Ŝ) > 0. Quartz is the first SDCA-like method analyzed for
an arbitrary sampling. The dual updates are then used to perform an update to the primal variable
w and the process is repeated. Our primal updates are different (less aggressive) from those used
in SDCA [22] and Prox-SDCA [20], thanks to which the decrease in the primal-dual error can be
bounded directly without first establishing the dual convergence as in [20], [23] and [9]. Our analysis
is novel and directly primal-dual in nature. As a result, our proof is more direct, and the logarithmic
term in our bound has a simpler form.
Main result. We prove that starting from an initial pair (w0, α0), Quartz finds a pair (w,α) for
which P (w)−D(α) ≤  (in expectation) in at most
max
i
(
1
pi
+ vipiλγn
)
log
(
P (w0)−D(α0)

)
(4)
iterations. The parameters v1, . . . , vn are assumed to satisfy the following ESO (expected separable
overapproximation) inequality:
EŜ
[∥∥∑
i∈Ŝ Aihi
∥∥2] ≤∑ni=1 pivi‖hi‖2, (5)
where ‖ ·‖ denotes the standard Euclidean norm. Moreover, the parameters v1, . . . , vn are needed to
run the method (they determine stepsizes), and hence it is critical that they can be cheaply computed
2
before the method starts. We wish to point out that (5) always holds for some parameters {vi}.
Indeed, the left hand side is a quadratic function of h and hence the inequality holds for large-
enough vi. Having said that, the size of these parameters directly influences the complexity, and
hence one would want to obtain as tight bounds as possible. As we will show, for many samplings
of interest small enough parameter v can be obtained in time required to read the data {Ai}. In
particular, if the data matrix A = (A1, . . . , An) is sufficiently sparse, our iteration complexity
result (4) specialized to the case of standard mini-batching can be better than that of accelerated
methods such as ASDCA [21] and SPDC [26] even when the condition number maxi Li/λγ is
larger than n, see Proposition 4 and Figure 2.
As described above, Quartz uses an arbitrary sampling for picking the dual variables to be updated
in each iteration. To the best of our knowledge, only two papers exist in the literature where a
stochastic method using an arbitrary sampling was analyzed: NSync [16] for unconstrained mini-
mization of a strongly convex function and ALPHA [15] for composite minimization of non-strongly
convex function. Assumption (5) was for the first time introduced in [16]. However, NSync is not
a primal-dual method. Besides NSync, the closest works to ours in terms of the generality of the
sampling are PCDM [17], SPCDM [3] and APPROX [2]. All these are randomized coordinate
descent methods, and all were analyzed for arbitrary uniform samplings (i.e., samplings satisfying
P(i ∈ Ŝ) = P(i′ ∈ Ŝ) for all i, i′ ∈ {1, . . . , n}). Again, none of these methods were analyzed in a
primal-dual framework.
In Section 2 we describe the algorithm, show that it admits a natural interpretation in terms of
Fenchel duality and discuss the flexibility of Quartz. We then proceed to Section 3 where we state the
main result, specialize it to the samplings discussed in Section 2, and give detailed comparison of our
results with existing results for related primal-dual stochastic methods in the literature. In Section 4
we demonstrate how Quartz compares to other related methods through numerical experiments.
2 The Quartz Algorithm
Throughout the paper we consider the standard Euclidean norm, denoted by ‖ · ‖. A function φ :
Rm → R is (1/γ)-smooth if it is differentiable and has Lipschitz continuous gradient with Lispchitz
constant 1/γ: ‖∇φ(x)−∇φ(y)‖ ≤ 1γ ‖x− y‖, for all x, y ∈ R
m. A function g : Rd → R∪ {+∞}
is 1-strongly convex if g(w) ≥ g(w′) + 〈∇g(w′), w − w′〉+ 12‖w − w
′‖2 for all w,w′ ∈ dom(g),
where dom(g) denotes the domain of g and ∇g(w′) is a subgradient of g at w′.
The most important parameter of Quartz is a random sampling Ŝ, which is a random subset of
[n] = {1, 2, . . . , n}. The only assumption we make on the sampling Ŝ in this paper is the following:
Assumption 1 (Proper sampling) Ŝ is a proper sampling; that is,
pi
def
= P(i ∈ Ŝ) > 0, i ∈ [n]. (6)
This assumption guarantees that each block (dual variable) has a chance to get updated by the
method. Prior to running the algorithm, we compute positive constants v1, . . . , vn satisfying (5)
to define the stepsize parameter θ used throughout in the algorithm:
θ = min
i
piλγn
vi+λγn
. (7)
Note from (5) that θ depends on both the data matrix A and the sampling Ŝ. We shall show how to
compute in less than two passes over the data the parameter v satisfying (5) for some examples of
sampling in Section 2.2.
2.1 Interpretation of Quartz through Fenchel duality
3
Algorithm 1 Quartz
Parameters: proper random sampling Ŝ and a positive vector v ∈ Rn
Initialization: α0 ∈ RN ; w0 ∈ Rd; pi = P(i ∈ Ŝ); θ = min
i
piλγn
vi+λγn
; ᾱ0 = 1λn
∑n
i=1Aiα
0
i
for t ≥ 1 do
wt = (1− θ)wt−1 + θ∇g∗(ᾱt−1)
αt = αt−1
Generate a random set St ⊆ [n], following the distribution of Ŝ
for i ∈ St do
αti = (1− θp
−1
i )α
t−1
i − θp
−1
i ∇φi(A>i wt)
end for
ᾱt = ᾱt−1 + (λn)−1
∑
i∈St Ai(α
t
i − α
t−1
i )
end for
Output: wt, αt
Quartz (Algorithm 1) has a natural interpretation in terms of Fenchel duality. Let (w,α) ∈ Rd×RN
and define ᾱ = 1λn
∑n
i=1Aiαi. The duality gap for the pair (w,α) can be decomposed as:
P (w)−D(α) (1)+(2)= λ (g(w) + g∗ (ᾱ)) + 1n
∑n
i=1 φi(A
>
i w) + φ
∗
i (−αi)
= λ(g(w) + g∗ (ᾱ)− 〈w, ᾱ〉︸ ︷︷ ︸
GAPg(w,α)
) + 1n
∑n
i=1 φi(A
>
i w) + φ
∗
i (−αi) + 〈A>i w,αi〉︸ ︷︷ ︸
GAPφi (w,αi)
.
By Fenchel-Young inequality, GAPg(w,α) ≥ 0 and GAPφi(w,αi) ≥ 0 for all i, which proves
weak duality for the problems (1) and (2), i.e., P (w) ≥ D(α). The pair (w,α) is optimal when both
GAPg and GAPφi for all i are zero. It is known that this happens precisely when the following
optimality conditions hold:
w = ∇g∗(ᾱ) (8)
αi = −∇φi(A>i w), i ∈ [n]. (9)
We will now interpret the primal and dual steps of Quartz in terms of the above discussion. It is easy
to see that Algorithm 1 updates the primal and dual variables as follows:
wt = (1− θ)wt−1 + θ∇g∗(ᾱt−1) (10)
αti =
{ (
1− θp−1i
)
αt−1i + θp
−1
i
(
−∇φi(A>i wt)
)
, i ∈ St
αt−1i , i /∈ St
(11)
where ᾱt−1 = 1λn
∑n
i=1Aiα
t−1
i , θ is a constant defined in (7) and St ∼ Ŝ is a random subset of
[n]. In other words, at iteration t we first set the primal variable wt to be a convex combination of
its current value wt−1 and a value reducing GAPg to zero: see (10). This is followed by adjusting a
subset of dual variables corresponding to a randomly chosen set of examples St such that for each
example i ∈ St, the i-th dual variable αti is set to be a convex combination of its current value α
t−1
i
and a value reducing GAPφi to zero, see (11).
2.2 Flexibility of Quartz
Clearly, there are many ways in which the distribution of Ŝ can be chosen, leading to numerous
variants of Quartz. The convex combination constant θ used throughout the algorithm should be
tuned according to (7) where v1, . . . , vn are constants satisfying (5). Note that the best possible v
is obtained by computing the maximal eigenvalue of the matrix (A>A) ◦ P where ◦ denotes the
Hadamard (component-wise) product of matrices and P ∈ RN×N is an n-by-n block matrix with
all elements in block (i, j) equal to P(i ∈ Ŝ, j ∈ Ŝ), see [14]. However, the worst-case complexity
of computing directly the maximal eigenvalue of (A>A) ◦ P amounts to O(N2), which requires
unreasonable preprocessing time in the context of machine learning where N is assumed to be very
large. We now describe some examples of sampling Ŝ and show how to compute in less than two
passes over the data the corresponding constants v1, . . . , vn. More examples including distributed
sampling are presented in the supplementary material.
4
Serial sampling. The most studied sampling in the literature on stochastic optimization is the
serial sampling, which corresponds to the selection of a single block i ∈ [n]. That is, |Ŝ| = 1 with
probability 1. The name “serial” is pointing to the fact that a method using such a sampling will
typically be a serial (as opposed to being parallel) method; updating a single block (dual variable) at
a time. A serial sampling is uniquely characterized by the vector of probabilities p = (p1, . . . , pn),
where pi is defined by (6). For serial sampling Ŝ, it is easy to see that (5) is satisfied for
vi = Li
def
= λmax(A
>
i Ai), i ∈ [n], (12)
where λmax(·) denotes the maximal eigenvalue.
Standard mini-batching. We now consider Ŝ which selects subsets of [n] of cardinality τ , uni-
formly at random. In the terminology established in [17], such Ŝ is called τ -nice. This sampling
satisfies pi = pj for all i, j ∈ [n]; and hence it is uniform. This sampling is well suited for parallel
computing. Indeed, Quartz could be implemented as follows. If we have τ processors available, then
at the beginning of iteration twe can assign each block (dual variable) in St to a dedicated processor.
The processor assigned to i would then compute ∆αti and apply the update. If all processors have
fast access to the memory where all the data is stored, as is the case in a shared-memory multicore
workstation, then this way of assigning workload to the individual processors does not cause any
major problems. For τ -nice sampling, (5) is satisfied for
vi = λmax(Mi), Mi =
∑d
j=1
(
1 +
(ωj−1)(τ−1)
n−1
)
A>jiAji, i ∈ [n], (13)
where for each j ∈ [d], ωj is the number of nonzero blocks in the j-th row of matrix A, i.e.,
ωj
def
= |{i ∈ [n] : Aji 6= 0}|, j ∈ [d]. (14)
Note that (13) follows from an extension of a formula given in [2] from m = 1 to m ≥ 1.
3 Main Result
The complexity of our method is given by the following theorem. The proof can be found in the
supplementary material.
Theorem 2 (Main Result) Assume that g is 1-strongly convex and that for each i ∈ [n], φi is convex
and (1/γ)-smooth. Let Ŝ be a proper sampling (Assumption 1) and v1, . . . , vn be positive scalars
satisfying (5). Then the sequence of primal and dual variables {wt, αt}t≥0 of Quartz (Algorithm 1)
satisfies:
E[P (wt)−D(αt)] ≤ (1− θ)t(P (w0)−D(α0)), (15)
where θ is defined in (7). In particular, if we fix  ≤ P (w0)−D(α0), then for
T ≥ max
i
(
1
pi
+ vipiλγn
)
log
(
P (w0)−D(α0)

)
⇒ E[P (wT )−D(αT )] ≤ . (16)
In order to put the above result into context, in the rest of this section we will specialize the above
result to two special samplings: a serial sampling, and the τ -nice sampling.
3.1 Quartz with serial sampling
When Ŝ is a serial sampling, we just need to plug (12) into (16) and derive the bound
T ≥ max
i
(
1
pi
+ Lipiλγn
)
log
(
P (w0)−D(α0)

)
=⇒ E[P (wT )−D(αT )] ≤ . (17)
If in addition, Ŝ is uniform, then pi = 1/n for all i ∈ [n] and we refer to this special case of Quartz
as Quartz-U. By replacing pi = 1/n in (17) we obtain directly the complexity of Quartz-U:
T ≥
(
n+ maxi Liλγ
)
log
(
P (w0)−D(α0)

)
=⇒ E[P (wT )−D(αT )] ≤ . (18)
5
Otherwise, we can seek to maximize the right-hand side of the inequality in (17) with respect to
the sampling probability p to obtain the best bound. A simple calculation reveals that the optimal
probability is given by:
P(Ŝ = {i}) = p∗i
def
= (Li + λγn)/
∑n
i=1 (Li + λγn) . (19)
We shall call Quartz-IP the algorithm obtained by using the above serial sampling probability. The
following complexity result of Quartz-IP can be derived easily by plugging (19) into (17):
T ≥
(
n+
∑n
i=1 Li
nλγ
)
log
(
P (w0)−D(α0)

)
=⇒ E[P (wT )−D(αT )] ≤ . (20)
Note that in contrast with the complexity result of Quartz-U (18), we now have dependence on the
average of the eigenvalues Li.
Quartz-U vs Prox-SDCA. Quartz-U should be compared to Proximal Stochastic Dual Coordinate
Ascent (Prox-SDCA) [22, 20]. Indeed, the dual update of Prox-SDCA takes exactly the same form
of Quartz-U1, see (11). The main difference is how the primal variable wt is updated: while Quartz
performs the update (10), Prox-SDCA (see also [24, 5]) performs the more aggressive update wt =
∇g∗(ᾱt−1) and the complexity result of Prox-SDCA is as follows:
T ≥
(
n+ maxi Liλγ
)
log
((
n+ maxi Liλγ
)(
D(α∗)−D(α0)

))
⇒ E[P (wT )−D(αT )] ≤ , (21)
where α∗ is the dual optimal solution. Notice that the dominant terms in (18) and (21) exactly match,
although our logarithmic term is better and simpler. This is due to a direct bound on the decrease
of the primal-dual error of Quartz, without the need to first analyze the dual error, in contrast to the
typical approach for most of the dual coordinate ascent methods [22, 23, 20, 21, 9].
Quartz-IP vs Iprox-SDCA. The importance sampling (19) was previously used in the algorithm
Iprox-SDCA [27], which extends Prox-SDCA to non-uniform serial sampling. The complexity of
Quartz-IP (20) should then be compared with the following complexity result of Iprox-SDCA [27]:
T ≥
(
n+
∑n
i=1 Li
nλγ
)
log
((
n+
∑n
i=1 Li
nλγ
)(
D(α∗)−D(α0)

))
⇒ E[P (wT )−D(αT )] ≤ . (22)
Again, the dominant terms in (20) and (22) exactly match but our logarithmic term is smaller.
3.2 Quartz with τ -nice Sampling (standard mini-batching)
We now specialize Theorem 2 to the case of the τ -nice sampling. We define w̃ such that:
maxi λmax
(∑d
j=1
(
1 +
(ωj−1)(τ−1)
n−1
)
A>jiAji
)
=
(
1 + (ω̃−1)(τ−1)n−1
)
maxi Li
It is clear that 1 ≤ w̃ ≤ maxj wj ≤ n and can be considered as a measure of the density of the data.
By plugging (13) into (16) we obtain directly the following corollary.
Corollary 3 Assume Ŝ is the τ -nice sampling and v is chosen as in (13). If we let  ≤ P (w0) −
D(α0) and
T ≥
n
τ +
(
1+
(ω̃−1)(τ−1)
n−1
)
maxi Li
λγτ
 log (P (w0)−D(α0) ) ⇒ E[P (wT )−D(αT )] ≤ . (23)
Let us now have a detailed look at the above result, especially in terms of how it compares with the
serial uniform case (18). For fully sparse data, we get perfect linear speedup: the bound in (23) is a
1/τ fraction of the bound in (18). For fully dense data, the condition number (κ def= maxi Li/(λγ))
is unaffected by mini-batching. For general data, the behaviour of Quartz with τ -nice sampling
interpolates these two extreme cases. It is important to note that regardless of the condition number
κ, as long as τ ≤ 1 + (n − 1)/(w̃ − 1) the bound in (23) is at most a 2/τ fraction of the bound
in (18). Hence, for sparser problems, Quartz can achieve linear speedup for larger mini-batch sizes.
1In [20] the authors proposed five options of dual updating rule. Our dual updating formula (11) should be
compared with option V in Prox-SDCA. For the same reason as given in the beginning of [20, Appendix A.],
Quartz implemented with the same other four options achieves the same complexity result as Theorem 2.
6
3.3 Quartz vs existing primal-dual mini-batch methods
We now compare the above result with existing mini-batch stochastic dual coordinate ascent meth-
ods. The mini-batch variants of SDCA, to which Quartz with τ -nice sampling can be naturally
compared, have been proposed and analyzed previously in [23], [21] and [26]. In [23], the authors
proposed to use a so-called safe mini-batching, which is precisely equivalent to finding the stepsize
parameter v satisfying (5) (in the special case of τ -nice sampling). However, they only analyzed
the case where the functions {φi}i are non-smooth. In [21], the authors studied accelerated mini-
batch SDCA (ASDCA), specialized to the case when the regularizer g is the squared L2 norm.
They showed that the complexity of ASDCA interpolates between that of SDCA and accelerated
gradient descent (AGD) [13] through varying the mini-batch size τ . In [26], the authors proposed
a mini-batch extension of their stochastic primal-dual coordinate algorithm (SPDC). Both ASDCA
and SPDC reach the same complexity as AGD when the mini-batch size equals to n, thus should be
considered as accelerated algorithms 2. The complexity bounds for all these algorithms are summa-
rized in Table 1. In Table 2 we compare the complexities of SDCA, ASDCA, SPDC and Quartz in
several regimes.
Algorithm Iteration complexity g
SDCA [22] n+ 1λγ
1
2
‖ · ‖2
ASDCA [21] 4×max
{
n
τ
,
√
n
λγτ
, 1
λγτ
, n
1
3
(λγτ)
2
3
}
1
2
‖ · ‖2
SPDC [26] nτ +
√
n
λγτ general
Quartz with τ -nice
sampling
n
τ
+
(
1 + (ω̃−1)(τ−1)
n−1
)
1
λγτ
general
Table 1: Comparison of the iteration complexity of several primal-dual algorithms performing stochastic
coordinate ascent steps in the dual using a mini-batch of examples of size τ (with the exception of SDCA,
which is a serial method using τ = 1.
Algorithm γλn = Θ( 1√n ) γλn = Θ(1) γλn = Θ(τ) γλn = Θ(
√
n)
SDCA [22] n3/2 n n n
ASDCA
[21]
n3/2/τ + n5/4/
√
τ +
n4/3/τ2/3
n/
√
τ n/τ n/τ + n3/4/
√
τ
SPDC [26] n5/4/
√
τ n/
√
τ n/τ n/τ + n3/4/
√
τ
Quartz
(τ -nice) n
3/2/τ + ω̃
√
n n/τ + ω̃ n/τ n/τ + ω̃/
√
n
Table 2: Comparison of leading factors in the complexity bounds of several methods in 5 regimes.
Looking at Table 2, we see that in the γλn = Θ(τ) regime (i.e., if the condition number is κ =
Θ(n/τ)), Quartz matches the linear speedup (when compared to SDCA) of ASDCA and SPDC.
When the condition number is roughly equal to the sample size (κ = Θ(n)), then Quartz does better
than both ASDCA and SPDC as long as n/τ + ω̃ ≤ n/
√
τ . In particular, this is the case when
the data is sparse: ω̃ ≤ n/
√
τ . If the data is even more sparse (and in many big data applications
one has ω̃ = O(1)) and we have ω̃ ≤ n/τ , then Quartz significantly outperforms both ASDCA
and SPDC. Note that Quartz can be better than both ASDCA and SPDC even in the domain of
accelerated methods, that is, when the condition number is larger than the number of examples:
κ = 1/(γλ) ≥ n. Indeed, we have the following result:
Proposition 4 Assume that nλγ ≤ 1 and that maxi Li = 1. If the data is sufficiently sparse so that
λγτn ≥
(
1 + nλγ + (ω̃−1)(τ−1)n−1
)2
, (24)
then the iteration complexity (in Õ order) of Quartz is better than that of ASDCA and SPDC.
The result can be interpreted as follows: if n ≤ κ ≤ τn/(1 + n/κ)2 (that is, τ ≥ λγτn ≥
(1 + nλγ)2), then there are sparse-enough problems for which Quartz is better than both ASDCA
and SPDC.
2APCG [9] also reaches accelerated convergence rate but was not proposed in the mini-batch setting.
7
4 Experimental Results
In this section we demonstrate how Quartz specialized to different samplings compares with other
methods. All of our experiments are performed withm = 1, for smoothed hinge-loss functions {φi}
with γ = 1 and squared L2-regularizer g, see [20]. The experiments were performed on the three
datasets reported in Table 3, and three randomly generated large dataset [12] with n = 100, 000
examples, d = 100, 000 features with different sparsity. In Figure 1 we compare Quartz specialized
to serial sampling and for both uniform and optimal sampling with Prox-SDCA and Iprox-SDCA,
previously discussed in Section 3.1, on three datasets. Due to the conservative primal date in Quartz,
Quartz-U appears to be slower than Prox-SDCA in practice. Nevertheless, in all the experiments,
Quartz-IP shows almost identical convergence behaviour to that of Iprox-SDCA. In Figure 2 we
compare Quartz specialized to τ -nice sampling with mini-batch SPDC for different values of τ , in
the domain of accelerated methods (κ = 10n). The datasets are randomly generated following [13,
Section 6]. When τ = 1, it is clear that SPDC outperforms Quartz as the condition number is
larger than n. However, as τ increases, the number of data processed by SPDC is increased by
√
τ
as predicted by its theory but the number of data processed by Quartz remains almost the same by
taking advantage of the large sparsity of the data. Hence, Quartz is much better in the large τ regime.
Dataset # Training size n # features d Sparsity (# nnz/(nd))
cov1 522,911 54 22.22%
w8a 49,749 300 3.91%
ijcnn1 49,990 22 59.09%
Table 3: Datasets used in our experiments.
0 50 100 150
10
−15
10
−10
10
−5
10
0
nb of epochs
P
ri
m
a
l d
u
a
l g
a
p
 
 
Prox-SDCA
Quartz-U
Iprox-SDCA
Quartz-IP
(a) cov1; n = 522911;λ = 1e-06
0 100 200 300
10
−15
10
−10
10
−5
10
0
nb of epochs
P
ri
m
a
l d
u
a
l g
a
p
 
 
Prox-SDCA
Quartz-U
Iprox-SDCA
Quartz-IP
(b) w8a; n = 49749;λ = 1e-05
0 50 100
10
−15
10
−10
10
−5
10
0
nb of epochs
P
ri
m
a
l d
u
a
l g
a
p
 
 
Prox-SDCA
Quartz-U
Iprox-SDCA
Quartz-IP
(c) ijcnn1; n = 49990;λ = 1e-05
Figure 1: Comparison of Quartz-U (uniform sampling), Quartz-IP (optimal importance sampling), Prox-
SDCA (uniform sampling) and Iprox-SDCA (optimal importance sampling).
0 100 200 300
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
nb of epochs
P
ri
m
a
l d
u
a
l g
a
p
 
 
Quartz-τ=1
SPDC-τ=1
Quartz-τ=100
SPDC-τ=100
Quartz-τ=1000
SPDC-τ=1000
(a) Rand1; n = 105; λ = 1e-06
0 100 200 300
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
nb of epochs
P
ri
m
a
l d
u
a
l g
a
p
 
 
Quartz-τ=1
SPDC-τ=1
Quartz-τ=10
SPDC-τ=10
Quartz-τ=100
SPDC-τ=100
(b) Rand2; n = 105; λ = 1e-06
0 100 200 300
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
nb of epochs
P
ri
m
a
l d
u
a
l g
a
p
 
 
Quartz-τ=1
SPDC-τ=1
Quartz-τ=10
SPDC-τ=10
Quartz-τ=40
SPDC-τ=40
(c) Rand3; n = 105; λ = 1e-06
Figure 2: Comparison of Quartz with SPDC for different mini-batch size τ in the regime κ = 10n. The three
random datasets Random1, Random2 and Random2 have respective sparsity 0.01%, 0.1% and 1%.
8
References
[1] A. Defazio, F. Bach, and S. Lacoste-julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27,
pages 1646–1654. 2014.
[2] O. Fercoq and P. Richtárik. Accelerated, parallel and proximal coordinate descent. SIAM Journal on
Optimization (after minor revision), arXiv:1312.5799, 2013.
[3] O. Fercoq and P. Richtárik. Smooth minimization of nonsmooth functions by parallel coordinate descent.
arXiv:1309.5885, 2013.
[4] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.S. Keerthi, and S. Sundararajan. A dual coordinate descent method
for large-scale linear SVM. In Proc. of the 25th International Conference on Machine Learning, ICML
’08, pages 408–415, 2008.
[5] M. Jaggi, V. Smith, M. Takac, J. Terhorst, S. Krishnan, T. Hofmann, and M.I. Jordan. Communication-
efficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems 27,
pages 3068–3076. Curran Associates, Inc., 2014.
[6] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In
C.j.c. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.q. Weinberger, editors, Advances in Neural
Information Processing Systems 26, pages 315–323. 2013.
[7] J. Konečný, J. Lu, P. Richtárik, and M. Takáč. mS2GD: Mini-batch semi-stochastic gradient descent in
the proximal setting. arXiv:1410.4744, 2014.
[8] J. Konečný and P. Richtárik. S2GD: Semi-stochastic gradient descent methods. arXiv:1312.1666, 2013.
[9] Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient method and its application to
regularized empirical risk minimization. Technical Report MSR-TR-2014-94, July 2014.
[10] J. Mairal. Incremental majorization-minimization optimization with application to large-scale machine
learning. SIAM J. Optim., 25(2):829–855, 2015.
[11] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM J. Optim., 19(4):1574–1609, 2008.
[12] Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM J.
Optim., 22(2):341–362, 2012.
[13] Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program., 140(1, Ser. B):125–
161, 2013.
[14] Z. Qu and P. Richtárik. Coordinate descent methods with arbitrary sampling II: Expected separable
overapproximation. arXiv:1412.8063, 2014.
[15] Z. Qu and P. Richtárik. Coordinate descent methods with arbitrary sampling I: Algorithms and complexity.
arXiv:1412.8060, 2014.
[16] P. Richtárik and M. Takáč. On optimal probabilities in stochastic coordinate descent methods. Optimiza-
tion Letters, published online 2015.
[17] P. Richtárik and M. Takáč. Parallel coordinate descent methods for big data optimization. Math. Program.,
published online 2015.
[18] H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statistics, 22:400–407, 1951.
[19] M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient.
arXiv:1309.2388, 2013.
[20] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. arXiv:1211.2717, 2012.
[21] S. Shalev-shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In Advances
in Neural Information Processing Systems 26, pages 378–385. 2013.
[22] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. J. Mach.
Learn. Res., 14(1):567–599, February 2013.
[23] M. Takáč, A.S. Bijral, P. Richtárik, and N. Srebro. Mini-batch primal and dual methods for svms. In
Proc. of the 30th International Conference on Machine Learning (ICML-13), pages 1022–1030, 2013.
[24] T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In
Advances in Neural Information Processing Systems 26, pages 629–637. 2013.
[25] T. Zhang. Solving large scale l.ear prediction problems using stochastic gradient descent algorithms. In
Proc. of the 21st International Conference on Machine Learning (ICML-04), pages 919–926, 2004.
[26] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimiza-
tion. In Proc. of the 32nd International Conference on Machine Learning (ICML-15), pages 353–361,
2015.
[27] P. Zhao and T. Zhang. Stochastic optimization with importance sampling. ICML, 2015.
9
