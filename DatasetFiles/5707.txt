


Paper ID = 5707
Title = Subspace Clustering with Irrelevant Features via
Robust Dantzig Selector
Chao Qu
Department of Mechanical Engineering
National University of Singapore
A0117143@u.nus.edu
Huan Xu
Department of Mechanical Engineering
National University of Singapore
mpexuh@nus.edu.sg
Abstract
This paper considers the subspace clustering problem where the data contains
irrelevant or corrupted features. We propose a method termed “robust Dantzig se-
lector” which can successfully identify the clustering structure even with the pres-
ence of irrelevant features. The idea is simple yet powerful: we replace the inner
product by its robust counterpart, which is insensitive to the irrelevant features
given an upper bound of the number of irrelevant features. We establish theoreti-
cal guarantees for the algorithm to identify the correct subspace, and demonstrate
the effectiveness of the algorithm via numerical simulations. To the best of our
knowledge, this is the first method developed to tackle subspace clustering with
irrelevant features.
1 Introduction
The last decade has witnessed fast growing attention in research of high-dimensional data: images,
videos, DNA microarray data and data from many other applications all have the property that the
dimensionality can be comparable or even much larger than the number of samples. While this
setup appears ill-posed in the first sight, the inference and recovery is possible by exploiting the fact
that high-dimensional data often possess low dimensional structures [3, 14, 19]. On the other hand,
in this era of big data, huge amounts of data are collected everywhere, and such data is generally
heterogeneous. Clean data and irrelevant or even corrupted information are often mixed together,
which motivates us to consider the high-dimensional, big but dirty data problem. In particular, we
study the subspace clustering problem in this setting.
Subspace clustering is an important subject in analyzing high-dimensional data, inspired by many
real applications[15]. Given data points lying in the union of multiple linear spaces, subspace clus-
tering aims to identify all these linear spaces, and cluster the sample points according to the linear
spaces they belong to. Here, different subspaces may correspond to motion of different objects in
video sequence [11, 17, 20], different rotations, translations and thickness in handwritten digit or
the latent communities for the social graph [15, 5].
A variety of algorithms of subspace clustering have been proposed in the last several years includ-
ing algebraic algorithms [16], iterative methods [9, 1], statistical methods [11, 10], and spectral
clustering-based methods [6, 7]. Among them, sparse subspace clustering (SSC) not only achieves
state-of-art empirical performance, but also possesses elegant theoretical guarantees. In [12], the
authors provide a geometric analysis of SSC which explains rigorously why SSC is successful even
when the subspaces are overlapping [12]. [18] and [13] extend SSC to the noisy case, where data are
contaminated by additive Gaussian noise. Different from these work, we focus on the case where
some irrelevant features are involved.
1
Mathematically, SSC indeed solves for each sample a sparse linear regression problem with the
dictionary being all other samples. Many properties of sparse linear regression problem are well
understood in the clean data case. However, the performance of most standard algorithms deterio-
rates (e.g. LASSO and OMP) even only a few entries are corrupted. As such, it is well expected
that standard SSC breaks for subspace clustering with irrelevant or corrupted features (see Section 5
for numerical evidences). Sparse regression under corruption is a hard problem, and few work has
addressed this problem [8][21] [4].
Our contribution: Inspired by [4], we use a simple yet powerful tool called robust inner product
and propose the robust Dantzig selector to solve the subspace clustering problem with irrelevant
features. While our work is based upon the robust inner product developed to solve robust sparse
regression, the analysis is quite different from the regression case since both the data structures and
the tasks are completely different: for example, the RIP condition – essential for sparse regression –
is hardly satisfied for subspace clustering [18]. We provide sufficient conditions to ensure that
the Robust Dantzig selector can detect the true subspace clustering. We further demonstrate via
numerical simulation the effectiveness of the proposed method. To the best of our knowledge, this
is the first attempt to perform subspace clustering with irrelevant features.
2 Problem setup and method
2.1 Notations and model
The clean data matrix is denoted by XA ∈ RD×N , where each column corresponds to a data point,
normalized to a unit vector. The data points are lying on a union of L subspace S = ∪Ll=1Sl.
Each subspace Sl is of dimension dl which is smaller than D and contains Nl data samples with
N1+N2+· · ·+NL = N . We denote the observed dirty data matrix byX ∈ R(D+D1)×N . Out of the
D +D1 features, up to D1 of them are irrelevant. Without loss of generality, let X = [XTO , X
T
A ]
T ,
where XO ∈ RD1×N denotes the irrelevant data. The subscript A and O denote the set of row
indices corresponding to true and irrelevant features and the superscript T denotes the transpose.
Notice that we do not knowO a priori except its cardinality isD1. The model is illustrated in Figure
1. Let X(l)A ∈ RD×Nl denote the selection of columns in XA that belongs to Sl. Similarly, denote
the corresponding columns inX byX(l). Without loss of generality, letX = [X(1), X(2), ..., X(L)]
be ordered. Further more, we use the subscript “−i”to describe a matrix that excludes the column
i, e.g., (XA)
(l)
−i = [(xA)
(l)
1 , ..., (xA)
(l)
i−1, (xA)
(l)
i+1, ..., (xA)
(l)
Nl
]. We use the superscript lc to describe
a matrix that excludes column in subspace l, e.g., (XA)l
c
= [X
(1)
A , ..., X
(l−1)
A , X
(l+1)
A , ..., X
(L)
A ].
For a matrix Σ, we use Σs,η to denote the submatrix with row indices in set s and column indices
in set η. For any matrix Z, P (Z) denotes the symmetrized convex hull of its column, i.e., P (Z) =
conv(±z1,±z2, ....,±zN ) . We define P l−i := P ((XA)
(l)
−i) for simplification, i.e., the symmetrized
convex hull of clean data in subspace l except data i. Finally we use ‖ · ‖2 to denote the l2 norm of
a vector and ‖ · ‖∞ to denote infinity norm of a vector or a matrix. Caligraphic letters such as X ,Xl
represent the set containing all columns of the corresponding clean data matrix.
Figure 1: Illustration of the model of irrelevant features in the subspace clustering problem. The
left one is the model addressed in this paper: Among total D + D1 features, up tp D1 of them are
irrelevant. The right one illustrates a more general case, where the value of any D1 element of each
column can be arbitrary (e.g., due to corruptions). It is a harder case and left for future work.
2
Figure 2: Illustration of the Subspace Detection Property. Here, each figure corresponds to a matrix
where each column is ci, and non-zero entries are in white. The left figure satisfies this property.
The right one does not.
2.2 Method
In this secion we present our method as well as the intuition that derives it. When all observed data
are clean, to solve the subspace clustering problem, the celebrated SSC [6] proposes to solve the
following convex programming
min
ci
‖ci‖1 s.t. xi = X−ici, (1)
for each data point xi. When data are corrupted by noise of small magnitude such as Gaussian noise,
a straightforward extension of SSC is the Lasso type method called “Lasso-SSC” [18, 13]
min
ci
‖ci‖1 +
λ
2
‖xi −X−ici‖22. (2)
Note that while Formulation (2) has the same form as Lasso, it is used to solve the subspace cluster-
ing task. In particular, the support recovery analysis of Lasso does not extend to this case, as X−i
typically does not satisfy the RIP condition [18].
This paper considers the case where X contains irrelevant/gross corrupted features. As we dis-
cussed above, Lasso is not robust to such corruption. An intuitive idea is to consider the following
formulation first proposed for sparse linear regression [21].
min
ci,E
‖ci‖1 +
λ
2
‖xi − (X−i − E)ci‖22 + η‖E‖∗, (3)
where ‖ · ‖∗ is some norm corresponding to the sparse type of E. One major challenge of this
formulation is that it is not convex. As such, it is not clear how to efficiently find the optimal
solution, and how to analyze the property of the solution (typically done via convex analysis) in the
subspace clustering task.
Our method is based on the idea of robust inner product. The robust inner product 〈a, b〉k is defined
as follows: For vector a ∈ RD, b ∈ RD, we compute qi = aibi, i = 1, ..., N . Then {|qi|} are sorted
and the smallest (D − k) are selected. Let Ω be the set of selected indices, then 〈a, b〉k =
∑
i∈Ω qi,
i.e., the largest k terms are truncated. Our main idea is to replace all inner products involved by
robust counterparts 〈a, b〉D1 , where D1 is the upper bound of the number of irrelevant features.
The intuition is that the irrelevant features with large magnitude may affect the correct subspace
clustering. This simple truncation process will avoid this. We remark that we do not need to know
the exact number of irrelevant feature, but instead only an upper bound of it.
Extending (2) using the robust inner product leads the following formulation:
min
ci
‖ci‖1 +
λ
2
cTi Σ̂ci − λγ̂T ci, (4)
where Σ̂ and γ̂ are robust counterparts of XT−iX−i and X
T
−ixi. Unfortunately, Σ̂ may not be a
positive semidefinite matrix, thus (4) is not a convex program. Unlike the work [4][8] which studies
3
non-convexity in linear regression, the difficulty of non-convexity in the subspace clustering task
appears to be hard to overcome.
Instead we turn to the Dantzig Selector, which is essentially a linear program (and hence no positive
semidefiniteness is required):
min
ci
‖ci‖1 + λ‖XT−i(X−ici − xi)‖∞. (5)
Replace all inner product by its robust counterpart, we propose the following Robust Dantzig Selec-
tor, which can be easily recast as a linear program:
Robust Dantzig Selector: min
ci
‖ci‖1 + λ‖Σ̂ci − γ̂‖∞, (6)
Subspace Detection Property: To measure whether the algorithm is successful, we define the
criterion Subspace Detection Property following [18]. We say that the Subspace Detection Prop-
erty holds, if and only if for all i, the optimal solution to the robust Dantzig Selector satisfies (1)
Non-triviality: ci is not a zero vector; (2) Self-Expressiveness Property: nonzeros entries of ci
correspond to only columns of X sampled from the same subspace as xi. See Figure 2 for illustra-
tions.
3 Main Results
To avoid repetition and cluttered notations, we denote the following primal convex problem by
P (Σ, γ)
min
c
‖c‖1 + λ‖Σc− γ‖∞.
Its dual problem, denoted by D(Σ, γ), is
max
ξ
〈ξ, γ〉 subject to ‖ξ‖1 = λ ‖Σξ‖∞ ≤ 1. (7)
Before we presents our results, we define some quantities.
The dual direction is an important geometric term introdcued in analyzing SSC [12]. Here we define
similarly the dual direction of the robust Dantzig selector: Notice that the dual of robust Dantzig
problem is D(Σ̂, γ̂), where γ̂ and Σ̂ are robust counterparts of XT−ixi and X
T
−iX−i respectively
(recall thatX−i and xi are the dirty data). We decompose Σ̂ into two parts Σ̂ = (XA)T−i(XA)−i+Σ̃,
where the first term corresponds to the clean data, and the second term is due to the irrelevant features
and truncation from the robust inner product. Thus, the second constraint of the dual problem
becomes ‖((XA)T−i(XA)−i + Σ̃)ξ‖∞ ≤ 1. Let ξ be the optimal solution to the above optimization
problem, we define v(xi, X−i, λ) := (XA)−iξ and the dual direction as vl =
v(xli,X
(l)
−i ,λ)
‖v(xli,X
(l)
−i ,λ)‖2
.
Similarly as SSC [12], we define the subspace incoherence. Let V l = [vl1, v
l
2, ..., v
l
Nl
]. The incoher-
ence of a point set Xl to other clean data points is defined as µ(Xl) = maxk:k 6=l ‖(X(k)A )TV l‖∞.
Recall that we decompose Σ̂ and γ̂ as Σ̂ = (XA)T−i(XA)−i + Σ̃ and γ̂ = (XA)
T
−i(xA)i + γ̃.
Intuitively, for robust Dantzig selecter to succeed, we want Σ̃ and γ̃ not too large. Particularly, we
assume ‖(xA)i‖∞ ≤ 1 and ‖(XA)−i‖∞ ≤ 2.
Theorem 1 (Deterministic Model). Denote µl := µ(Xl), rl := mini:xi∈Xl r(P l−i), r :=
minl=1,...,L rl and suppose µl < rl for all l. If
1
r2 − 4D112r − 2D122
< min
l
rl − ul
2D122(ul + rl)
, (8)
then the subspace detection property holds for all λ in the range
1
r2 − 4D112r − 2D122
< λ < min
l
rl − ul
2D122(ul + rl)
. (9)
4
In an ideal case when D1 = 0, the condition of the upper bound of λ reduces to rl > ul, similar to
the condition for SSC in the noiseless case [12].
Based on Condition (8), under a randomized generative model, we can derive how many irrelevant
features can be tolerated.
Theorem 2 (Random model). Suppose there are L subspaces and for simplicity, all subspaces have
same dimension d and are chosen uniformly at random. For each subspace there are ρd+ 1 points
chosen independently and uniformly at random. Up to D1 features of data are irrelevant. Each
data point (including true and irrelevant features) is independent from other data points. Then for
some universal constants C1, C2, the subspace detection property holds with probability at least
1− 4N −N exp(−
√
ρd) if
d ≤ Dc
2(ρ) log(ρ)
12 logN
,
and
1
1
2c
2(ρ) log ρd − (
√
2c(ρ)
√
log ρ
d + 1)
C1D1(logD+C2 logN)
D
< λ <
1− κ
1 + κ
D
C1D1(logD + C2 logN)
,
where κ =
√
12d logN
Dc2(ρ) log ρ ; c(ρ) is a constant only depending on the density of data points on
subspace and satisfies (1) c(ρ) > 0 for all ρ > 1, (2) there is a numerical value ρ0, such that for all
ρ > ρ0, one can take c(ρ) = 1/
√
8.
Simplifying the above conditions, we can determine the number of irrelevant features that can be
tolerated. In particular, if d ≥ 2c2(ρ) log ρ and we choose the λ as
λ =
4d
c2(ρ) log ρ
,
then the maximal number of irrelevant feature D1 that can be torelated is
D1 = min{
c(ρ)D log ρ
8C1d(log(D) + C2 logN)
,
1− κ
1 + κ
C0Dc
2(ρ) log ρ
C1d(log(D) + C2 logN)
},
with probability at least 1− 4N −N exp(−
√
ρd).
If d ≤ 2c2(ρ) log ρ, and we choose the same λ, then the number of irrelevant feature we can tolerate
is
D1 = min{
Dc(ρ)
√
log ρ
d
4
√
2C1(log(D) + C2 logN)
,
1− κ
1 + κ
C0Dc
2(ρ) log ρ
C1d(log(D) + C2 logN)
},
with probability at least 1− 4N −N exp(−
√
ρd).
Remark 1. If D is much larger than D1, the lower bound of λ is proportional to the subspace
dimension d. When d increases, the upper bound of λ decreases, since 1−κ1+κ decreases. Thus the
valid range of λ shrinks when d increases.
Remark 2. Ignoring the logarithm terms, when d is large, the tolerable D1 is proportional to
min(C1
1−κ
1+κ
D
d , C2
D
d ). When d is small, D1 is proportional to min(C1
1−κ
1+κ
D
d , C2D/
√
d) .
4 Roadmap of the Proof
In this section, we lay out the roadmap of proof. In specific we want to establish the condition with
the number of irrelevant features, and the structure of data (i.e., the incoherence µ and inradius r)
for the algorithm to succeed. Indeed, we provide a lower bound of λ such that the optimal solution
ci is not trivial; and an upper bound of λ so that the Self-Expressiveness Property holds. Combining
them together established the theorems.
5
4.1 Self-Expressiveness Property
The Self-Expressiveness Property is related to the upper bound of λ. The proof technique is inspired
by [18] and [12], we first establish the following lemma, which provides a sufficient condition such
that Self-Expressiveness Property holds of the problem 6.
Lemma 1. Consider a matrix Σ ∈ RN×N and γ ∈ RN×1, If there exist a pair (c̃, ξ) such that c̃
has a support S ⊆ T and
sgn(c̃s) + Σs,ηξη = 0,
‖Σsc∩T,ηξη‖∞ ≤ 1,
‖ξ‖1 = λ,
‖ΣT c,ηξη‖∞ < 1,
(10)
where η is the set of indices of entry i such that |(Σc̃ − γ)i| = ‖Σc̃ − γ‖∞, then for all optimal
solution c∗ to the problem P (Σ, γ), we have c∗T c = 0.
The variable ξ in Lemma 1 is often termed the “dual certificate”. We next consider an oracle problem
P (Σ̂l,l, γ̂l), and use its dual optimal variable denoted by ξ̂, to construct such a dual certificate. This
candidate satisfies all conditions in the Lemma 1 automatically except to show
‖Σ̂lc,η̂ ξ̂η̂‖∞ < 1, (11)
where lc denotes the set of indices expect the ones corresponding to subspace l. We can compare
this condition with the corresponding one in analyzing SSC, in which one need ‖(X)(lc)T v‖∞ < 1,
where v is the dual certificate. Recall that we can decompose Σ̂lc,η̂ = (XA)(l
c)T (XA)η̂ + Σ̃lc,η̂ .
Thus Condition 11 becomes
‖(XA)(l
c)T ((XA)η̂ ξ̂η̂) + Σ̃lc,η̂ ξ̂η̂‖∞ < 1. (12)
To show this holds, we need to bound two terms ‖(XA)η̂ ξ̂η̂‖2 and ‖Σ̃lc,η̂ ξ̂η̂‖∞.
Bounding ‖Σ̃‖∞, ‖γ̃‖∞
The following lemma relates D1 with ‖Σ̃‖∞ and ‖γ̃‖∞.
Lemma 2. Suppose Σ̂ and γ̂ are robust counterparts ofXT−iX−i andXT−ixi respectively and among
D + D1 features, up to D1 are irrelevant. We can decompose Σ̂ and γ̂ into following form Σ̂ =
(XA)
T
−i(XA)−i + Σ̃ and γ̂ = (XA)
T
−i(xA)i + γ̃. We define δ1 := ‖γ̃‖∞ and δ2 := ‖Σ̃‖∞ .If
‖(xA)i‖∞ ≤ 1 and ‖(XA)−i‖∞ ≤ 2, then δ2 ≤ 2D122, δ1 ≤ 2D112.
We then bound 1 and 2 in the random model using the upper bound of the spherical cap [2].
Indeed we have 1 ≤ C1(logD + C2 logN)/
√
D and 2 ≤ C1(logD + C2 logN)/
√
D with high
probability.
Bounding ‖Xη̂ ξ̂η̂‖2
By exploiting the feasible condition in the dual of the oracle problem, we obtain the following
bound:
‖Xη̂ ξ̂η̂‖2 ≤
1 + 2D1λ
2
2
r(P l−i)
.
Furthermore, r(P l−i) can be lower bound by
c(ρ)√
2
√
log ρ
d and 2 can be upper bounded byC1(logD+
C2 logN)/
√
D in the random model with high probability. Thus the RHS can be upper bounded.
Plugging this upper bound into (12), we obtain the upper bound of λ.
4.2 Non-triviality with sufficiently large λ
To ensure that the solution is not trivial (i.e., not all-zero), we need a lower bound on λ.
6
If λ satisfies the following condition, the optimal solution to problem 6 can not be zero
λ >
1
r2(P l−i)− 2D122 − 4r(P l−i)D112
. (13)
The proof idea is to show when λ is large enough, the trivial solution c = 0 can not be optimal. In
particular, if c = 0, the corresponding value in the primal problem is λ‖γ̂l‖∞. We then establish a
lower bound of ‖γ̂l‖∞ and a upper bound of ‖c‖1 +λ‖Σ̂l,lc− γ̂l‖∞ so that the following inequality
always holds by some carefully choosen c.
‖c‖1 + λ‖Σ̂l,lc− γ̂l‖∞ < λ‖γ̂l‖∞. (14)
We then further lower bound the RHS of Equation (13) using the bound of 1, 2 and r(P l−i). Notice
that condition (14) requires that λ > A and condition (11) requires λ < B, whereA andB are some
terms depending on the number of irrelevant features. Thus we require A < B to get the maximal
number of irrelevant features that can be tolerated.
5 Numerical simulations
In this section, we use three numerical experiments to demonstrate the effectiveness of our method
to handle irrelevant/corrupted features. In particular, we test the performance of our method and
effect of number of irrelevant features and dimension subspaces d with respect to different λ. In
all experiments, the ambient dimension D = 200, sample density ρ = 5, the subspace are drawn
uniformly at random. Each subspace has ρd+1 points chosen independently and uniformly random.
We measure the success of the algorithms using the relative violation of the subspace detection
property defined as follows,
RelV iolation(C,M) =
∑
(i,j)/∈M |C|i,j∑
(i,j)∈M |C|i,j
,
where C = [c1, c2, ..., cN ],M is the ground truth mask containing all (i, j) such that xi, xj belong
to a same subspace. If RelV iolation(C,M) = 0, then the subspace detection property is satisfied.
We also check whether we obtain a trivial solution, i.e., if any column in C is all-zero.
We first compare the robust Dantzig selector(λ = 2) with SSC and LASSO-SSC ( λ = 10). The
results are shown in Figure 3. The X-axis is the number of irrelevant features and the Y-axis is the
Relviolation defined above. The ambient dimension D = 200, L = 3, d = 5, the relative sample
density ρ = 5. The values of irrelevant features are independently sampled from a uniform distribu-
tion in the region [−2.5, 2.5] in (a) and [−10, 10] in (b). We observe from Figure 3 that both SSC
and Lasso SSC are very sensitive to irrelevant information. (Notice that RelViolation=0.1 is pretty
large and can be considered as clustering failure.) Compared with that, the proposed Robust Dantzig
Selector performs very well. Even when D1 = 20, it still detects the true subspaces perfectly. In
the same setting, we do some further experiments, our method breaks when D1 is about 40. We
also do further experiment for Lasso-SSC with different λ in the supplementary material to show
Lasso-SSC is not robust to irrelevant features.
We also examine the relation of λ to the performance of the algorithm. In Figure 4a, we test the
subspace detection property with different λ and D1. When λ is too small, the algorithm gives a
trivial solution (the black region in the figure). As we increase the value of λ, the corresponding
solutions satisfy the subspace detection property (represented as the white region in the figure).
When λ is larger than certain upper bound, RelV iolation becomes non-zero, indicating errors in
subspace clustering. In Figure 4b, we test the subspace detection property with different λ and d.
Notice we rescale λ with d, since by Theorem 3, λ should be proportional to d. We observe that the
valid region of λ shrinks with increasing d which matches our theorem.
6 Conclusion and future work
We studied subspace clustering with irrelevant features, and proposed the “robust Dantzig selector”
based on the idea of robust inner product, essentially a truncated version of inner product to avoid
7
0 5 10 15 20
0
0.2
0.4
0.6
0.8
1
Number of irrelevant features
R
el
V
io
la
tio
n
 
 
Original SSC
Lasso SSC
Robust Dantzig Selector
(a)
0 5 10 15 20
0
0.2
0.4
0.6
0.8
1
Number of irrelevant features
R
el
V
io
la
tio
n
 
 
Original SSC
Lasso SSC
Robust Dantzig Selector
(b)
Figure 3: Relviolation with different D1. Simulated with D = 200, d = 5, L = 3, ρ = 5, λ = 2,
and D1 from 1 to 20.
Number of irrelevant features D1
λ
1 2 3 4 5 6 7 8 9 10
10.5
10  
9.5 
9   
8.5 
8   
7.5 
7   
6.5 
6   
5.5 
5   
4.5 
4   
3.5 
3   
2.5 
2   
1.5 
1   
(a) Exact recovery with different number of
irrelevant features. Simulated with D =
200, d = 5, L = 3, ρ = 5 with an in-
creasing D1 from 1 to 10. Black region:
trivial solution. White region: Non-trivial
solution with RelViolation=0. Gray region:
RelViolation> 0.02.
Subspace dimension d
λ
/d
4 6 8 10 12 14 16
2.5
2.3
2.1
1.9
1.7
1.5
1.3
1.1
0.9
0.7
0.5
0.3
0.1
(b) Exact recovery with different subspace
dimension d. Simulated with D = 200,
L = 3, ρ = 5, D1 = 5 and an increas-
ing d from 4 to 16. Black region: triv-
ial solution. White region: Non-trivial so-
lution with RelViolation=0. Gray region:
RelViolation> 0.02.
Figure 4: Subspace detection property with different λ, D1, d.
any single entry having too large influnce on the result. We established the sufficient conditions
for the algorithm to exactly detect the true subspace under the deterministic model and the random
model. Simulation results demonstrate that the proposed method is robust to irrelevant information
whereas the performance of original SSC and LASSO-SSC significantly deteriorates.
We now outline some directions of future research. An immediate future work is to study theoretical
guarantees of the proposed method under the semi-random model, where each subspace is chosen
deterministically, while samples are randomly distributed on the respective subspace. The challenge
here is to bound the subspace incoherence, previous methods uses the rotation invariance of the data,
which is not possible in our case as the robust inner product is invariant to rotations.
Acknowledgments
This work is partially supported by the Ministry of Education of Singapore AcRF Tier Two grant
R-265-000-443-112, and A*STAR SERC PSF grant R-265-000-540-305.
References
[1] Pankaj K Agarwal and Nabil H Mustafa. k-means projective clustering. In Proceedings of the
23rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages
8
155–165, 2004.
[2] Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry,
31:1–58, 1997.
[3] Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component
analysis? Journal of the ACM, 58(3):11, 2011.
[4] Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under ad-
versarial corruption. In Proceedings of the 30th International Conference on Machine Learn-
ing, pages 774–782, 2013.
[5] Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs
via convex optimization. The Journal of Machine Learning Research, 15(1):2213–2238, 2014.
[6] Ehsan Elhamifar and René Vidal. Sparse subspace clustering. In CVPR 2009, pages 2790–
2797.
[7] Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank rep-
resentation. In Proceedings of the 27th International Conference on Machine Learning, pages
663–670, 2010.
[8] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing
data: Provable guarantees with non-convexity. In Advances in Neural Information Processing
Systems, pages 2726–2734, 2011.
[9] Le Lu and René Vidal. Combined central and subspace clustering for computer vision ap-
plications. In Proceedings of the 23rd international conference on Machine learning, pages
593–600, 2006.
[10] Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data
via lossy data coding and compression. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29(9):1546–1562, 2007.
[11] Shankar R Rao, Roberto Tron, René Vidal, and Yi Ma. Motion segmentation via robust sub-
space separation in the presence of outlying, incomplete, or corrupted trajectories. In CVPR
2008.
[12] Mahdi Soltanolkotabi, Emmanuel J Candes, et al. A geometric analysis of subspace clustering
with outliers. The Annals of Statistics, 40(4):2195–2238, 2012.
[13] Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J Candes, et al. Robust subspace cluster-
ing. The Annals of Statistics, 42(2):669–699, 2014.
[14] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical
Society. Series B, pages 267–288, 1996.
[15] René Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine, 28(2):52–
68, 2010.
[16] Rene Vidal, Yi Ma, and Shankar Sastry. Generalized principal component analysis (gpca).
IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(12):1945–1959, 2005.
[17] René Vidal, Roberto Tron, and Richard Hartley. Multiframe motion segmentation with missing
data using powerfactorization and gpca. International Journal of Computer Vision, 79(1):85–
105, 2008.
[18] Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In Proceedings of The 30th
International Conference on Machine Learning, pages 89–97, 2013.
[19] H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE Transactions on
Information Theory, 58(5):3047–3064, 2012.
[20] Jingyu Yan and Marc Pollefeys. A general framework for motion segmentation: Independent,
articulated, rigid, non-rigid, degenerate and non-degenerate. In ECCV 2006, pages 94–106.
2006.
[21] Hao Zhu, Geert Leus, and Georgios B Giannakis. Sparsity-cognizant total least-squares for
perturbed compressive sampling. IEEE Transactions on Signal Processing, 59(5):2002–2016,
2011.
9
