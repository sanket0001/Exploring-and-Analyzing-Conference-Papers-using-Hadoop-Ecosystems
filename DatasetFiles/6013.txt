


Paper ID = 6013
Title = Optimal Linear Estimation under Unknown
Nonlinear Transform
Xinyang Yi
The University of Texas at Austin
yixy@utexas.edu
Zhaoran Wang
Princeton University
zhaoran@princeton.edu
Constantine Caramanis
The University of Texas at Austin
constantine@utexas.edu
Han Liu
Princeton University
hanliu@princeton.edu
Abstract
Linear regression studies the problem of estimating a model parameter β∗∈Rp,
from n observations {(yi,xi)}ni=1 from linear model yi = 〈xi,β∗〉 + i. We
consider a significant generalization in which the relationship between 〈xi,β∗〉
and yi is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as
well as unknown. This model is known as the single-index model in statistics, and,
among other things, it represents a significant generalization of one-bit compressed
sensing. We propose a novel spectral-based estimation procedure and show that
we can recover β∗ in settings (i.e., classes of link function f ) where previous
algorithms fail. In general, our algorithm requires only very mild restrictions on the
(unknown) functional relationship between yi and 〈xi,β∗〉. We also consider the
high dimensional setting where β∗ is sparse, and introduce a two-stage nonconvex
framework that addresses estimation challenges in high dimensional regimes where
p n. For a broad class of link functions between 〈xi,β∗〉 and yi, we establish
minimax lower bounds that demonstrate the optimality of our estimators in both
the classical and high dimensional regimes.
1 Introduction
We consider a generalization of the one-bit quantized regression problem, where we seek to recover
the regression coefficient β∗ ∈ Rp from one-bit measurements. Specifically, suppose that X is a
random vector in Rp and Y is a binary random variable taking values in {−1, 1}. We assume the
conditional distribution of Y givenX takes the form
P(Y = 1|X = x) = 1
2
f(〈x,β∗〉) + 1
2
, (1.1)
where f : R→ [−1, 1] is called the link function. We aim to estimate β∗ from n i.i.d. observations
{(yi,xi)}ni=1 of the pair (Y,X). In particular, we assume the link function f is unknown. Without
any loss of generality, we take β∗ to be on the unit sphere Sp−1 since its magnitude can always be
incorporated into the link function f .
The model in (1.1) is simple but general. Under specific choices of the link function f , (1.1) immedi-
ately leads to many practical models in machine learning and signal processing, including logistic
regression and one-bit compressed sensing. In the settings where the link function is assumed to
be known, a popular estimation procedure is to calculate an estimator that minimizes a certain loss
1
function. However, for particular link functions, this approach involves minimizing a nonconvex
objective function for which the global minimizer is in general intractable to obtain. Furthermore, it
is difficult or even impossible to know the link function in practice, and a poor choice of link function
may result in inaccurate parameter estimation and high prediction error. We take a more general
approach, and in particular, target the setting where f is unknown. We propose an algorithm that can
estimate the parameter β∗ in the absence of prior knowledge on the link function f . As our results
make precise, our algorithm succeeds as long as the function f satisfies a single moment condition.
As we demonstrate, this moment condition is only a mild restriction on f . In particular, our methods
and theory are widely applicable even to the settings where f is non-smooth, e.g., f(z) = sign(z), or
noninvertible, e.g., f(z) = sin(z).
In particular, as we show in §2, our restrictions on f are sufficiently flexible so that our results provide
a unified framework that encompasses a broad range of problems, including logistic regression,
one-bit compressed sensing, one-bit phase retrieval as well as their robust extensions. We use these
important examples to illustrate our results, and discuss them at several points throughout the paper.
Main contributions. The key conceptual contribution of this work is a novel use of the method of
moments. Rather than considering moments of the covariate,X , and the response variable, Y , we
look at moments of differences of covariates, and differences of response variables. Such a simple yet
critical observation enables everything that follows and leads to our spectral-based procedure.
We also make two theoretical contributions. First, we simultaneously establish the statistical and
computational rates of convergence of the proposed spectral algorithm. We consider both the low
dimensional setting where the number of samples exceeds the dimension and the high dimensional
setting where the dimensionality may (greatly) exceed the number of samples. In both these settings,
our proposed algorithm achieves the same statistical rate of convergence as that of linear regression
applied on data generated by the linear model without quantization. Second, we provide minimax
lower bounds for the statistical rate of convergence, and thereby establish the optimality of our
procedure within a broad model class. In the low dimensional setting, our results obtain the optimal
rate with the optimal sample complexity. In the high dimensional setting, our algorithm requires
estimating a sparse eigenvector, and thus our sample complexity coincides with what is believed to
be the best achievable via polynomial time methods [2]; the error rate itself, however, is information-
theoretically optimal. We discuss this further in §3.4.
Related works. Our model in (1.1) is close to the single-index model (SIM) in statistics. In the SIM,
we assume that the response-covariate pair (Y,X) is determined by
Y = f(〈X,β∗〉) +W (1.2)
with unknown link function f and noise W . Our setting is a special case of this, as we restrict Y
to be a binary random variable. The single index model is a classical topic, and therefore there is
extensive literature – too much to exhaustively review it. We therefore outline the pieces of work most
relevant to our setting and our results. For estimating β∗ in (1.2), a feasible approach is M -estimation
[8, 9, 12], in which the unknown link function f is jointly estimated using nonparametric estimators.
Although these M -estimators have been shown to be consistent, they are not computationally efficient
since they involve solving a nonconvex optimization problem. Another approach to estimate β∗ is
named the average derivative estimator (ADE; [24]). Further improvements of ADE are considered
in [13, 22]. ADE and its related methods require that the link function f is at least differentiable, and
thus excludes important models such as one-bit compressed sensing with f(z) = sign(z). Beyond
estimating β∗, the works in [15, 16] focus on iteratively estimating a function f and vector β that
are good for prediction, and they attempt to control the generalization error. Their algorithms are
based on isotonic regression, and are therefore only applicable when the link function is monotonic
and satisfies Lipschitz constraints. The work discussed above focuses on the low dimensional setting
where p n. Another related line of works is sufficient dimension reduction, where the goal is to
find a subspace U of the input space such that the response Y only depends on the projection U>X .
Single-index model and our problem can be regarded as special cases of this problem as we are
primarily interested in recovering a one-dimensional subspace. Due to space limit, we refer readers to
the long version of this paper for a detailed survey [29].
2
In the high dimensional regime with p n and β∗ has some structure (for us this means sparsity),
we note there exists some recent progress [1] on estimating f via PAC Bayesian methods. In the
special case when f is linear function, sparse linear regression has attracted extensive study over
the years. The recent work by Plan et al. [21] is closest to our setting. They consider the setting of
normal covariates,X ∼ N (0, Ip), and they propose a marginal regression estimator for estimating
β∗, that, like our approach, requires no prior knowledge about f . Their proposed algorithm relies
on the assumption that Ez∼N (0,1)
[
zf(z)
]
6= 0, and hence cannot work for link functions that are
even. As we will describe below, our algorithm is based on a novel moment-based estimator, and
avoids requiring such a condition, thus allowing us to handle even link functions under a very mild
moment restriction, which we describe in detail below. Generally, the work in [21] requires different
conditions, and thus beyond the discussion above, is not directly comparable to the work here. In
cases where both approaches apply, the results are minimax optimal.
2 Example models
In this section, we discuss several popular (and important) models in machine learning and signal
processing that fall into our general model (1.1) under specific link functions. Variants of these models
have been studied extensively in the recent literature. These examples trace through the paper, and we
use them to illustrate the details of our algorithms and results.
Logistic regression. In logistic regression (LR), we assume that P(Y = 1|X = x) =
1
1+exp (−〈x,β∗〉−ζ) , where ζ is the intercept. The link function corresponds to f(z) =
exp (z+ζ)−1
exp (z+ζ)+1 .
One robust variant of LR is called flipped logistic regression, where we assume that the labels
Y generated from standard LR model are flipped with probability pe, i.e., P(Y = 1|X = x) =
1−pe
1+exp (−〈x,β∗〉−ζ) +
pe
1+exp (〈x,β∗〉+ζ) . This reduces to the standard LR model when pe = 0. For
flipped LR, the link function f can be written as
f(z) =
exp (z + ζ)− 1
exp (z + ζ) + 1
+ 2pe ·
1− exp (z + ζ)
1 + exp (z + ζ)
. (2.1)
Flipped LR has been studied by [19, 25]. In both papers, estimating β∗ is based on minimizing some
surrogate loss function involving a certain tuning parameter connected to pe. However, pe is unknown
in practice. In contrast to their approaches, our method does not hinge on the unknown parameter pe.
Our approach has the same formulation for both standard and flipped LR, thus unifies the two models.
One-bit compressed sensing. One-bit compressed sensing (CS) aims at recovering sparse signals
from quantized linear measurements (see e.g., [11, 20]). In detail, we define B0(s, p) := {β ∈ Rp :
| supp(β)| ≤ s} as the set of sparse vectors in Rp with at most s nonzero elements. We assume
(Y,X) ∈ {−1, 1} × Rp satisfies
Y = sign(〈X,β∗〉), (2.2)
where β∗ ∈ B0(s, p). In this paper, we also consider its robust version with noise , i.e., Y =
sign(〈X,β∗〉+ ). Assuming  ∼ N (0, σ2), the link function f of robust 1-bit CS thus corresponds
to
f(z) = 2
∫ ∞
0
1√
2πσ
e−(u−z)
2/2σ2du− 1. (2.3)
Note that (2.2) also corresponds to the probit regression model without the sparse constraint on β∗.
Throughout the paper, we do not distinguish between the two model names. Model (2.2) is referred
to as one-bit compressed sensing even in the case where β∗ is not sparse.
One-bit phase retrieval. The goal of phase retrieval (e.g., [5]) is to recover signals based on linear
measurements with phase information erased, i.e., pair (Y,X) ∈ R× Rp is determined by equation
Y = |〈X,β∗〉|. Analogous to one-bit compressed sensing, we consider a new model named one-bit
phase retrieval where the linear measurement with phase information erased is quantized to one bit.
In detail, pair (Y,X) ∈ {−1, 1} × Rp is linked through Y = sign(|〈X,β∗〉| − θ), where θ is the
quantization threshold. Compared with one-bit compressed sensing, this problem is more difficult
because Y only depends on β∗ through the magnitude of 〈X,β∗〉 instead of the value of 〈X,β∗〉.
Also, it is more difficult than the original phase retrieval problem due to the additional quantization.
3
Using our general model, The link function thus corresponds to
f(z) = sign(|z| − θ). (2.4)
It is worth noting that, unlike previous models, here f is neither odd nor monotonic.
3 Main results
We now turn to our algorithms for estimating β∗ in both low and high dimensional settings. We first
introduce a second moment estimator based on pairwise differences. We prove that the eigenstructure
of the constructed second moment estimator encodes the information of β∗. We then propose
algorithms to estimate β∗ based upon this second moment estimator. In the high dimensional setting
where β∗ is sparse, computing the top eigenvector of our pairwise-difference matrix reduces to
computing a sparse eigenvector. Beyond algorithms, we discuss minimax lower bound in §3.5. We
present simulation results in §3.6
3.1 Conditions for success
We now introduce several key quantities, which allow us to state precisely the conditions required for
the success of our algorithm.
Definition 3.1. For any (unknown) link function, f , define the quantity φ(f) as follows:
φ(f) := µ21 − µ0µ2 + µ20. (3.1)
where µ0, µ1 and µ2 are given by
µk := E
[
f(Z)Zk
]
, k = 0, 1, 2 . . . , (3.2)
where Z ∼ N (0, 1).
As we discuss in detail below, the key condition for success of our algorithm is φ(f) 6= 0. As we
show below, this is a relatively mild condition, and in particular, it is satisfied by the three examples
introduced in §2. For odd and monotonic f , φ(f) > 0 unless f(z) = 0 for all z in which case no
algorithm is able to recover β∗. For even f , we have µ1 = 0. Thus φ(f) 6= 0 if and only if µ0 6= µ2.
3.2 Second moment estimator
We describe a novel moment estimator that enables our algorithm. Let {(yi,xi)}ni=1 be the n i.i.d.
observations of (Y,X). Assuming without loss of generality that n is even, we consider the following
key transformation
∆yi := y2i − y2i−1, ∆xi := x2i − x2i−1, (3.3)
for i = 1, 2, ..., n/2. Our procedure is based on the following second moment
M :=
2
n
n/2∑
i=1
∆y2i∆xi∆x
>
i ∈ Rp×p. (3.4)
The intuition behind this second moment is as follows. By (1.1), the variation ofX along the direction
β∗ has the largest impact on the variation of 〈X,β∗〉. Thus, the variation of Y directly depends
on the variation ofX along β∗. Consequently, {(∆yi,∆xi)}n/2i=1 encodes the information of such a
dependency relationship. In the following, we make this intuition more rigorous by analyzing the
eigenstructure of E(M) and its relationship with β∗.
Lemma 3.2. For β∗ ∈ Sp−1, we assume that (Y,X) ∈ {−1, 1} × Rp satisfies (1.1). For X ∼
N (0, Ip), we have
E(M) = 4φ(f) · β∗β∗> + 4(1− µ20) · Ip, (3.5)
where µ0 and φ(f) are defined in (3.2) and (3.1).
Lemma 3.2 proves that β∗ is the leading eigenvector of E(M) as long as the eigengap φ(f) is positive.
If instead we have φ(f) < 0, we can use a related moment estimator which has analogous properties.
4
To this end, define M′ := 2n
∑n/2
i=1(y2i + y2i−1)
2∆xi∆x
>
i . In parallel to Lemma 3.2, we have a
similar result for M′ as stated below.
Corollary 3.3. Under the setting of Lemma 3.2,
E(M′) = −4φ(f) · β∗β∗> + 4(1 + µ20) · Ip.
Corollary 3.3 therefore shows that when φ(f) < 0, we can construct another second moment estimator
M′ such that β∗ is the leading eigenvector of E(M′). As discussed above, this is precisely the setting
for one-bit phase retrieval when the quantization threshold in (3.1) satisfies θ < θm. For simplicity of
the discussion, hereafter we assume that φ(f) > 0 and focus on the second moment estimator M
defined in (3.4).
A natural question to ask is whether φ(f) 6= 0 holds for specific models. The following lemma
demonstrates exactly this, for the example models introduced in §2.
Lemma 3.4. (a) Consider the flipped logistic regression where f is given in (2.1). By setting the
intercept to be ζ = 0, we have φ(f) & (1−2pe)2. (b) For robust one-bit compressed sensing where f
is given in (2.3). We have φ(f) & min
{(
1−σ2
1+σ2
)2
, C
′σ4
(1+σ3)2
}
. (c) For one-bit phase retrieval where
f is given in (2.4). For Z ∼ N (0, 1), we let θm be the median of |Z|, i.e., P(|Z| ≥ θm) = 1/2. We
have |φ(f)| & θ|θ − θm| exp(−θ2) and sign[φ(f)] = sign(θ − θm). We thus obtain φ(f) > 0 for
θ > θm.
3.3 Low dimensional recovery
We consider estimating β∗ in the classical (low dimensional) setting where p  n. Based on the
second moment estimator M defined in (3.4), estimating β∗ amounts to solving a noisy eigenvalue
problem. We solve this by a simple iterative algorithm: provided an initial vector β0 ∈ Sp−1 (which
may be chosen at random) we perform power iterations as shown in Algorithm 1.
Theorem 3.5. We assume X ∼ N (0, Ip) and (Y,X) follows (1.1). Let {(yi,xi)}ni=1 be n i.i.d.
samples of response input pair (Y,X). For any link function f in (1.1) with µ0, φ(f) defined in (3.2)
and (3.1), and φ(f) > 01. We let
γ :=
[
1− µ20
φ(f) + 1− µ20
+ 1
]/
2, and ξ :=
γφ(f) + (γ − 1)(1− µ20)
(1 + γ)
[
φ(f) + 1− µ20
] . (3.6)
There exist constant Ci such that when n ≥ C1p/ξ2, for Algorithm 1, we have that with probability
at least 1− 2 exp(−C2p),∥∥βt − β∗∥∥
2
≤ C3 ·
φ(f) + 1− µ20
φ(f)
·
√
p
n︸ ︷︷ ︸
Statistical Error
+
√
1− α2
α2
· γt︸ ︷︷ ︸
Optimization Error
, for t = 1, . . . , Tmax. (3.7)
Here α =
〈
β0, β̂
〉
, where β̂ is the first leading eigenvector of M.
Note that by (3.6) we have γ ∈ (0, 1). Thus, the optimization error term in (3.7) decreases at
a geometric rate to zero as t increases. For Tmax sufficiently large such that the statistical error
and optimization error terms in (3.7) are of the same order, we have
∥∥βTmax − β∗∥∥
2
.
√
p/n.
This statistical rate of convergence matches the rate of estimating a p-dimensional vector in linear
regression without any quantization, and will later be shown to be optimal. This result shows that the
lack of prior knowledge on the link function and the information loss from quantization do not keep
our procedure from obtaining the optimal statistical rate.
3.4 High dimensional recovery
Next we consider the high dimensional setting where p  n and β∗ is sparse, i.e., β∗ ∈ Sp−1 ∩
B0(s, p) with s being support size. Although this high dimensional estimation problem is closely
1Recall that we have an analogous treatment and thus results for φ(f) < 0.
5
related to the well-studied sparse PCA problem, the existing works [4, 6, 17, 23, 27, 28, 31, 32] on
sparse PCA do not provide a direct solution to our problem. In particular, they either lack statistical
guarantees on the convergence rate of the obtained estimator [6, 23, 28] or rely on the properties of
the sample covariance matrix of Gaussian data [4, 17], which are violated by the second moment
estimator defined in (3.4). For the sample covariance matrix of sub-Gaussian data, [27] prove that the
convex relaxation proposed by [7] achieves a suboptimal s
√
log p/n rate of convergence. Yuan and
Zhang [31] propose the truncated power method, and show that it attains the optimal
√
s log p/n rate
Algorithm 1 Low dimensional recovery
Input {(yi,xi)}ni=1, number of iterations Tmax
1: Second moment estimation: Construct M
from samples according to (3.4).
2: Initialization: Choose a random vectorβ0 ∈
Sn−1
3: For t = 1, 2, . . . , Tmax do
4: βt ←M · βt−1
5: βt ← βt/‖βt‖2
6: end For
Output βTmax
locally; that is, it exhibits this rate of convergence
only in a neighborhood of the true solution where
〈β0,β∗〉 > C where C > 0 is some constant. It
is well understood that for a random initialization
on Sp−1, such a condition fails with probability
going to one as p→∞.
Algorithm 2 Sparse recovery
Input {(yi,xi)}ni=1, number of iterations Tmax,
regularization parameter ρ, sparsity level ŝ.
1: Second moment estimation: Construct M
from samples according to (3.4).
2: Initialization:
3: Π0 ← argmin
Π∈Rp×p
{−〈M,Π〉+ ρ‖Π‖1,1
|Tr(Π) = 1,0  Π  I} (3.8)
4: β0 ← first leading eigenvector of Π0
5: β0 ← trunc(β0, ŝ)
6: β0 ← β0/‖β0‖2
7: For t = 1, 2, . . . , Tmax do
8: βt ← trunc(M · βt−1, ŝ)
9: βt ← βt/‖βt‖2
10: end For
Output βTmax
Instead, we propose a two-stage procedure for estimating β∗ in our setting. In the first stage, we adapt
the convex relaxation proposed by [27] and use it as an initialization step, in order to obtain a good
enough initial point satisfying the condition 〈β0,β∗〉 > C. The convex optimization problem can be
easily solved by the alternating direction method of multipliers (ADMM) algorithm (see [3, 27] for
details). Then we adapt the truncated power method. This procedure is illustrated in Algorithm 2. In
particular, we define truncation operator trunc(·, ·) as [trunc(β, s)]j = 1(j ∈ S)βj , where S is the
index set corresponding to the top s largest |βj |. The initialization phase of our algorithm requires
O(s2 log p) samples (see below for more precise details) to succeed. As work in [2] suggests, it is
unlikely that a polynomial time algorithm can avoid such dependence. However, once we are near the
solution, as we show, this two-step procedure achieves the optimal error rate of
√
s log p/n.
Theorem 3.6. Let
κ :=
[
4(1− µ20) + φ(f)
]/[
4(1− µ20) + 3φ(f)
]
< 1, (3.9)
and the minimum sample size be
nmin := C · s2 log p · φ(f)2 ·min
{
κ(1− κ1/2)/2, κ/8
}/ [
(1− µ20) + φ(f)
]2
. (3.10)
Suppose ρ=C
[
φ(f)+(1−µ20)
]√
log p/n with a sufficiently large constant C, where φ(f) and µ0
are specified in (3.2) and (3.5). Meanwhile, assume the sparsity parameter ŝ in Algorithm 2 is set to
be ŝ=C ′′max
{⌈
1/(κ−1/2−1)2
⌉
,1
}
·s∗. For n ≥ nmin with nmin defined in (3.10), we have
‖βt − β∗‖2 ≤ C ·
[
φ(f) + (1− µ20)
] 5
2 (1− µ20)
1
2
φ(f)3
·
√
s log p
n︸ ︷︷ ︸
Statistical Error
+κt ·
√
min
{
(1− κ1/2)/2, 1/8
}︸ ︷︷ ︸
Optimization Error
(3.11)
with high probability. Here κ is defined in (3.9).
The first term on the right-hand side of (3.11) is the statistical error while the second term gives the
optimization error. Note that the optimization error decays at a geometric rate since κ < 1. For Tmax
6
sufficiently large, we have ∥∥βTmax − β∗∥∥
2
.
√
s log p/n.
In the sequel, we show that the right-hand side gives the optimal statistical rate of convergence for a
broad model class under the high dimensional setting with p n.
3.5 Minimax lower bound
We establish the minimax lower bound for estimating β∗ in the model defined in (1.1). In the sequel
we define the family of link functions that are Lipschitz continuous and are bounded away from ±1.
Formally, for any m ∈ (0, 1) and L > 0, we define
F(m,L) :=
{
f : |f(z)| ≤ 1−m, |f(z)− f(z′)| ≤ L|z − z′|, for all z, z′ ∈ R
}
. (3.12)
Let Xnf := {(yi,xi)}ni=1 be the n i.i.d. realizations of (Y,X), where X follows N (0, Ip) and Y
satisfies (1.1) with link function f . Correspondingly, we denote the estimator of β∗ ∈ B to be β̂(Xnf ),
where B is the domain of β∗. We define the minimax risk for estimating β∗ as
R(n,m,L,B) := inf
f∈F(m,L)
inf
β̂(Xnf )
sup
β∗∈B
E
∥∥β̂(Xnf )− β∗∥∥2. (3.13)
In the above definition, we not only take the infimum over all possible estimators β̂, but also all
possible link functions in F(m,L). For a fixed f , our formulation recovers the standard definition
of minimax risk [30]. By taking the infimum over all link functions, our formulation characterizes
the minimax lower bound under the least challenging f in F(m,L). In the sequel we prove that our
procedure attains such a minimax lower bound for the least challenging f given any unknown link
function in F(m,L). That is to say, even when f is unknown, our estimation procedure is as accurate
as in the setting where we are provided the least challenging f , and the achieved accuracy is not
improvable due to the information-theoretic limit. The following theorem establishes the minimax
lower bound in the high dimensional setting.
Theorem 3.7. LetB=Sp−1∩B0(s, p). We assume thatn>m(1−m)/(2L2)2·
[
Cs log(p/s)/2−log 2
]
.
For any s ∈ (0, p/4], the minimax risk defined in (3.13) satisfies
R(n,m,L,B) ≥ C ′ ·
√
m(1−m)
L
·
√
s log(p/s)
n
.
Here C and C ′ are absolute constants, while m and L are defined in (3.12).
Theorem 3.7 establishes the minimax optimality of the statistical rate attained by our procedure for
pn and s-sparse β∗. In particular, for arbitrary f ∈ F(m,L) ∩ {f : φ(f) > 0}, the estimator β̂
attained by Algorithm 2 is minimax-optimal in the sense that its
√
s log p/n rate of convergence is
not improvable, even when the information on the link function f is available. For general β∗ ∈ Rp,
one can show the best possible convergence rate is Ω(
√
m(1−m)p/n/L) by setting s = p/4 in
Theorem 3.7.
It is worth to note that our lower bound becomes trivial for m = 0, i.e., there exists some z such
that |f(z)| = 1. One example is the noiseless one-bit compressed sensing for which we have
f(z) = sign(z). In fact, for noiseless one-bit compressed sensing, the
√
s log p/n rate is not optimal.
For example, the Jacques et al. [14] provide an algorithm (with exponential running time) that achieves
rate s log p/n. Understanding such a rate transition phenomenon for link functions with zero margin,
i.e., m = 0 in (3.12), is an interesting future direction.
3.6 Numerical results
We now turn to the numerical results that support our theory. For the three models introduced in §2,
we apply Algorithm 1 and Algorithm 2 to do parameter estimation in the classic and high dimensional
regimes. Our simulations are based on synthetic data. For classic recovery, β∗ is randomly chosen
from Sp−1; for sparse recovery, we set β∗j = s−1/21(j ∈ S) for all j ∈ [p], where S is a random
index subset of [p] with size s. In Figure 1, as predicted by Theorem 3.5, we observe that the same
7
√
p/n leads to nearly identical estimation error. Figure 2 demonstrates similar results for the predicted
rate
√
s log p/n of sparse recovery and thus validates Theorem 3.6.
√
p/n
0.05 0.1 0.15 0.2
E
s
t
im
a
t
io
n
E
r
r
o
r
0.2
0.3
0.4
0.5
0.6
0.7
p = 10
p = 20
p = 40
(a) Flipped Logistic Regression
√
p/n
0.05 0.1 0.15 0.2
E
s
t
im
a
t
io
n
E
r
r
o
r
0.2
0.3
0.4
0.5
p = 10
p = 20
p = 40
(b) One-bit Compressed Sensing
√
p/n
0.05 0.1 0.15 0.2
E
s
t
im
a
t
io
n
E
r
r
o
r
0.2
0.3
0.4
0.5
0.6
0.7
p = 10
p = 20
p = 40
(c) One-bit Phase Retrieval
Figure 1: Estimation error of low dimensional recovery. (a) pe = 0.1. (b) δ2 = 0.1. (c) θ = 1.
√
s log p/n
0.1 0.2 0.3 0.4
E
st
im
a
ti
o
n
E
rr
o
r
0.2
0.4
0.6
0.8
1
1.2
p = 100, s = 5
p = 100, s = 10
p = 200, s = 5
p = 200, s = 10
(a) Flipped Logistic Regression
√
s log p/n
0.1 0.2 0.3 0.4
E
st
im
a
ti
o
n
E
rr
o
r
0
0.2
0.4
0.6
0.8
p = 100, s = 5
p = 100, s = 10
p = 200, s = 5
p = 200, s = 10
(b) One-bit Compressed Sensing
√
s log p/n
0.1 0.15 0.2 0.25 0.3
E
st
im
a
ti
o
n
E
rr
o
r
0
0.2
0.4
0.6
0.8
1
p = 100, s = 5
p = 100, s = 10
p = 200, s = 5
p = 200, s = 10
(c) One-bit Phase Retrieval
Figure 2: Estimation error of sparse recovery. (a) pe = 0.1. (b) δ2 = 0.1. (c) θ = 1.
4 Discussion
Sample complexity. In high dimensional regime, while our algorithm achieves optimal convergence
rate, the sample complexity we need is Ω(s2 log p). The natural question is whether it can be reduced
to O(s log p). We note that breaking the barrier s2 log p is challenging. Consider a simpler problem
sparse phase retrieval where yi = |〈xi,β∗〉|, with a fairly extensive body of literature, the state-of-
the-art efficient algorithms (i.e., with polynomial running time) for recovering sparse β∗ requires
sample complexity Ω(s2 log p) [10]. It remains open to show whether it’s possible to do consistent
sparse recovery with O(s log p) samples by any polynomial time algorithms.
Acknowledgment
XY and CC would like to acknowledge NSF grants 1056028, 1302435 and 1116955. This research
was also partially supported by the U.S. Department of Transportation through the Data-Supported
Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center. HL is
grateful for the support of NSF CAREER Award DMS1454377, NSF IIS1408910, NSF IIS1332109,
NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841. ZW was partially supported by
MSR PhD fellowship while this work was done.
References
[1] A L Q U I E R , P. and B I A U , G . (2013). Sparse single-index model. Journal of Machine Learning
Research, 14 243–280.
[2] B E R T H E T , Q . and R I G O L L E T , P. (2013). Complexity theoretic lower bounds for sparse principal
component detection. In Conference on Learning Theory.
[3] B O Y D , S ., PA R I K H , N ., C H U , E ., P E L E AT O , B . and E C K S T E I N , J . (2011). Distributed
optimization and statistical learning via the alternating direction method of multipliers. Foundations and
Trends R© in Machine Learning, 3 1–122.
8
[4] C A I , T. T., M A , Z . and W U , Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. Annals
of Statistics, 41 3074–3110.
[5] C A N D È S , E . J ., E L D A R , Y. C ., S T R O H M E R , T. and V O R O N I N S K I , V. (2013). Phase retrieval
via matrix completion. SIAM Journal on Imaging Sciences, 6 199–225.
[6] D ’ A S P R E M O N T , A ., B A C H , F. and E L G H A O U I , L . (2008). Optimal solutions for sparse principal
component analysis. Journal of Machine Learning Research, 9 1269–1294.
[7] D ’ A S P R E M O N T , A ., E L G H A O U I , L ., J O R D A N , M . I . and L A N C K R I E T , G . R . (2007). A
direct formulation for sparse PCA using semidefinite programming. SIAM Review 434–448.
[8] D E L E C R O I X , M ., H R I S TA C H E , M . and PAT I L E A , V. (2000). Optimal smoothing in semiparametric
index approximation of regression functions. Tech. rep., Interdisciplinary Research Project: Quantification
and Simulation of Economic Processes.
[9] D E L E C R O I X , M ., H R I S TA C H E , M . and PAT I L E A , V. (2006). On semiparametric M -estimation in
single-index regression. Journal of Statistical Planning and Inference, 136 730–769.
[10] E L D A R , Y. C . and M E N D E L S O N , S . (2014). Phase retrieval: Stability and recovery guarantees.
Applied and Computational Harmonic Analysis, 36 473–494.
[11] G O P I , S ., N E T R A PA L L I , P., JA I N , P. and N O R I , A . (2013). One-bit compressed sensing: Provable
support and vector recovery. In International Conference on Machine Learning.
[12] H A R D L E , W., H A L L , P. and I C H I M U R A , H . (1993). Optimal smoothing in single-index models.
Annals of Statistics, 21 157–178.
[13] H R I S TA C H E , M ., J U D I T S K Y, A . and S P O K O I N Y, V. (2001). Direct estimation of the index
coefficient in a single-index model. Annals of Statistics, 29 pp. 595–623.
[14] JA C Q U E S , L ., L A S K A , J . N ., B O U F O U N O S , P. T. and B A R A N I U K , R . G . (2011). Robust 1-bit
compressive sensing via binary stable embeddings of sparse vectors. arXiv preprint arXiv:1104.3160.
[15] K A K A D E , S . M ., K A N A D E , V., S H A M I R , O . and K A L A I , A . (2011). Efficient learning of
generalized linear and single index models with isotonic regression. In Advances in Neural Information
Processing Systems.
[16] K A L A I , A . T. and S A S T RY, R . (2009). The isotron algorithm: High-dimensional isotonic regression.
In Conference on Learning Theory.
[17] M A , Z . (2013). Sparse principal component analysis and iterative thresholding. The Annals of Statistics,
41 772–801.
[18] M A S S A R T , P. and P I C A R D , J . (2007). Concentration inequalities and model selection, vol. 1896.
Springer.
[19] N ATA R A J A N , N ., D H I L L O N , I ., R AV I K U M A R , P. and T E WA R I , A . (2013). Learning with noisy
labels. In Advances in Neural Information Processing Systems.
[20] P L A N , Y. and V E R S H Y N I N , R . (2013). One-bit compressed sensing by linear programming. Commu-
nications on Pure and Applied Mathematics, 66 1275–1297.
[21] P L A N , Y., V E R S H Y N I N , R . and Y U D O V I N A , E . (2014). High-dimensional estimation with
geometric constraints. arXiv preprint arXiv:1404.3749.
[22] P O W E L L , J . L ., S T O C K , J . H . and S T O K E R , T. M . (1989). Semiparametric estimation of index
coefficients. Econometrica, 57 pp. 1403–1430.
[23] S H E N , H . and H U A N G , J . (2008). Sparse principal component analysis via regularized low rank matrix
approximation. Journal of Multivariate Analysis, 99 1015–1034.
[24] S T O K E R , T. M . (1986). Consistent estimation of scaled coefficients. Econometrica, 54 pp. 1461–1481.
[25] T I B S H I R A N I , J . and M A N N I N G , C . D . (2013). Robust logistic regression using shift parameters.
arXiv preprint arXiv:1305.4987.
[26] V E R S H Y N I N , R . (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027.
[27] V U , V. Q ., C H O , J ., L E I , J . and R O H E , K . (2013). Fantope projection and selection: A near-optimal
convex relaxation of sparse PCA. In Advances in Neural Information Processing Systems.
[28] W I T T E N , D ., T I B S H I R A N I , R . and H A S T I E , T. (2009). A penalized matrix decomposition, with
applications to sparse principal components and canonical correlation analysis. Biostatistics, 10 515–534.
[29] Y I , X ., WA N G , Z ., C A R A M A N I S , C . and L I U , H . (2015). Optimal linear estimation under unknown
nonlinear transform. arXiv preprint arXiv:1505.03257.
[30] Y U , B . (1997). Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam. Springer, 423–435.
[31] Y U A N , X . - T. and Z H A N G , T. (2013). Truncated power method for sparse eigenvalue problems.
Journal of Machine Learning Research, 14 899–925.
[32] Z O U , H ., H A S T I E , T. and T I B S H I R A N I , R . (2006). Sparse principal component analysis. Journal
of Computational and Graphical Statistics, 15 265–286.
9
