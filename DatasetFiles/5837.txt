


Paper ID = 5837
Title = Empirical Localization of Homogeneous Divergences
on Discrete Sample Spaces
Takashi Takenouchi
Department of Complex and Intelligent Systems
Future University Hakodate
116-2 Kamedanakano, Hakodate, Hokkaido, 040-8655, Japan
ttakashi@fun.ac.jp
Takafumi Kanamori
Department of Computer Science and Mathematical Informatics
Nagoya University
Furocho, Chikusaku, Nagoya 464-8601, Japan
kanamori@is.nagoya-u.ac.jp
Abstract
In this paper, we propose a novel parameter estimator for probabilistic models on
discrete space. The proposed estimator is derived from minimization of homoge-
neous divergence and can be constructed without calculation of the normalization
constant, which is frequently infeasible for models in the discrete space. We in-
vestigate statistical properties of the proposed estimator such as consistency and
asymptotic normality, and reveal a relationship with the information geometry.
Some experiments show that the proposed estimator attains comparable perfor-
mance to the maximum likelihood estimator with drastically lower computational
cost.
1 Introduction
Parameter estimation of probabilistic models on discrete space is a popular and important issue in
the fields of machine learning and pattern recognition. For example, the Boltzmann machine (with
hidden variables) [1] [2] [3] is a very popular probabilistic model to represent binary variables, and
attracts increasing attention in the context of Deep learning [4]. A training of the Boltzmann ma-
chine, i.e., estimation of parameters is usually done by the maximum likelihood estimation (MLE).
The MLE for the Boltzmann machine cannot be explicitly solved and the gradient-based optimiza-
tion is frequently used. A difficulty of the gradient-based optimization is that the calculation of the
gradient requires calculation of a normalization constant or a partition function in each step of the op-
timization and its computational cost is sometimes exponential order. The problem of computational
cost is common to the other probabilistic models on discrete spaces and various kinds of approxi-
mation methods have been proposed to solve the difficulty. One approach tries to approximate the
probabilistic model by a tractable model by the mean-field approximation, which considers a model
assuming independence of variables [5]. Another approach such as the contrastive divergence [6]
avoids the exponential time calculation by the Markov Chain Monte Carlo (MCMC) sampling.
In the literature of parameters estimation of probabilistic model for continuous variables, [7] em-
ploys a score function which is a gradient of log-density with respect to the data vector rather than
parameters. This approach makes it possible to estimate parameters without calculating the normal-
ization term by focusing on the shape of the density function. [8] extended the method to discrete
variables, which defines information of “neighbor” by contrasting probability with that of a flipped
1
variable. [9] proposed a generalized local scoring rules on discrete sample spaces and [10] proposed
an approximated estimator with the Bregman divergence.
In this paper, we propose a novel parameter estimator for models on discrete space, which does not
require calculation of the normalization constant. The proposed estimator is defined by minimization
of a risk function derived by an unnormalized model and the homogeneous divergence having a weak
coincidence axiom. The derived risk function is convex for various kind of models including higher
order Boltzmann machine. We investigate statistical properties of the proposed estimator such as the
consistency and reveal a relationship between the proposed estimator and the α-divergence [11].
2 Settings
Let X be a d-dimensional vector of random variables in a discrete space X (typically {+1,−1}d)
and a bracket ⟨f⟩ be summation of a function f(x) on X , i.e., ⟨f⟩ =
∑
x∈X f(x). Let M and
P be a space of all non-negative finite measures on X and a subspace consisting of all probability
measures on X , respectively.
M = {f(x) |⟨f⟩ < ∞, f(x) ≥ 0} , P = {f(x) |⟨f⟩ = 1, f(x) ≥ 0} .
In this paper, we focus on parameter estimation of a probabilistic model q̄θ(x) on X , written as
q̄θ(x) =
qθ(x)
Zθ
(1)
where θ is an m-dimensional vector of parameters, qθ(x) is an unnormalized model in M and
Zθ = ⟨qθ⟩ is a normalization constant. A computation of the normalization constant Zθ sometimes
requires calculation of exponential order and is sometimes difficult for models on the discrete space.
Note that the unnormalized model qθ(x) is not normalized and ⟨qθ⟩ =
∑
x∈X qθ(x) = 1 does not
necessarily hold. Let ψθ(x) be a function on X and throughout the paper, we assume without loss
of generality that the unnormalized model qθ(x) can be written as
qθ(x) = exp(ψθ(x)). (2)
Remark 1. By setting ψθ(x) as ψθ(x)− logZθ, the normalized model (1) can be written as (2).
Example 1. The Bernoulli distribution on X = {+1,−1} is a simplest example of the probabilistic
model (1) with the function ψθ(x) = θx.
Example 2. With a function ψθ,k(x) = (x1, . . . , xd, x1x2, . . . , xd−1xd, x1x2x3, . . .)θ, we can de-
fine a k-th order Boltzmann machine [1, 12].
Example 3. Let xo ∈ {+1,−1}d1 and xh ∈ {+1,−1}d2 be an observed vector and hidden vector,
respectively, and x =
(
xTo ,x
T
h
)
∈ {+1,−1}d1+d2 where T indicates the transpose, be a con-
catenated vector. A function ψh,θ(xo) for the Boltzmann machine with hidden variables is written
as
ψh,θ(xo) = log
∑
xh
exp(ψθ,2(x)), (3)
where
∑
xh
is the summation with respect to the hidden variable xh.
Let us assume that a datasetD = {xi}ni=1 generated by an underlying distribution p(x), is given and
Z be a set of all patterns which appear in the dataset D. An empirical distribution p̃(x) associated
with the dataset D is defined as
p̃(x) =
{
nx
n x ∈ Z,
0 otherwise,
where nx =
∑n
i=1 I(xi = x) is a number of pattern x appeared in the dataset D.
Definition 1. For the unnormalized model (2) and distributions p(x) and p̃(x) in P , probability
functions rα,θ(x) and r̃α,θ(x) on X are defined by
rα,θ(x) =
p(x)αqθ(x)
1−α⟨
pαq1−αθ
⟩ , r̃α,θ(x) = p̃(x)αqθ(x)1−α⟨
p̃αq1−αθ
⟩ .
2
The distribution rα,θ (r̃α,θ) is an e-mixture model of the unnormalized model (2) and p(x) (p̃(x))
with ratio α [11].
Remark 2. We observe that r0,θ(x) = r̃0,θ(x) = q̄θ(x), r1,θ(x) = p(x), r̃1,θ(x) = p̃(x). Also if
p(x) = q̄θ0(x), rα,θ0(x) = q̄θ0(x) holds for an arbitrary α.
To estimate the parameter θ of probabilistic model q̄θ, the MLE defined by θ̂mle = argmaxθ L(θ) is
frequently employed, where L(θ) =
∑n
i=1 log q̄θ(xi) is the log-likelihood of the parameter θ with
the model q̄θ. Though theMLE is asymptotically consistent and efficient estimator, a main drawback
of the MLE is that computational cost for probabilistic models on the discrete space sometimes
becomes exponential. Unfortunately the MLE does not have an explicit solution in general, the
estimation of the parameter can be done by the gradient based optimization with a gradient ⟨p̃ψ′θ⟩−
⟨q̄θψ′θ⟩ of log-likelihood, where ψ′θ =
∂ψθ
∂θ . While the first term can be easily calculated, the second
term includes calculation of the normalization term Zθ, which requires 2d times summation for
X = {+1,−1}d and is not feasible when d is large.
3 Homogeneous Divergences for Statistical Inference
Divergences are an extension of the squared distance and are often used in statistical inference. A
formal definition of the divergenceD(f, g) is a non-negative valued function onM×M or onP×P
such thatD(f, f) = 0 holds for arbitrary f . Many popular divergences such as the Kullback-Leilber
(KL) divergence defined on P × P enjoy the coincidence axiom, i.e., D(f, g) = 0 leads to f = g.
The parameter in the statistical model q̄θ is estimated by minimizing the divergence D(p̃, q̄θ), with
respect to θ.
In the statistical inference using unnormalized models, the coincidence axiom of the divergence is
not suitable, since the probability and the unnormalized model do not exactly match in general. Our
purpose is to estimate the underlying distribution up to a constant factor using unnormalized models.
Hence, divergences having the property of the weak coincidence axiom, i.e.,D(f, g) = 0 if and only
if g = cf for some c > 0, are good candidate. As a class of divergences with the weak coincidence
axiom, we focus on homogeneous divergences that satisfy the equality D(f, g) = D(f, cg) for any
f, g ∈ M and any c > 0.
A representative of homogeneous divergences is the pseudo-spherical (PS) divergence [13], or in
other words, γ-divergence [14], that is defined from the Hölder inequality. Assume that γ is a
positive constant. For all non-negative functions f, g inM, the Hölder inequality⟨
fγ+1
⟩ 1
γ+1
⟨
gγ+1
⟩ γ
γ+1 − ⟨fgγ⟩ ≥ 0
holds. The inequality becomes an equality if and only if f and g are linearly dependent. The PS-
divergence Dγ(f, g) for f, g ∈ M is defined by
Dγ(f, g) =
1
1 + γ
log
⟨
fγ+1
⟩
+
γ
1 + γ
log
⟨
gγ+1
⟩
− log ⟨fgγ⟩ , γ > 0. (4)
The PS divergence is homogeneous, and the Hölder inequality ensures the non-negativity and the
weak coincidence axiom of the PS-divergence. One can confirm that the scaled PS-divergence,
γ−1Dγ , converges to the extended KL-divergence defined onM×M, as γ → 0. The PS-divergence
is used to obtain a robust estimator [14].
As shown in (4), the standard PS-divergence from the empirical distribution p̃ to the unnormalized
model qθ requires the computation of ⟨qγ+1θ ⟩, that may be infeasible in our setup. To circumvent
such an expensive computation, we employ a trick and substitute a model p̃qθ localized by the
empirical distribution for qθ, which makes it possible to replace the total sum in ⟨qγ+1θ ⟩ with the
empirical mean. More precisely, let us consider the PS-divergence from f = (pαq1−α)
1
1+γ to
g = (pα
′
q1−α
′
)
1
1+γ for the probability distribution p ∈ P and the unnormalized model q ∈ M,
where α, α′ are two distinct real numbers. Then, the divergence vanishes if and only if pαq1−α ∝
pα
′
q1−α
′
, i.e., q ∝ p. We define the localized PS-divergence Sα,α′,γ(p, q) by
Sα,α′,γ(p, q) = Dγ((p
αq1−α)1/(1+γ), (pα
′
q1−α
′
)1/(1+γ))
=
1
1 + γ
log
⟨
pαq1−α
⟩
+
γ
1 + γ
log⟨pα
′
q1−α
′
⟩ − log
⟨
pβq1−β
⟩
, (5)
3
where β = (α + γα′)/(1 + γ). Substituting the empirical distribution p̃ into p, the total sum over
X is replaced with a variant of the empirical mean such as
⟨
p̃αq1−α
⟩
=
∑
x∈Z
(
nx
n
)α
q1−α(x)
for a non-zero real number α. Since Sα,α′,γ(p, q) = Sα′,α,1/γ(p, q) holds, we can assume α > α′
without loss of generality. In summary, the conditions of the real parameters α, α′, γ are given by
γ > 0, α > α′, α ̸= 0, α′ ̸= 0, α+ γα′ ̸= 0,
where the last condition denotes β ̸= 0.
Let us consider another aspect of the computational issue about the localized PS-divergence. For the
probability distribution p and the unnormalized exponential model qθ, we show that the localized
PS-divergence Sα,α′,γ(p, qθ) is convex in θ, when the parameters α, α′ and γ are properly chosen.
Theorem 1. Let p ∈ P be any probability distribution, and let qθ be the unnormalized exponential
model qθ(x) = exp(θ
Tϕ(x)), where ϕ(x) is any vector-valued function corresponding to the suffi-
cient statistic in the (normalized) exponential model q̄θ. For a given β, the localized PS-divergence
Sα,α′,γ(p, qθ) is convex in θ for any α, α′, γ satisfying β = (α+ γα′)/(1 + γ) if and only if β = 1.
Proof. After some calculation, we have
∂2 log⟨pαq1−αθ ⟩
∂θ∂θT
= (1 − α)2Vrα,θ [ϕ], where Vrα,θ [ϕ] is the
covariance matrix of ϕ(x) under the probability rα,θ(x). Thus, the Hessian matrix of Sα,α′,γ(p, qθ)
is written as
∂2
∂θ∂θT
Sα,α′,γ(p, qθ) =
(1− α)2
1 + γ
Vrα,θ [ϕ] +
γ(1− α′)2
1 + γ
Vrα′,θ [ϕ]− (1− β)
2Vrβ,θ [ϕ].
The Hessian matrix is non-negative definite if β = 1. The converse direction is deferred to the
supplementary material.
Up to a constant factor, the localized PS-divergence with β = 1 characterized by Theorem 1 is
denotes as Sα,α′(p, q) that is defined by
Sα,α′(p, q) =
1
α− 1
log
⟨
pαq1−α
⟩
+
1
1− α′
log⟨pα
′
q1−α
′
⟩
for α > 1 > α′ ̸= 0. The parameter α′ can be negative if p is positive on X . Clearly, Sα,α′(p, q)
satisfies the homogeneity and the weak coincidence axiom as well as Sα,α′,γ(p, q).
4 Estimation with the localized pseudo-spherical divergence
Given the empirical distribution p̃ and the unnormalized model qθ, we define a novel estimator with
the localized PS-divergence Sα,α′,γ (or Sα,α′). Though the localized PS-divergence plugged-in
the empirical distribution is not well-defined when α′ < 0, we can formally define the following
estimator by restricting the domain X to the observed set of examples Z , even for negative α′:
θ̂ = argmin
θ
Sα,α′,γ(p̃, qθ) (6)
= argmin
θ
1
1 + γ
log
∑
x∈Z
(nx
n
)α
qθ(x)
1−α +
γ
1 + γ
log
∑
x∈Z
(nx
n
)α′
qθ(x)
1−α′
− log
∑
x∈Z
(nx
n
)β
qθ(x)
1−β .
Remark 3. The summation in (6) is defined on Z and then is computable even when α, α′, β < 0.
Also the summation includes only Z(≤ n) terms and its computational cost is O(n).
Proposition 1. For the unnormalized model (2), the estimator (6) is Fisher consistent.
Proof. We observe
∂
∂θ
Sα,α′,γ(q̄θ0 , qθ)
∣∣∣∣
θ=θ0
=
(
β − α+ γα
′
1 + γ
)⟨
q̄θ0ψ
′
θ0
⟩
= 0
implying the Fisher consistency of θ̂.
4
Theorem 2. Let qθ(x) be the unnormalized model (2), and θ0 be the true parameter of underlying
distribution p(x) = q̄θ0(x). Then an asymptotic distribution of the estimator (6) is written as√
n(θ̂ − θ0) ∼ N (0, I(θ0)−1)
where I(θ0) = Vq̄θ0 [ψ
′
θ0
] is the Fisher information matrix.
Proof. We shall sketch a proof and the detailed proof is given in supplementary material. Let us
assume that the empirical distribution is written as
p̃(x) = q̄θ0(x) + ϵ(x).
Note that ⟨ϵ⟩ = 0 because p̃, q̄θ0 ∈ P . The asymptotic expansion of the equilibrium condition for
the estimator (6) around θ = θ0 leads to
0 =
∂
∂θ
Sα,α′,γ(p̃, qθ)
∣∣∣∣
θ=θ̂
=
∂
∂θ
Sα,α′,γ(p̃, qθ)
∣∣∣∣
θ=θ0
+
∂2
∂θ∂θT
Sα,α′,γ(p̃, qθ)
∣∣∣∣
θ=θ0
(θ̂ − θ0) +O(||θ̂ − θ0||2)
By the delta method [15], we have
∂
∂θ
Sα,α′,γ(p̃, qθ)
∣∣∣∣
θ=θ0
− ∂
∂θ
Sα,α′,γ(p, qθ)
∣∣∣∣
θ=θ0
≃ − γ
(1 + γ)2
(α− α′)2
⟨
ψ′θ0ϵ
⟩
and from the central limit theorem, we observe that
√
n
⟨
ψ′θ0ϵ
⟩
=
√
n
1
n
n∑
i=1
(
ψ′θ0(xi)−
⟨
q̄θ0ψ
′
θ0
⟩)
asymptotically follows the normal distribution with mean 0, and variance I(θ0) = Vq̄θ0 [ψ
′
θ0
], which
is known as the Fisher information matrix. Also from the law of large numbers, we have
∂2
∂θ∂θT
Sα,α′,γ(p̃, qθ)
∣∣∣∣
θ=θ0
(θ̂ − θ0) →
γ
(1 + γ)2
(α− α′)2I(θ0),
in the limit of n → ∞. Consequently, we observe that (2).
Remark 4. The asymptotic distribution of (6) is equal to that of the MLE, and its variance does not
depend on α, α′, γ.
Remark 5. As shown in Remark 1, the normalized model (1) is a special case of the unnormalized
model (2) and then Theorem 2 holds for the normalized model.
5 Characterization of localized pseudo-spherical divergence Sα,α′
Throughout this section, we assume that β = 1 holds and investigate properties of the localized PS-
divergence Sα,α′ . We discuss influence of selection of α, α′ and characterization of the localized
PS-divergence Sα,α′ in the following subsections.
5.1 Influence of selection of α, α′
We investigate influence of selection of α, α′ for the localized PS-divergence Sα,α′ with a view of
the estimating equation. The estimator θ̂ derived from Sα,α′ satisfies
∂Sα,α′(p̃, qθ)
∂θ
∣∣∣∣
θ=θ̂
∝
⟨
r̃α′,θ̂ψ
′
θ̂
⟩
−
⟨
r̃α,θ̂ψ
′
θ̂
⟩
= 0. (7)
which is a moment matching with respect to two distributions r̃α,θ and r̃α′,θ (α, α′ ̸= 0, 1). On the
other hand, the estimating equation of the MLE is written as
∂L(θ)
∂θ
∣∣∣∣
θ=θmle
∝
⟨
p̃ψ′θmle
⟩
− ⟨q̄θmleψθmle⟩ =
⟨
r̃1,θmleψ
′
θmle
⟩
−
⟨
r̃0,θmleψ
′
θmle
⟩
= 0, (8)
which is a moment matching with respect to the empirical distribution p̃ = r̃1,θmle and the
normalized model q̄θ = r̃0,θmle . While the localized PS-divergence Sα,α′ is not defined with
(α, α′) = (0, 1), comparison of (7) with (8) implies that behavior the estimator θ̂ becomes simi-
lar to that of the MLE in the limit of α → 1 and α′ → 0.
5
5.2 Relationship with the α-divergence
The α-divergence between two positive measures f, g ∈ M is defined as
Dα(f, g) =
1
α(1− α)
⟨
αf + (1− α)g − fαg1−α
⟩
,
where α is a real number. Note that Dα(f, g) ≥ 0 and 0 if and only if f = g, and the α-divergence
reduces to KL(f, g) and KL(g, f) in the limit of α → 1 and 0, respectively.
Remark 6. An estimator defined by minimizing α-divergence Dα(p̃, q̄θ) between the empirical
distribution and normalized model, satisfies
∂Dα(p̃, q̄θ)
∂θ
∝
⟨
p̃αq1−αθ (ψ
′
θ − ⟨q̄θψ′θ⟩)
⟩
= 0
and requires calculation proportional to |X | which is infeasible. Also the same hold for an estimator
defined by minimizing α-divergenceDα(p̃, qθ) between the empirical distribution and unnormalized
model, satisfying ∂Dα(p̃,qθ)∂θ ∝
⟨
(1− α)qθψ′θ − p̃αq
1−α
θ
⟩
= 0.
Here, we assume that α, α′ ̸= 0, 1 and consider a trick to cancel out the term ⟨g⟩ by mixing two
α-divergences as follows.
Dα,α′(f, g) =Dα(f, g) +
(
−α′
α
)
Dα′(f, g)
=
⟨(
1
1− α
− α
′
α(1− α′)
)
f − 1
α(1− α)
fαg1−α +
1
α(1− α′)
fα
′
g1−α
′
⟩
.
Remark 7. Dα,α′(f, g) ≥ 0 is divergence when αα′ < 0 holds, i.e., Dα,α′(f, g) ≥ 0 and
Dα,α′(f, g) = 0 if and only if f = g. Without loss of generality, we assume α > 0 > α′ for
Dα,α′ .
Firstly, we consider an estimator defined by the minmizer of
min
θ
∑
x∈Z
{
1
1− α′
(nx
n
)α′
qθ(x)
1−α′ − 1
1− α
(nx
n
)α
qθ(x)
1−α
}
. (9)
Note that the summation in (9) includes only Z(≤ n) terms. We remark the following.
Remark 8. Let q̄θ0(x) be the underlying distribution and qθ(x) be the unnormalized model (2).
Then an estimator defined by minimizing Dα,α′(q̄θ0 , qθ) is not in general Fisher consistent, i.e.,
∂Dα,α′(q̄θ0 , qθ)
∂θ
∣∣∣∣
θ=θ0
∝
⟨
q̄α
′
θ0q
1−α′
θ0
ψ′θ0 − q̄
α
θ0q
1−α
θ0
ψ′θ0
⟩
=
(
⟨qθ0⟩
−α′ − ⟨qθ0⟩
−α
) ⟨
qθ0ψ
′
θ0
⟩
̸= 0.
This remark shows that an estimator associated with Dα,α′(p̃, qθ) does not have suitable properties
such as (asymptotic) unbiasedness and consistency while required computational cost is drastically
reduced. Intuitively, this is because the (mixture of) α-divergence satisfies the coincidence axiom.
To overcome this drawback, we consider the following minimization problem for estimation of the
parameter θ of model q̄θ(x).
(θ̂, r̂) = argmin
θ,r
Dα,α′(p̃, rqθ)
where r is a constant corresponding to an inverse of the normalization term Zθ = ⟨qθ⟩.
Proposition 2. Let qθ(x) be the unnormalized model (2). For α > 1 and 0 > α′, the minimization
of Dα,α′(p̃, rqθ) is equivalent to the minimization of
Sα,α′(p̃, qθ).
Proof. For a given θ, we observe that
r̂θ = argmin
r
Dα,α′(p̃, rqθ) =
( ⟨
p̃αq1−αθ
⟩⟨
p̃α′q1−α
′
θ
⟩) 1α−α′ . (10)
6
Note that computation of (10) requires only sample order O(n) calculation. By plugging (10) into
Dα,α′(p̃, rqθ), we observe
θ̂ = argmin
θ
Dα,α′(p̃, r̂θqθ) = argmin
θ
Sα,α′(p̃, qθ). (11)
If α > 1 and α′ < 0 hold, the estimator (11) is equivalent to the estimator associated with the
localized PS-divergence Sα,α′ , implying that Sα,α′ is characterized by the mixture of α-divergences.
Remark 9. From a viewpoint of the information geometry [11], a metric (information geometrical
structure) induced by the α-divergence is the Fisher metric induced by the KL-divergence. This im-
plies that the estimation based on the (mixture of) α-divergence is Fisher efficient and is an intuitive
explanation of the Theorem 2. The localized PS divergence Sα,α′,γ and Sα,α′ with αα′ > 0 can be
interpreted as an extension of the α-divergence, which preserves Fisher efficiency.
6 Experiments
We especially focus on a setting of β = 1, i.e., convexity of the risk function with the unnormalized
model exp(θTϕ(x)) holds (Theorem 1) and examined performance of the proposed estimator.
6.1 Fully visible Boltzmann machine
In the first experiment, we compared the proposed estimator with parameter settings (α, α′) =
(1.01, 0.01), (1.01,−0.01), (2,−1), with the MLE and the ratio matching method [8]. Note that
the ratio matching method also does not require calculation of the normalization constant, and the
proposed method with (α, α′) = (1.01,±0.01) may behave like the MLE as discussed in section
5.1.
All methods were optimized with the optim function in R language [16]. The dimension d of input
was set to 10 and the synthetic dataset was randomly generated from the second order Boltzmann
machine (Example 2) with a parameter θ∗ ∼ N (0, I). We repeated comparison 50 times and
observed averaged performance. Figure 1 (a) shows median of the root mean square errors (RMSEs)
between θ∗ and θ̂ of each method over 50 trials, against the number n of examples. We observe that
the proposed estimator works well and is superior to the ratio matching method. In this experiment,
the MLE outperforms the proposed method contrary to the prediction of Theorem 2. This is because
observed patterns were only a small portion of all possible patterns, as shown in Figure 1 (b). Even
in such a case, the MLE can take all possible patterns (210 = 1024) into account through the
normalization term logZθ ≃ Const + 12 ||θ||
2 that works like a regularizer. On the other hand, the
proposed method genuinely uses only the observed examples, and the asymptotic analysis would not
be relevant in this case. Figure 1 (c) shows median of computational time of each method against
n. The computational time of the MLE does not vary against n because the computational cost is
dominated by the calculation of the normalization constant. Both the proposed estimator and the
ratio matching method are significantly faster than the MLE, and the ratio matching method is faster
than the proposed estimator while the RMSE of the proposed estimator is less than that of the ratio
matching.
6.2 Boltzmann machine with hidden variables
In this subsection, we applied the proposed estimator for the Boltzmann machine with hidden vari-
ables whose associated function is written as (3). The proposed estimator with parameter settings
(α, α′) = (1.01, 0.01), (1.01,−0.01), (2,−1) was compared with the MLE. The dimension d1 of
observed variables was fixed to 10 and d2 of hidden variables was set to 2, and the parameter θ∗
was generated as θ∗ ∼ N (0, I) including parameters corresponding to hidden variables. Note
that the Boltzmann machine with hidden variables is not identifiable and different values of the pa-
rameter do not necessarily generate different probability distributions, implying that estimators are
influenced by local minimums. Then we measured performance of each estimator by the averaged
7
0 5000 10000 15000 20000 25000
0
.1
0
.2
0
.5
1
.0
n
R
M
S
E
MLE
Ratio matching
a1=1.01,a2=0.01
a1=1.01,a2=−0.01
a1=2,a2=−1
100 200 400 800 1600 3200 6400 12800 25600
0
5
0
1
0
0
1
5
0
2
0
0
2
5
0
3
0
0
n
N
u
m
b
e
r 
|Z
| 
o
f 
u
n
iq
u
e
 p
a
tt
e
rn
s
0 5000 10000 15000 20000 25000
2
5
1
0
2
0
5
0
1
0
0
2
0
0
5
0
0
n
T
im
e
[s
]
MLE
Ratio matching
a1=1.01,a2=0.01
a1=1.01,a2=−0.01
a1=2,a2=−1
Figure 1: (a) Median of RMSEs of each method against n, in log scale. (b) Box-whisker plot of
number |Z| of unique patterns in the dataset D against n. (c) Median of computational time of each
method against n, in log scale.
log-likelihood 1n
∑n
i=1 log q̄θ̂(xi) rather than the RMSE. An initial value of the parameter was set
by N (0, I) and commonly used by all methods. We repeated the comparison 50 times and ob-
served the averaged performance. Figure 2 (a) shows median of averaged log-likelihoods of each
method over 50 trials, against the number n of example. We observe that the proposed estimator is
comparable with the MLE when the number n of examples becomes large. Note that the averaged
log-likelihood of MLE once decreases when n is samll, and this is due to overfitting of the model.
Figure 2 (b) shows median of averaged log-likelihoods of each method for test dataset consists of
10000 examples, over 50 trials. Figure 2 (c) shows median of computational time of each method
against n, and we observe that the proposed estimator is significantly faster than the MLE.
0 5000 10000 15000 20000 25000
−
1
5
−
1
0
−
5
n
A
v
e
ra
g
e
d
 L
o
g
 l
ik
e
lih
o
o
d
MLE
a1=1.01,a2=0.01
a1=1.01,a2=−0.01
a1=2,a2=−1
0 5000 10000 15000 20000 25000
−
1
5
−
1
0
−
5
n
A
v
e
ra
g
e
d
 L
o
g
 l
ik
e
lih
o
o
d
MLE
a1=1.01,a2=0.01
a1=1.01,a2=−0.01
a1=2,a2=−1
0 5000 10000 15000 20000 25000
5
1
0
2
0
5
0
1
0
0
2
0
0
5
0
0
1
0
0
0
n
T
im
e
[s
]
MLE
a1=1.01,a2=0.01
a1=1.01,a2=−0.01
a1=2,a2=−1
Figure 2: (a) Median of averaged log-likelihoods of each method against n. (b) Median of averaged
log-likelihoods of each method calculated for test dataset against n. (c) Median of computational
time of each method against n, in log scale.
7 Conclusions
We proposed a novel estimator for probabilistic model on discrete space, based on the unnormal-
ized model and the localized PS-divergence which has the homogeneous property. The proposed
estimator can be constructed without calculation of the normalization constant and is asymptotically
efficient, which is the most important virtue of the proposed estimator. Numerical experiments show
that the proposed estimator is comparable to the MLE and required computational cost is drastically
reduced.
8
References
[1] Hinton, G. E. & Sejnowski, T. J. (1986) Learning and relearning in boltzmann machines. MIT
Press, Cambridge, Mass, 1:282–317.
[2] Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. (1985) A learning algorithm for boltzmann
machines. Cognitive Science, 9(1):147–169.
[3] Amari, S., Kurata, K. & Nagaoka, H. (1992) Information geometry of Boltzmann machines.
In IEEE Transactions on Neural Networks, 3: 260–271.
[4] Hinton, G. E. & Salakhutdinov, R. R. (2012) A better way to pretrain deep boltzmann ma-
chines. In Advances in Neural Information Processing Systems, pp. 2447–2455 Cambridge,
MA: MIT Press.
[5] Opper, M. & Saad, D. (2001) Advanced Mean Field Methods: Theory and Practice. MIT
Press, Cambridge, MA.
[6] Hinton, G.E. (2002) Training Products of Experts by Minimizing Contrastive Divergence.
Neural Computation, 14(8):1771–1800.
[7] Hyvärinen, A. (2005) Estimation of non-normalized statistical models by score matching.
Journal of Machine Learning Research, 6:695–708.
[8] Hyvärinen, A. (2007) Some extensions of score matching. Computational statistics & data
analysis, 51(5):2499–2512.
[9] Dawid, A. P., Lauritzen, S. & Parry, M. (2012) Proper local scoring rules on discrete sample
spaces. The Annals of Statistics, 40(1):593–608.
[10] Gutmann, M. & Hirayama, H. (2012) Bregman divergence as general framework to estimate
unnormalized statistical models. arXiv preprint arXiv:1202.3727.
[11] Amari, S & Nagaoka, H. (2000) Methods of Information Geometry, volume 191 of Transla-
tions of Mathematical Monographs. Oxford University Press.
[12] Sejnowski, T. J. (1986) Higher-order boltzmann machines. In American Institute of Physics
Conference Series, 151:398–403.
[13] Good, I. J. (1971) Comment on “measuring information and uncertainty,” by R. J. Buehler.
In Godambe, V. P. & Sprott, D. A. editors, Foundations of Statistical Inference, pp. 337–339,
Toronto: Holt, Rinehart and Winston.
[14] Fujisawa, H. & Eguchi, S. (2008) Robust parameter estimation with a small bias against heavy
contamination. Journal of Multivariate Analysis, 99(9):2053–2081.
[15] Van der Vaart, A. W. (1998) Asymptotic Statistics. Cambridge University Press.
[16] R Core Team. (2013) R: A Language and Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria.
9
