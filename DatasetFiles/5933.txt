


Paper ID = 5933
Title = On the consistency theory of high dimensional
variable screening
Xiangyu Wang
Dept. of Statistical Science
Duke University, USA
xw56@stat.duke.edu
Chenlei Leng
Dept. of Statistics
University of Warwick, UK
C.Leng@warwick.ac.uk
David B. Dunson
Dept. of Statistical Science
Duke University, USA
dunson@stat.duke.edu
Abstract
Variable screening is a fast dimension reduction technique for assisting high di-
mensional feature selection. As a preselection method, it selects a moderate size
subset of candidate variables for further refining via feature selection to produce
the final model. The performance of variable screening depends on both compu-
tational efficiency and the ability to dramatically reduce the number of variables
without discarding the important ones. When the data dimension p is substantially
larger than the sample size n, variable screening becomes crucial as 1) Faster fea-
ture selection algorithms are needed; 2) Conditions guaranteeing selection consis-
tency might fail to hold. This article studies a class of linear screening methods
and establishes consistency theory for this special class. In particular, we prove
the restricted diagonally dominant (RDD) condition is a necessary and sufficient
condition for strong screening consistency. As concrete examples, we show two
screening methods SIS and HOLP are both strong screening consistent (subject
to additional constraints) with large probability if n > O((Ïs+Ïƒ/Ï„)2 log p) under
random designs. In addition, we relate the RDD condition to the irrepresentable
condition, and highlight limitations of SIS.
1 Introduction
The rapidly growing data dimension has brought new challenges to statistical variable selection, a
crucial technique for identifying important variables to facilitate interpretation and improve predic-
tion accuracy. Recent decades have witnessed an explosion of research in variable selection and
related fields such as compressed sensing [1, 2], with a core focus on regularized methods [3â€“7].
Regularized methods can consistently recover the support of coefficients, i.e., the non-zero signals,
via optimizing regularized loss functions under certain conditions [8â€“10]. However, in the big data
era when p far exceeds n, such regularized methods might fail due to two reasons. First, the con-
ditions that guarantee variable selection consistency for convex regularized methods such as lasso
might fail to hold when p >> n; Second, the computational expense of both convex and non-convex
regularized methods increases dramatically with large p.
Bearing these concerns in mind, [11] propose the concept of â€œvariable screeningâ€, a fast technique
that reduces data dimensionality from p to a size comparable to n, with all predictors having non-
zero coefficients preserved. They propose a marginal correlation based fast screening technique
â€œSure Independence Screeningâ€ (SIS) that can preserve signals with large probability. However,
this method relies on a strong assumption that the marginal correlations between the response and
the important predictors are high [11], which is easily violated in the practice. [12] extends the
marginal correlation to the Spearmanâ€™s rank correlation, which is shown to gain certain robustness
but is still limited by the same strong assumption. [13] and [14] take a different approach to attack
the screening problem. They both adopt variants of a forward selection type algorithm that includes
one variable at a time for constructing a candidate variable set for further refining. These methods
1
eliminate the strong marginal assumption in [11] and have been shown to achieve better empirical
performance. However, such improvement is limited by the extra computational burden caused
by their iterative framework, which is reported to be high when p is large [15]. To ameliorate
concerns in both screening performance and computational efficiency, [15] develop a new type of
screening method termed â€œHigh-dimensional ordinary least-square projectionâ€ (HOLP ). This new
screener relaxes the strong marginal assumption required by SIS and can be computed efficiently
(complexity is O(n2p)), thus scalable to ultra-high dimensionality.
This article focuses on linear models for tractability. As computation is one vital concern for design-
ing a good screening method, we primarily focus on a class of linear screeners that can be efficiently
computed, and study their theoretical properties. The main contributions of this article lie in three
aspects.
1. We define the notion of strong screening consistency to provide a unified framework for
analyzing screening methods. In particular, we show a necessary and sufficient condi-
tion for a screening method to be strong screening consistent is that the screening matrix
is restricted diagonally dominant (RDD). This condition gives insights into the design of
screening matrices, while providing a framework to assess the effectiveness of screening
methods.
2. We relate RDD to other existing conditions. The irrepresentable condition (IC) [8] is nec-
essary and sufficient for sign consistency of lasso [3]. In contrast to IC that is specific to the
design matrix, RDD involves another ancillary matrix that can be chosen arbitrarily. Such
flexibility allows RDD to hold even when IC fails if the ancillary matrix is carefully chosen
(as in HOLP ). When the ancillary matrix is chosen as the design matrix, certain equiva-
lence is shown between RDD and IC, revealing the difficulty for SIS to achieve screening
consistency. We also comment on the relationship between RDD and the restricted eigen-
value condition (REC) [6] which is commonly seen in the high dimensional literature. We
illustrate via a simple example that RDD might not be necessarily stronger than REC.
3. We study the behavior of SIS and HOLP under random designs, and prove that a sample
size of n = O
(
(Ïs + Ïƒ/Ï„)2 log p
)
is sufficient for SIS and HOLP to be screening con-
sistent, where s is the sparsity, Ï measures the diversity of signals and Ï„/Ïƒ evaluates the
signal-to-noise ratio. This is to be compared to the sign consistency results in [9] where the
design matrix is fixed and assumed to follow the IC.
The article is organized as follows. In Section 1, we set up the basic problem and describe the
framework of variable screening. In Section 2, we provide a deterministic necessary and sufficient
condition for consistent screening. Its relationship with the irrepresentable condition is discussed
in Section 3. In Section 4, we prove the consistency of SIS and HOLP under random designs by
showing the RDD condition is satisfied with large probability, although the requirement on SIS is
much more restictive.
2 Linear screening
Consider the usual linear regression
Y = XÎ² + ,
where Y is the nÃ—1 response vector, X is the nÃ—p design matrix and  is the noise. The regression
task is to learn the coefficient vector Î². In the high dimensional setting where p >> n, a sparsity
assumption is often imposed on Î² so that only a small portion of the coordinates are non-zero. Such
an assumption splits the task of learning Î² into two phases. The first is to recover the support of
Î², i.e., the location of non-zero coefficients; The second is to estimate the value of these non-zero
signals. This article mainly focuses on the first phase.
As pointed out in the introduction, when the dimensionality is too high, using regularization methods
methods raises concerns both computationally and theoretically. To reduce the dimensionality, [11]
suggest a variable screening framework by finding a submodel
Md = {i : |Î²Ì‚i| is among the largest d coordinates of |Î²Ì‚|} or MÎ³ = {i : |Î²Ì‚i| > Î³}.
2
Let Q = {1, 2, Â· Â· Â· , p} and define S as the true model with s = |S| being its cardinarlity. The
hope is that the submodel size |Md| or |MÎ³ | will be smaller or comparable to n, while S âŠ† Md
or S âŠ† MÎ³ . To achieve this goal two steps are usually involved in the screening analysis. The
first is to show there exists some Î³ such that miniâˆˆS |Î²Ì‚i| > Î³ and the second step is to bound the
size of |MÎ³ | such that |MÎ³ | = O(n). To unify these steps for a more comprehensive theoretical
framework, we put forward a slightly stronger definition of screening consistency in this article.
Definition 2.1. (Strong screening consistency) An estimator Î²Ì‚ (of Î²) is strong screening consistent
if it satisfies that
min
iâˆˆS
|Î²Ì‚i| > max
i 6âˆˆS
|Î²Ì‚i| (1)
and
sign(Î²Ì‚i) = sign(Î²i), âˆ€i âˆˆ S. (2)
Remark 2.1. This definition does not differ much from the usual screening property studied in the
literature, which requires miniâˆˆS |Î²Ì‚i| > max(nâˆ’s)i 6âˆˆS |Î²Ì‚i|, where max(k) denotes the kth largest item.
The key of strong screening consistency is the property (1) that requires the estimator to preserve
consistent ordering of the zero and non-zero coefficients. It is weaker than variable selection consis-
tency in [8]. The requirement in (2) can be seen as a relaxation of the sign consistency defined in [8],
as no requirement for Î²Ì‚i, i 6âˆˆ S is needed. As shown later, such relaxation tremendously reduces the
restriction on the design matrix, and allows screening methods to work for a broader choice of X .
The focus of this article is to study the theoretical properties of a special class of screeners that take
the linear form as
Î²Ì‚ = AY
for some pÃ—n ancillary matrixA. Examples include sure independence screening (SIS) whereA =
XT /n and high-dimensional ordinary least-square projection (HOLP ) where A = XT (XXT )âˆ’1.
We choose to study the class of linear estimators because linear screening is computationally effi-
cient and theoretically tractable. We note that the usual ordinary least-squares estimator is also a
special case of linear estimators although it is not well defined for p > n.
3 Deterministic guarantees
In this section, we derive the necessary and sufficient condition that guarantees Î²Ì‚ = AY to be strong
screening consistent. The design matrix X and the error  are treated as fixed in this section and we
will investigate random designs later. We consider the set of sparse coefficient vectors defined by
B(s, Ï) =
{
Î² âˆˆ Rp : |supp(Î²)| â‰¤ s,
maxiâˆˆsupp(Î²) |Î²i|
miniâˆˆsupp(Î²) |Î²i|
â‰¤ Ï
}
.
The set B(s, Ï) contains vectors having at most s non-zero coordinates with the ratio of the largest
and smallest coordinate bounded by Ï. Before proceeding to the main result of this section, we
introduce some terminology that helps to establish the theory.
Definition 3.1. (restricted diagonally dominant matrix) A p Ã— p symmetric matrix Î¦ is restricted
diagonally dominant with sparsity s if for any I âŠ† Q, |I| â‰¤ sâˆ’ 1 and i âˆˆ Q \ I
Î¦ii > C0 max
{âˆ‘
jâˆˆI
|Î¦ij + Î¦kj |,
âˆ‘
jâˆˆI
|Î¦ij âˆ’ Î¦kj |
}
+ |Î¦ik| âˆ€k 6= i, k âˆˆ Q \ I,
where C0 â‰¥ 1 is a constant.
Notice this definition implies that for i âˆˆ Q \ I
Î¦ii â‰¥ C0
(âˆ‘
jâˆˆI
|Î¦ij + Î¦kj |+
âˆ‘
jâˆˆI
|Î¦ij âˆ’ Î¦kj |
)
/2 â‰¥ C0
âˆ‘
jâˆˆI
|Î¦ij |, (3)
which is related to the usual diagonally dominant matrix. The restricted diagonally dominant ma-
trix provides a necessary and sufficient condition for any linear estimators Î²Ì‚ = AY to be strong
screening consistent. More precisely, we have the following result.
3
Theorem 1. For the noiseless case where  = 0, a linear estimator Î²Ì‚ = AY is strong screening
consistent for every Î² âˆˆ B(s, Ï), if and only if the screening matrix Î¦ = AX is restricted diagonally
dominant with sparsity s and C0 â‰¥ Ï.
Proof. Assume Î¦ is restricted diagonally dominant with sparsity s and C0 â‰¥ Ï. Recall Î²Ì‚ = Î¦Î².
Suppose S is the index set of non-zero predictors. For any i âˆˆ S, k 6âˆˆ S, if we let I = S \ {i}, then
we have
|Î²Ì‚i| = |Î²i|
(
Î¦ii +
âˆ‘
jâˆˆI
Î²j
Î²i
Î¦ij
)
= |Î²i|
{
Î¦ii +
âˆ‘
jâˆˆI
Î²j
Î²i
(Î¦ij + Î¦kj) + Î¦ki âˆ’
âˆ‘
jâˆˆI
Î²j
Î²i
Î¦kj âˆ’ Î¦ki
}
> âˆ’|Î²i|
(âˆ‘
jâˆˆI
Î²j
Î²i
Î¦kj + Î¦ki
)
= âˆ’|Î²i|
Î²i
(âˆ‘
jâˆˆI
Î²jÎ¦kj + Î²iÎ¦ki
)
= âˆ’sign(Î²i) Â· Î²Ì‚k,
and
|Î²Ì‚i| = |Î²i|
(
Î¦ii +
âˆ‘
jâˆˆI
Î²j
Î²i
Î¦ij
)
= |Î²i|
{
Î¦ii +
âˆ‘
jâˆˆI
Î²j
Î²i
(Î¦ij âˆ’ Î¦kj)âˆ’ Î¦ki +
âˆ‘
jâˆˆI
Î²j
Î²i
Î¦kj + Î¦ki
}
> |Î²i|
(âˆ‘
jâˆˆI
Î²j
Î²i
Î¦kj + Î¦ki
)
= sign(Î²i) Â· Î²Ì‚k.
Therefore, whatever value sign(Î²i) is, it always holds that |Î²Ì‚i| > |Î²Ì‚k| and thus miniâˆˆS |Î²Ì‚i| >
maxk 6âˆˆS |Î²Ì‚k|.
To prove the sign consistency for non-zero coefficients, we notice that for i âˆˆ S,
Î²Ì‚iÎ²i = Î¦iiÎ²
2
i +
âˆ‘
jâˆˆI
Î¦ijÎ²jÎ²i = Î²
2
i
(
Î¦ii +
âˆ‘
jâˆˆI
Î²j
Î²i
Î¦ij
)
> 0.
The proof of necessity is left to the supplementary materials.
The noiseless case is a good starting point to analyze Î²Ì‚. Intuitively, in order to preserve the correct
order of the coefficients in Î²Ì‚ = AXÎ², one needs AX to be close to a diagonally dominant matrix,
so that Î²Ì‚i, i âˆˆ MS will take advantage of the large diagonal terms in AX to dominate Î²Ì‚i, i 6âˆˆ MS
that is just linear combinations of off-diagonal terms.
When noise is considered, the condition in Theorem 1 needs to be changed slightly to accommodate
extra discrepancies. In addition, the smallest non-zero coefficient has to be lower bounded to ensure
a certain level of signal-to-noise ratio. Thus, we augment our previous definition of B(s, Ï) to have
a signal strength control
BÏ„ (s, Ï) = {Î² âˆˆ B(s, Ï)| min
iâˆˆsupp(Î²)
|Î²i| â‰¥ Ï„}.
Then we can obtain the following modified Theorem.
Theorem 2. With noise, the linear estimator Î²Ì‚ = AY is strong screening consistent for every
Î² âˆˆ BÏ„ (s, Ï) if Î¦ = AX âˆ’ 2Ï„âˆ’1â€–Aâ€–âˆIp is restricted diagonally dominant with sparsity s and
C0 â‰¥ Ï.
The proof of Theorem 2 is essentially the same as Theorem 1 and is thus left to the supplementary
materials. The condition in Theorem 2 can be further tailored to a necessary and sufficient version
with extra manipulation on the noise term. Nevertheless, this might not be useful in practice due to
the randomness in noise. In addition, the current version of Theorem 2 is already tight in the sense
that there exists some noise vector  such that the condition in Theorem 2 is also necessary for strong
screening consistency.
Theorems 1 and 2 establish ground rules for verifying consistency of a given screener and provide
practical guidance for screening design. In Section 4, we consider some concrete examples of an-
cillary matrix A and prove that conditions in Theorems 1 and 2 are satisfied by the corresponding
screeners with large probability under random designs.
4
4 Relationship with other conditions
For some special cases such sure independence screening (â€SISâ€), the restricted diagonally dominant
(RDD) condition is related to the strong irrepresentable condition (IC) proposed in [8]. Assume each
column of X is standardized to have mean zero. Letting C = XTX/n and Î² be a given coefficient
vector, the IC is expressed as
â€–CSc,SCâˆ’1S,S Â· sign(Î²S)â€–âˆ â‰¤ 1âˆ’ Î¸ (4)
for some Î¸ > 0, where CA,B represents the sub-matrix of C with row indices in A and column
indices inB. The authors enumerate several scenarios of C such that IC is satisfied. We verify some
of these scenarios for screening matrix Î¦.
Corollary 1. If Î¦ii = 1, âˆ€i and |Î¦ij | < c/(2s), âˆ€i 6= j for some 0 â‰¤ c < 1 as defined in Corollary
1 and 2 in [8], then Î¦ is a restricted diagonally dominant matrix with sparsity s and C0 â‰¥ 1/c.
If |Î¦ij | < r|iâˆ’j|, âˆ€i, j for some 0 < r < 1 as defined in Corollary 3 in [8], then Î¦ is a restricted
diagonally dominant matrix with sparsity s and C0 â‰¥ (1âˆ’ r)2/(4r).
A more explicit but nontrivial relationship between IC and RDD is illustrated below when |S| = 2.
Theorem 3. Assume Î¦ii = 1, âˆ€i and |Î¦ij | < r, âˆ€i 6= j. If Î¦ is restricted diagonally dominant with
sparsity 2 and C0 â‰¥ Ï, then Î¦ satisfies
â€–Î¦Sc,SÎ¦âˆ’1S,S Â· sign(Î²S)â€–âˆ â‰¤
Ïâˆ’1
1âˆ’ r
for all Î² âˆˆ B(2, Ï). On the other hand, if Î¦ satisfies the IC for all Î² âˆˆ B(2, Ï) for some Î¸, then Î¦ is
a restricted diagonally dominant matrix with sparsity 2 and
C0 â‰¥
1
1âˆ’ Î¸
1âˆ’ r
1 + r
.
Theorem 3 demonstrates certain equivalence between IC and RDD. However, it does not mean
that RDD is also a strong requirement. Notice that IC is directly imposed on the covariance matrix
XTX/n. This makes IC a strong assumption that is easily violated; for example, when the predictors
are highly correlated. In contrast to IC, RDD is imposed on matrix AX where there is flexibility in
choosing A. Only when A is chose to be X/n, RDD is equivalently strong as IC, as shown in next
theorem. For other choices ofA, such asHOLP defined in next section, the estimator satisfies RDD
even when predictors are highly correlated. Therefore, RDD is considered as weak requirement.
For â€SISâ€, the screening matrix Î¦ = XTX/n coincides with the covariance matrix, making RDD
and IC effectively equivalent. The following theorem formalizes this.
Theorem 4. Let A = XT /n and standardize columns of X to have sample variance one. Assume
X satisfies the sparse Riesz condition [16], i.e,
min
Ï€âŠ†Q, |Ï€|â‰¤s
Î»min(X
T
Ï€XÏ€/n) â‰¥ Âµ,
for some Âµ > 0. Now if AX is restricted diagonally dominant with sparsity s+ 1 and C0 â‰¥ Ï with
Ï >
âˆš
s/Âµ, then X satisfies the IC for any Î² âˆˆ B(s, Ï).
In other words, under the condition Ï >
âˆš
s/Âµ, the strong screening consistency of SIS for B(s +
1, Ï) implies the model selection consistency of lasso for B(s, Ï).
Theorem 4 illustrates the difficulty of SIS. The necessary condition that guarantees good screening
performance of SIS also guarantees the model selection consistency of lasso. However, such a
strong necessary condition does not mean that SIS should be avoided in practice given its substantial
advantages in terms of simplicity and computational efficiency. The strong screening consistency
defined in this article is stronger than conditions commonly used in justifying screening procedures
as in [11].
Another common assumption in the high dimensional literature is the restricted eigenvalue condition
(REC). Compared to REC, RDD is not necessarily stronger due to its flexibility in choosing the
ancillary matrix A. [17, 18] prove that the REC is satisfied when the design matrix is sub-Gaussian.
However, REC might not be guaranteed when the row of X follows heavy-tailed distribution. In
contrast, as the example shown in next section and in [15], by choosing A = XT (XXT )âˆ’1, the
resulting estimator satisfies RDD even when the rows of X follow heavy-tailed distributions.
5
5 Screening under random designs
In this section, we consider linear screening under random designs when X and  are Gaussian.
The theory developed in this section can be easily extended to a broader family of distributions, for
example, where  follows a sub-Gaussian distribution [19] and X follows an elliptical distribution
[11, 15]. We focus on the Gaussian case for conciseness. Let  âˆ¼ N(0, Ïƒ2) and X âˆ¼ N(0,Î£).
We prove the screening consistency of SIS and HOLP by verifying the condition in Theorem 2.
Recall the ancillary matrices for SIS and HOLP are defined respectively as
ASIS = X/n, AHOLP = X
T (XXT )âˆ’1.
For simplicity, we assume Î£ii = 1 for i = 1, 2, Â· Â· Â· , p. To verify the RDD condition, it is essential
to quantify the magnitude of the entries of AX and A.
Lemma 1. Let Î¦ = ASISX , then for any t > 0 and i 6= j âˆˆ Q, we have
P
(
|Î¦ii âˆ’ Î£ii| â‰¥ t
)
â‰¤ 2 exp
{
âˆ’min
(
t2n
8e2K
,
tn
2eK
)}
,
and
P
(
|Î¦ij âˆ’ Î£ij | â‰¥ t
)
â‰¤ 6 exp
{
âˆ’min
(
t2n
72e2K
,
tn
6eK
)}
,
where K = â€–X 2(1) âˆ’ 1â€–Ïˆ1 is a constant, X 2(1) is a chi-square random variable with one degree
of freedom and the norm â€– Â· â€–Ïˆ1 is defined in [19].
Lemma 1 states that the screening matrix Î¦ = ASISX for SIS will eventually converge to the co-
variance matrix Î£ in lâˆ when n tends to infinity and log p = o(n). Thus, the screening performance
of SIS strongly relies on the structure of Î£. In particular, the (asymptotically) necessary and suffi-
cient condition for SIS being strong screening consistent is Î£ satisfying the RDD condition. For
the noise term, we have the following lemma.
Lemma 2. Let Î· = ASIS. For any t > 0 and i âˆˆ Q, we have
P (|Î·i| â‰¥ Ïƒt) â‰¤ 6 exp
{
âˆ’min
(
t2n
72e2K
,
tn
6eK
)}
,
where K is defined the same as in Lemma 1.
The proof of Lemma 2 is essentially the same as the proof of off-diagonal terms in Lemma 1 and
is thus omitted. As indicated before, the necessary and sufficient condition for SIS to be strong
screening consistent is that Î£ follows RDD. As RDD is usually hard to verify, we consider a stronger
sufficient condition inspired by Corollary 1.
Theorem 5. Let r = maxi 6=j |Î£ij |. If r < 12Ïs , then for any Î´ > 0, if the sample size satisfies
n > 144K
(
1 + 2Ïs+ 2Ïƒ/Ï„
1âˆ’ 2Ïsr
)2
log(3p/Î´), (5)
where K is defined in Lemma 1, then with probability at least 1 âˆ’ Î´, Î¦ = ASISX âˆ’
2Ï„âˆ’1â€–ASISâ€–âˆIp is restricted diagonally dominant with sparsity s and C0 â‰¥ Ï. In other words,
SIS is screening consistent for any Î² âˆˆ BÏ„ (s, Ï).
Proof. Taking union bound on the results from Lemma 1 and 2, we have for any t > 0 and p > 2,
P
(
min
iâˆˆQ
Î¦ii â‰¤ 1âˆ’ t or max
i6=j
|Î¦ij | â‰¥ r + t or â€–Î·â€–âˆ â‰¥ Ïƒt
)
â‰¤ 7p2 exp
{
âˆ’ n
K
min
(
t2
72e2
,
t
6e
)}
.
In other words, for any Î´ > 0, when n â‰¥ K log(7p2/Î´), with probability at least 1âˆ’ Î´, we have
min
iâˆˆQ
Î¦ii â‰¥ 1âˆ’ 6
âˆš
2e
âˆš
K log(7p2/Î´)
n
, max
i6=j
|Î¦ij | â‰¤ r + 6
âˆš
2e
âˆš
K log(7p2/Î´)
n
,
6
max
iâˆˆQ
|Î·i| â‰¤ 6
âˆš
2eÏƒ
âˆš
K log(7p2/Î´)
n
.
A sufficient condition for Î¦ to be restricted diagonally dominant is that
min
i
Î¦ii > 2Ïsmax
i6=j
|Î¦ij |+ 2Ï„âˆ’1 max
i
|Î·i|.
Plugging in the values we have
1âˆ’ 6
âˆš
2e
âˆš
K log(7p2/Î´)
n
> 2Ïs(r + 6
âˆš
2e
âˆš
K log(7p2/Î´)
n
) + 12
âˆš
2eÏ„âˆ’1Ïƒ
âˆš
K log(7p2/Î´)
n
.
Solving the above inequality (notice that 7p2/Î´ < 9p2/Î´2 and Ï > 1) completes the proof.
The requirement that maxi6=j |Î£ij | < 1/(Ïsr) or the necessary and sufficient condition that Î£ is
RDD strictly constrains the correlation structure of X , causing the difficulty for SIS to be strong
screening consistent. For HOLP we instead have the following result.
Lemma 3. Let Î¦ = AHOLPX . Assume p > c0n for some c0 > 1, then for any C > 0 there exists
some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i âˆˆ Q, j 6= i, we have
P
(
|Î¦ii| < c1Îºâˆ’1
n
p
)
â‰¤ 2eâˆ’Cn, P
(
|Î¦ii| > c2Îº
n
p
)
â‰¤ 2eâˆ’Cn
and
P
(
|Î¦ij | > c4Îºt
âˆš
n
p
)
â‰¤ 5eâˆ’Cn + 2eâˆ’t
2/2,
where c4 =
âˆš
c2(c0âˆ’c1)âˆš
c3(c0âˆ’1)
.
Proof. The proof of Lemma 3 relies heavily on previous results for the Stiefel Manifold provided in
the supplementary materials. We only sketch the basic idea here and leave the complete proof to the
supplementary materials. Defining H = XT (XXT )âˆ’1/2, then we have Î¦ = HHT and H follows
the Matrix Angular Central Gaussian (MACG) with covariance Î£. The diagonal terms of HHT
can be bounded similarly via the Johnson-Lindenstrauss lemma, by using the fact that HHT =
Î£1/2U(UTÎ£U)âˆ’1UÎ£, where U is a p Ã— n random projection matrix. Now for off-diagonal terms,
we decompose the Stiefel manifold as H = (G(H2)H1 H2), where H1 is a (pâˆ’ n+ 1)Ã— 1 vector,
H2 is a p Ã— (n âˆ’ 1) matrix and G(H2) is chosen so that (G(H2) H2) âˆˆ O(p), and show that H1
follows Angular Central Gaussian (ACG) distribution with covarianceG(H2)TÎ£G(H2) conditional
on H2. It can be shown that e2HHT e1
(d)
= e2G(H2)H1|eT1H2 = 0. Let t21 = eT1HHT e1, then
eT1H2 = 0 is equivalent to e
T
1G(H2)H1 = t1, and we obtain the desired coupling distribution as
eT2HH
T e1
(d)
= eT2G(H2)H1|eT1G(H2)H1 = t1. Using the normal representation of ACG(Î£), i.e.,
if x = (x1, Â· Â· Â· , xp) âˆ¼ N(0,Î£), then x/â€–xâ€– âˆ¼ ACG(Î£), we can write G(H2)H1 in terms of
normal variables and then bound all terms using concentration inequalities.
Lemma 3 quantifies the entries of the screening matrix for HOLP . As illustrated in the lemma,
regardless of the covariance Î£, diagonal terms of Î¦ are always O(np ) and the off-diagonal terms are
O(
âˆš
n
p ). Thus, with n â‰¥ O(s
2), Î¦ is likely to satisfy the RDD condition with large probability. For
the noise vector we have the following result.
Lemma 4. Let Î· = AHOLP . Assume p > c0n for some c0 > 1, then for any C > 0 there exist the
same c1, c2, c3 as in Lemma 3 such that for any t > 0 and i âˆˆ Q,
P
(
|Î·i| â‰¥
2Ïƒ
âˆš
c2Îºt
1âˆ’ câˆ’10
âˆš
n
p
)
< 4eâˆ’Cn + 2eâˆ’t
2/2,
if n â‰¥ 8C/(c0 âˆ’ 1)2.
The proof is almost identical to Lemma 2 and is provided in the supplementary materials. The
following theorem results after combining Lemma 3 and 4.
7
Theorem 6. Assume p > c0n for some c0 > 1. For any Î´ > 0, if the sample size satisfies
n > max
{
2C â€²Îº4(Ïs+ Ïƒ/Ï„)2 log(3p/Î´),
8C
(c0 âˆ’ 1)2
}
, (6)
where C â€² = max{ 4c
2
4
c21
, 4c2
c21(1âˆ’c
âˆ’1
0 )
2
} and c1, c2, c3, c4, C are the same constants defined in Lemma 3,
then with probability at least 1âˆ’ Î´, Î¦ = AHOLPX âˆ’ 2Ï„âˆ’1â€–AHOLP â€–âˆIp is restricted diagonally
dominant with sparsity s and C0 â‰¥ Ï. This implies HOLP is screening consistent for any Î² âˆˆ
BÏ„ (s, Ï).
Proof. Notice that if
min
i
|Î¦ii| > 2sÏmax
ij
|Î¦ij |+ 2Ï„âˆ’1â€–XT (XXT )âˆ’1â€–âˆ, (7)
then the proof is complete because Î¦ âˆ’ 2Ï„âˆ’1â€–XT (XXT )âˆ’1â€–âˆ is already a restricted diagonally
dominant matrix. Let t =
âˆš
Cn/Î½. The above equation then requires
c1Îº
âˆ’1n
p
âˆ’ 2c4
âˆš
CÎºsÏ
Î½
n
p
âˆ’ 2Ïƒ
âˆš
c2CÎºt
(1âˆ’ câˆ’10 )Ï„Î½
n
p
=
(
c1Îº
âˆ’1 âˆ’ 2c4
âˆš
CÎºsÏ
Î½
âˆ’ 2Ïƒ
âˆš
c2CÎº
(1âˆ’ câˆ’10 )Ï„Î½
)n
p
> 0,
which implies that
Î½ >
2c4
âˆš
CÎº2Ïs
c1
+
2Ïƒ
âˆš
c2CÎº
2
c1(1âˆ’ câˆ’10 )Ï„
= C1Îº
2Ïs+ C2Îº
2Ï„âˆ’1Ïƒ > 1,
where C1 = 2c4
âˆš
C
c1
, C2 =
2
âˆš
c2C
c1(1âˆ’câˆ’10 )
. Therefore, taking union bounds on all matrix entries, we
have
P
({
(7) does not hold
})
< (p+ 5p2)eâˆ’Cn + 2p2eâˆ’Cn/Î½ < (7 +
1
n
)p2eâˆ’Cn/Î½
2
,
where the second inequality is due to the fact that p > n and Î½ > 1. Now for any Î´ > 0, (7) holds
with probability at least 1âˆ’ Î´ if
n â‰¥ Î½
2
C
(
log(7 + 1/n) + 2 log pâˆ’ log Î´
)
,
which is satisfied provided (noticing
âˆš
8 < 3) n â‰¥ 2Î½
2
C log
3p
Î´ . Now pushing Î½ to the limit gives (6),
the precise condition we need.
There are several interesting observations on equation (5) and (6). First, (Ïs + Ïƒ/Ï„)2 appears in
both expressions. We note that Ïs evaluates the sparsity and the diversity of the signal Î² while Ïƒ/Ï„
is closely related to the signal-to-noise ratio. Furthermore, HOLP relaxes the correlation constraint
r < 1/(2Ïs) or the covariance constraint (Î£ is RDD) with the conditional number constraint. Thus
for any Î£, as long as the sample size is large enough, strong screening consistency is assured.
Finally, HOLP provides an example to satisfy the RDD condition in answer to the question raised
in Section 4.
6 Concluding remarks
This article studies and establishes a necessary and sufficient condition in the form of restricted
diagonally dominant screening matrices for strong screening consistency of a linear screener. We
verify the condition for both SIS and HOLP under random designs. In addition, we show a
close relationship between RDD and the IC, highlighting the difficulty of using SIS in screening for
arbitrarily correlated predictors. For future work, it is of interest to see how linear screening can be
adapted to compressed sensing [20] and how techniques such as preconditioning [21] can improve
the performance of marginal screening and variable selection.
Acknowledgments This research was partly support by grant NIH R01-ES017436 from the Na-
tional Institute of Environmental Health Sciences.
8
References
[1] David L Donoho. Compressed sensing. IEEE Transactions on Information Theory,
52(4):1289â€“1306, 2006.
[2] Richard Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24(4), 2007.
[3] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Statistical Methodology), 58(1):267â€“288, 1996.
[4] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its
oracle properties. Journal of the American Statistical Association, 96(456):1348â€“1360, 2001.
[5] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is
much larger than n. The Annals of Statistics, 35(6):2313â€“2351, 2007.
[6] Peter J Bickel, Yaâ€™acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and
dantzig selector. The Annals of Statistics, 37(4):1705â€“1732, 2009.
[7] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The
Annals of Statistics, 38(2):894â€“942, 2010.
[8] Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine
Learning Research, 7:2541â€“2563, 2006.
[9] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity
using l1-constrained quadratic programming. IEEE Transactions on Information Theory, 2009.
[10] Jason D Lee, Yuekai Sun, and Jonathan E Taylor. On model selection consistency of m-
estimators with geometrically decomposable penalties. Advances in Neural Processing Infor-
mation Systems, 2013.
[11] Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature
space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849â€“
911, 2008.
[12] Gaorong Li, Heng Peng, Jun Zhang, Lixing Zhu, et al. Robust rank correlation based screening.
The Annals of Statistics, 40(3):1846â€“1877, 2012.
[13] Hansheng Wang. Forward regression for ultra-high dimensional variable screening. Journal
of the American Statistical Association, 104(488):1512â€“1524, 2009.
[14] Haeran Cho and Piotr Fryzlewicz. High dimensional variable selection via tilting. Journal of
the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593â€“622, 2012.
[15] Xiangyu Wang and Chenlei Leng. High-dimensional ordinary least-squares projection for
screening variables. https://stat.duke.edu/Ëœxw56/holp-paper.pdf, 2015.
[16] Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in high-
dimensional linear regression. The Annals of Statistics, 36(4):1567â€“1594, 2008.
[17] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for
correlated gaussian designs. The Journal of Machine Learning Research, 11:2241â€“2259, 2010.
[18] Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. arXiv
preprint arXiv:0912.4045, 2009.
[19] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027, 2010.
[20] Lingzhou Xue and Hui Zou. Sure independence screening and compressed random sensing.
Biometrika, 98(2):371â€“380, 2011.
[21] Jinzhu Jia and Karl Rohe. Preconditioning to comply with the irrepresentable condition. arXiv
preprint arXiv:1208.5584, 2012.
9
