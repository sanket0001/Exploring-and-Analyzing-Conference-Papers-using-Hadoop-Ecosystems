


Paper ID = 6031
Title = Asynchronous stochastic convex optimization:
the noise is in the noise and SGD don’t care
Sorathan Chaturapruek1 John C. Duchi2 Christopher Ré1
Departments of 1Computer Science, 2Electrical Engineering, and 2Statistics
Stanford University
Stanford, CA 94305
{sorathan,jduchi,chrismre}@stanford.edu
Abstract
We show that asymptotically, completely asynchronous stochastic gradient proce-
dures achieve optimal (even to constant factors) convergence rates for the solution
of convex optimization problems under nearly the same conditions required for
asymptotic optimality of standard stochastic gradient procedures. Roughly, the
noise inherent to the stochastic approximation scheme dominates any noise from
asynchrony. We also give empirical evidence demonstrating the strong perfor-
mance of asynchronous, parallel stochastic optimization schemes, demonstrating
that the robustness inherent to stochastic approximation problems allows substan-
tially faster parallel and asynchronous solution methods. In short, we show that
for many stochastic approximation problems, as Freddie Mercury sings in Queen’s
Bohemian Rhapsody, “Nothing really matters.”
1 Introduction
We study a natural asynchronous stochastic gradient method for the solution of minimization prob-
lems of the form
minimize f(x) := EP [F (x;W )] =
∫
Ω
F (x;ω)dP (ω), (1)
where x 7→ F (x;ω) is convex for each ω ∈ Ω, P is a probability distribution on Ω, and the vector
x ∈ Rd. Stochastic gradient techniques for the solution of problem (1) have a long history in
optimization, starting from the early work of Robbins and Monro [19] and continuing on through
Ermoliev [7], Polyak and Juditsky [16], and Nemirovski et al. [14]. The latter two show how certain
long stepsizes and averaging techniques yield more robust and asymptotically optimal optimization
schemes, and we show how their results extend to practical parallel and asynchronous settings.
We consider an extension of previous stochastic gradient methods to a natural family of asyn-
chronous gradient methods [3], where multiple processors can draw samples from the distribution
P and asynchronously perform updates to a centralized (shared) decision vector x. Our iterative
scheme is based on the HOGWILD! algorithm of Niu et al. [15], which is designed to asynchronously
solve certain stochastic optimization problems in multi-core environments, though our analysis and
iterations are different. In particular, we study the following procedure, where each processor runs
asynchronously and independently of the others, though they maintain a shared integer iteration
counter k; each processor P asynchronously performs the following:
(i) Processor P reads current problem data x
(ii) Processor P draws a random sample W ∼ P , computes g = ∇F (x;W ), and increments the
centralized counter k
(iii) Processor P updates x ← x − αkg sequentially for each coordinate j = 1, 2, . . . , d by incre-
menting [x]j ← [x]j − αk[g]j , where the scalars αk are a non-increasing stepsize sequence.
1
We assume that scalar addition is atomic: the addition of −αk[g]j propagates eventually, but maybe
out of order. Our main results show that because of the noise inherent to the sampling process for
W , the errors introduced by asynchrony in iterations (i)–(iii) are asymptotically negligible: they do
not matter. Even more, we can efficiently construct an x from the asynchronous process possessing
optimal convergence rate and asymptotic variance. This has consequences for solving stochastic
optimization problems on multi-core and multi-processor systems; we can leverage parallel com-
puting without performing any synchronization, so that given a machine withm processors, we can
read data and perform updates m times as quickly as with a single processor, and the error from
reading stale information on x becomes asymptotically negligible. In Section 2, we state our main
convergence theorems about the asynchronous iteration (i)–(iii) for solving problem (1). Our main
result, Theorem 1, gives explicit conditions under which our results hold, and we give applications
to specific stochastic optimization problems as well as a general result for asynchronous solution of
operator equations. Roughly, all we require for our (optimal) convergence results is that the Hessian
of f be positive definite near x⋆ = argminx f(x) and that the gradients ∇f(x) be smooth.
Several researchers have provided and analyzed asynchronous algorithms for optimization. Bert-
sekas and Tsitsiklis [3] provide a comprehensive study both of models of asynchronous computation
and analyses of asynchronous numerical algorithms. More recent work has studied asynchronous
gradient procedures, though it often imposes strong conditions on gradient sparsity, conditioning of
the Hessian of f , or allowable types of asynchrony; as we show, none are essential. Niu et al. [15]
propose HOGWILD! and show that under sparsity and smoothness assumptions (essentially, that
the gradients ∇F (x;W ) have a vanishing fraction of non-zero entries, that f is strongly convex,
and ∇F (x;ω) is Lipschitz for all ω), convergence guarantees similar to the synchronous case are
possible; Agarwal and Duchi [1] showed under restrictive ordering assumptions that some delayed
gradient calculations have negligible asymptotic effect; and Duchi et al. [4] extended Niu et al.’s
results to a dual averaging algorithm that works for non-smooth, non strongly-convex problems,
so long as certain gradient sparsity assumptions hold. Researchers have also investigated parallel
coordinate descent solvers; Richtárik and Takáč [18] and Liu et al. [13] show how certain “near-
separability” properties of an objective function f govern convergence rate of parallel coordinate
descent methods, the latter focusing on asynchronous schemes. As we show, large-scale stochastic
optimization renders many of these problem assumptions unnecessary.
In addition to theoretical results, in Section 3 we give empirical results on the power of parallelism
and asynchrony in the implementation of stochastic approximation procedures. Our experiments
demonstrate two results: first, even in non-asymptotic finite-sample settings, asynchrony introduces
little degradation in solution quality, regardless of data sparsity (a common assumption in previous
analyses); that is, asynchronously-constructed estimates are statistically efficient. Second, we show
that there is some subtlety in implementation of these procedures in real hardware; while increases in
parallelism lead to concomitant linear improvements in the speed with which we compute solutions
to problem (1), in some cases we require strategies to reduce hardware resource competition between
processors to achieve the full benefits of asynchrony.
Notation A sequence of random variables or vectors Xn converges in distribution to Z, denoted
Xn
d→ Z, if E[f(Xn)] → E[f(Z)] for all bounded continuous functions f . We let Xn
p→ Z denote
convergence in probability, meaning that limn P(‖Xn − Z‖ > ǫ) = 0 for any ǫ > 0. The notation
N(µ,Σ) denotes the multivariate Gaussian with mean µ and covariance Σ.
2 Main results
Our main results repose on a few standard assumptions often used for the analysis of stochastic op-
timization procedures, which we now detail, along with a few necessary definitions. We let k denote
the iteration counter used throughout the asynchronous gradient procedure. Given that we compute
g = ∇F (x;W ) with counter value k in the iterations (i)–(iii), we let xk denote the (possibly in-
consistent) particular x used to compute g, and likewise say that g = gk, noting that the update to
x is then performed using αk. In addition, throughout paper, we assume there is some finite bound
M < ∞ such that no processor reads information more than M steps out of date.
2.1 Asynchronous convex optimization
We now present our main theoretical results for solving the stochastic convex problem (1), giving
the necessary assumptions on f and F (·;W ) for our results. Our first assumption roughly states that
f has quadratic expansion near the (unique) optimal point x⋆ and is smooth.
2
Assumption A. The function f has unique minimizer x⋆ and is twice continuously differentiable in
the neighborhood of x⋆ with positive definite HessianH = ∇2f(x⋆) ≻ 0 and there is a covariance
matrix Σ ≻ 0 such that
E[∇F (x⋆;W )∇F (x⋆;W )⊤] = Σ.
Additionally, there exists a constant C < ∞ such that the gradients ∇F (x;W ) satisfy
E[‖∇F (x;W )−∇F (x⋆;W )‖2] ≤ C ‖x− x⋆‖2 for all x ∈ Rd. (2)
Lastly, f has L-Lipschitz continuous gradient: ‖∇f(x)−∇f(y)‖ ≤ L ‖x− y‖ for all x, y ∈ Rd.
Assumption A guarantees the uniqueness of the vector x⋆ minimizing f(x) over Rd and ensures that
f is well-behaved enough for our asynchronous iteration procedure to introduce negligible noise
over a non-asynchronous procedure. In addition to Assumption A, we make one of two additional
assumptions. In the first case, we assume that f is strongly convex:
Assumption B. The function f is λ-strongly convex over all of Rd for some λ > 0, that is,
f(y) ≥ f(x) + 〈∇f(x), y − x〉+ λ
2
‖x− y‖2 for x, y ∈ Rd. (3)
Our alternate assumption is a Lipschitz assumption on f itself, made by virtue of a second moment
bound on ∇F (x;W ).
Assumption B’. There exists a constant G < ∞ such that for all x ∈ Rd,
E[‖∇F (x;W )‖2] ≤ G2. (4)
With our assumptions in place, we state our main theorem.
Theorem 1. Let the iterates xk be generated by the asynchronous process (i), (ii), (iii) with stepsize
choice αk = αk
−β , where β ∈ ( 1
2
, 1) and α > 0. Let Assumption A and either of Assumptions B
or B’ hold. Then
1√
n
n∑
k=1
(xk − x⋆) d→ N
(
0, H−1ΣH−1
)
= N
(
0, (∇2f(x⋆))−1Σ(∇2f(x⋆))−1
)
.
Before moving to example applications of Theorem 1, we note that its convergence guarantee is
generally unimprovable even by numerical constants. Indeed, for classical statistical problems, the
covariance H−1ΣH−1 is the inverse Fisher information, and by the Le Cam-Hájek local minimax
theorems [9] and results on Bahadur efficiency [21, Chapter 8], this is the optimal covariance matrix,
and the best possible rate is n−
1
2 . As for function values, using the delta method [e.g. 10, Theorem
1.8.12], we can show the optimal convergence rate of 1/n on function values.
Corollary 1. Let the conditions of Theorem 1 hold. Then n
(
f
(
1
n
∑n
k=1 xk
)
− f(x⋆)
) d→
1
2
tr
[
H−1Σ
]
· χ21, where χ21 denotes a chi-squared random variable with 1 degree of freedom, and
H = ∇2f(x⋆) and Σ = E[∇F (x⋆;W )∇F (x⋆;W )⊤].
2.2 Examples
We now give two classical statistical optimization problems to illustrate Theorem 1. We verify that
the conditions of Assumptions A and B or B’ are not overly restrictive.
Linear regression Standard linear regression problems satisfies the conditions of Assumption B.
In this case, the data ω = (a, b) ∈ Rd × R and the objective F (x;ω) = 1
2
(〈a, x〉 − b)2. If we have
moment bounds E[‖a‖4
2
] < ∞, E[b2] < ∞ and H = E[aa⊤] ≻ 0, we have ∇2f(x⋆) = H , and
the assumptions of Theorem 1 are certainly satisfied. Standard modeling assumptions yield more
concrete guarantees. For example, if b = 〈a, x⋆〉 + ε where ε is independent mean-zero noise with
E[ε2] = σ2, the minimizer of f(x) = E[F (x;W )] is x⋆, we have 〈a, x⋆〉 − b = −ε, and
E[∇F (x⋆;W )∇F (x⋆;W )⊤] = E[(〈a, x⋆〉−b)aa⊤(〈a, x⋆〉−b)] = E[aa⊤ε2] = σ2E[aa⊤] = σ2H.
In particular, the asynchronous iterates satisfy
1√
n
n∑
k=1
(xk − x⋆) d→ N(0, σ2H−1) = N
(
0, σ2E[aa⊤]−1
)
,
which is the (minimax optimal) asymptotic covariance of the ordinary least squares estimate of x⋆.
3
Logistic regression As long as the data has finite second moment, logistic regression problems
satisfy all the conditions of Assumption B’ in Theorem 1. We have ω = (a, b) ∈ Rd × {−1, 1} and
instantaneous objective F (x;ω) = log(1 + exp(−b 〈a, x〉)). For fixed ω, this function is Lipschitz
continuous and has gradient and Hessian
∇F (x;ω) = − 1
1 + exp(b 〈a, x〉)ba and ∇
2F (x;ω) =
eb〈a,x〉
(1 + eb〈a,x〉)2
aa⊤,
where ∇F (x;ω) is Lipschitz continuous as ‖∇2F (x; (a, b))‖ ≤ 1
4
‖a‖2
2
. So long as E[‖a‖2
2
] < ∞
and E[∇2F (x⋆;W )] ≻ 0 (i.e. E[aa⊤] is positive definite), Theorem 1 applies to logistic regression.
2.3 Extension to nonlinear problems
We prove Theorem 1 by way of a more general result on finding the zeros of a residual operator
R : Rd → Rd, where we only observe noisy views of R(x), and there is unique x⋆ such that
R(x⋆) = 0. Such situations arise, for example, in the solution of stochastic monotone operator
problems (cf. Juditsky, Nemirovski, and Tauvel [8]). In this more general setting, we consider the
following asynchronous iterative process, which extends that for the convex case outlined previously.
Each processor P performs the following asynchronously and independently:
(i) Processor P reads current problem data x
(ii) Processor P receives vector g = R(x) + ξ, where ξ is a random (conditionally) mean-zero
noise vector, and increments a centralized counter k
(iii) Processor P updates x ← x − αkg sequentially for each coordinate j = 1, 2, . . . , d by incre-
menting [x]j = [x]j − αk[g]j .
As in the convex case, we associate vectors xk and gk with the update performed using αk, and we
let ξk denote the noise vector used to construct gk. These iterates and assignment of indices imply
that xk has the form
xk = −
k−1∑
i=1
αiE
kigi, (5)
where Eki ∈ {0, 1}d×d is a diagonal matrix whose jth diagonal entry captures that coordinate j of
the ith gradient has been incorporated into iterate xk.
We define the an increasing sequence of σ-fields Fk by
Fk = σ
(
ξ1, . . . , ξk,
{
Eij : i ≤ k + 1, j ≤ i
})
, (6)
that is, the noise variables ξk are adapted to the filtration Fk, and these σ-fields are the smallest
containing both the noise and all index updates that have occurred and that will occur to compute
xk+1. Thus we have xk+1 ∈ Fk, and our mean-zero assumption on the noise ξ is
E[ξk | Fk−1] = 0.
We base our analysis on Polyak and Juditsky’s study [16] of stochastic approximation procedures,
so we enumerate a few more requirements—modeled on theirs—for our results on convergence of
the asynchronous iterations for solving the nonlinear equality R(x⋆) = 0. We assume there is a
Lyapunov function V satisfying V (x) ≥ λ ‖x‖2 for all x ∈ Rd, ‖∇V (x)−∇V (y)‖ ≤ L ‖x− y‖
for all x, y, that ∇V (0) = 0, and V (0) = 0. This implies
λ ‖x‖2 ≤ V (x) ≤ V (0) + 〈∇V (0), x− 0〉+ L
2
‖x‖2 = L
2
‖x‖2 (7)
and ‖∇V (x)‖2 ≤ L2 ‖x‖2 ≤ (L2/λ)V (x). We make the following assumptions on the residual R.
Assumption C. There exists a matrix H ∈ Rd×d with H ≻ 0, a parameter 0 < γ ≤ 1, constant
C < ∞, and ǫ > 0 such that if x satisfies ‖x− x⋆‖ ≤ ǫ,
‖R(x)−H(x− x⋆)‖ ≤ C ‖x− x⋆‖1+γ .
4
Assumption C essentially requires that R is differentiable at x⋆ with derivative matrix H ≻ 0. We
also make a few assumptions on the noise process ξ; specifically, we assume ξ implicitly depends
on x ∈ Rd (so that we may write ξk = ξ(xk)), and that the following assumption holds.
Assumption D. The noise vector ξ(x) decomposes as ξ(x) = ξ(0) + ζ(x), where ξ(0) is a process
satisfying E[ξk(0)ξk(0)
⊤ | Fk−1]
p→ Σ ≻ 0 for a matrixΣ ∈ Rd×d, supk E[‖ξk(0)‖2 | Fk−1] < ∞
with probability 1, and E[‖ζk(x)‖2 | Fk−1] ≤ C ‖x− x⋆‖2 for a constant C < ∞ and all x ∈ Rd.
As in the convex case, we make one of two additional assumptions, which should be compared with
Assumptions B and B’. The first is that R gives globally strong information about x⋆.
Assumption E (Strongly convex residuals). There exists a constant λ0 > 0 such that for all x ∈ Rd,
〈∇V (x− x⋆), R(x)〉 ≥ λ0V (x− x⋆).
Alternatively, we may make an assumption on the boundedness of R, which we shall see suffices
for proving our main results.
Assumption E’ (Bounded residuals). There exist λ0 > 0 and ǫ > 0 such that
inf
0<‖x−x⋆‖≤ǫ
〈∇V (x− x⋆), R(x)〉
V (x− x⋆) ≥ λ0 and infǫ<‖x−x⋆‖ 〈∇V (x− x
⋆), R(x)〉 > 0.
In addition there exists C < ∞ such that, ‖R(x)‖ ≤ C and E[‖ξk‖2 | Fk−1] ≤ C2 for all k and x.
With these assumptions in place, we obtain the following more general version of Theorem 1; in-
deed, we show that Theorem 1 is a consequence of this result.
Theorem 2. Let V be a function satisfying inequality (7), and let Assumptions C and D hold. Let
the stepsizes αk = αk
−β , where 1
1+γ
< β < 1. Let one of Assumptions E or E’ hold. Then
1√
n
n∑
k=1
(xk − x⋆) d→ N
(
0, H−1ΣH−1
)
.
We may compare this result to Polyak and Juditsky’s Theorem 2 [16], which gives identical asymp-
totic convergence guarantees but with somewhat weaker conditions on the function V and stepsize
sequence αk. Our stronger assumptions, however, allow our result to apply even in fully asyn-
chronous settings.
2.4 Proof sketch
We provide rigorous proofs in the long version of this paper [5], providing an amputated sketch
here. First, to show that Theorem 1 follows from Theorem 2, we set R(x) = ∇f(x) and V (x) =
1
2
‖x‖2. We can then show that Assumption A, which guarantees a second-order Taylor expansion,
implies Assumption C with γ = 1 and H = ∇2f(x⋆). Moreover, Assumption B (or B’) implies
Assumption E (respectively, E’), while to see that Assumption D holds, we set ξ(0) = ∇F (x⋆;W ),
taking Σ = E[∇F (x⋆;W )∇F (x⋆;W )⊤] and ζ(x) = ∇F (x;W ) − ∇F (x⋆;W ), and applying
inequality (2) of Assumption A to satisfy Assumption D with the vector ζ.
The proof of Theorem 2 is somewhat more involved. Roughly, we show the asymptotic equivalence
of the sequence xk from expression (5) to the easier to analyze sequence x̃k = −
∑k−1
i=1 αigi.
Asymptotically, we obtainE[‖xk − x̃k‖2] = O(α2k), while the iterates x̃k—in spite of their incorrect
gradient calculations—are close enough to a correct stochastic gradient iterate that they possess
optimal asymptotic normality properties. This “close enough” follows by virtue of the squared error
bounds for ζ in Assumption D, which guarantee that ξk essentially behaves like an i.i.d. sequence
asymptotically (after application of the Robbins-Siegmund martingale convergence theorem [20]),
which we then average to obtain a central-limit-theorem.
3 Experimental results
We provide empirical results studying the performance of asynchronous stochastic approximation
schemes on several simulated and real-world datasets. Our theoretical results suggest that asyn-
chrony should introduce little degradation in solution quality, which we would like to verify; we
5
also investigate the engineering techniques necessary to truly leverage the power of asynchronous
stochastic procedures. In our experiments, we focus on linear and logistic regression, the ex-
amples given in Section 2.2; that is, we have data (ai, bi) ∈ Rd × R (for linear regression) or
(ai, bi) ∈ Rd × {−1, 1} (for logistic regression), for i = 1, . . . , N , and objectives
f(x) =
1
2N
N∑
i=1
(〈ai, x〉 − bi)2 and f(x) =
1
N
N∑
i=1
log
(
1 + exp(−bi 〈ai, x〉)
)
. (8)
We perform each of our experiments using a 48-core Intel Xeon machine with 1 terabyte of RAM,
and have put code and binaries to replicate our experiments on CodaLab [6]. The Xeon architecture
puts each core onto one of four sockets, where each socket has its own memory. To limit the impact
of communication overhead in our experiments, we limit all experiments to at most 12 cores, all
on the same socket. Within an experiment—based on the empirical expectations (8)—we iterate in
epochs, meaning that our stochastic gradient procedure repeatedly loops through all examples, each
exactly once.1 Within an epoch, we use a fixed stepsize α, decreasing the stepsize by a factor of .9
between each epoch (this matches the experimental protocol of Niu et al. [15]). Within each epoch,
we choose examples in a randomly permuted order, where the order changes from epoch to epoch
(cf. [17]). To address issues of hardware resource contention (see Section 3.2 for more on this), in
some cases we use a mini-batching strategy. Abstractly, in the formulation of the basic problem (1),
this means that in each calculation of a stochastic gradient g we draw B ≥ 1 samples W1, . . . ,WB
i.i.d. according to P , then set
g(x) =
1
B
B∑
b=1
∇F (x;Wb). (9)
The mini-batching strategy (9) does not change the (asymptotic) convergence guarantees of asyn-
chronous stochastic gradient descent, as the covariance matrix Σ = E[g(x⋆)g(x⋆)⊤] satisfies
Σ = 1
B
E[∇F (x⋆;W )∇F (x⋆;W )⊤], while the total iteration count is reduced by the a factor B.
Lastly, we measure the performance of optimization schemes via speedup, defined as
speedup =
average epoch runtime on a single core using HOGWILD!
average epoch runtime on m cores
. (10)
In our experiments, as increasing the number m of cores does not change the gap in optimality
f(xk)− f(x⋆) after each epoch, speedup is equivalent to the ratio of the time required to obtain an
ǫ-accurate solution using a single processor/core to that required to obtain ǫ-accurate solution using
m processors/cores.
3.1 Efficiency and sparsity
For our first set of experiments, we study the effect that data sparsity has on the convergence behavior
of asynchronous methods—sparsity has been an essential part of the analysis of many asynchronous
and parallel optimization schemes [15, 4, 18], while our theoretical results suggest it should be
unimportant—using the linear regression objective (8). We generate synthetic linear regression prob-
lems with N = 106 examples in d = 103 dimensions via the following procedure. Let ρ ∈ (0, 1]
be the desired fraction of non-zero gradient entries, and let Πρ be a random projection operator
that zeros out all but a fraction ρ of the elements of its argument, meaning that for a ∈ Rd, Πρ(a)
uniformly at random chooses ρd elements of a, leaves them identical, and zeroes the remaining
elements. We generate data for our linear regression drawing a random vector u⋆ ∼ N(0, I), then
constructing bi = 〈ai, u⋆〉+ εi, i = 1, . . . , N , where εi i.i.d.∼ N(0, 1), ai = Πρ(ãi), ãi i.i.d.∼ N(0, I),
and Πρ(ãi) denotes an independent random sparse projection of ãi. To measure optimality gap, we
directly compute x⋆ = (ATA)−1AT b, where A = [a1 a2 · · · aN ]⊤ ∈ RN×d.
In Figure 1, we plot the results of simulations using densities ρ ∈ {.005, .01, .2, 1} and mini-batch
size B = 10, showing the gap f(xk) − f(x⋆) as a function of the number of epochs for each of
the given sparsity levels. We give results using 1, 2, 4, and 10 processor cores (increasing degrees
of asynchrony), and from the plots, we see that regardless of the number of cores, the convergence
1Strictly speaking, this violates the stochastic gradient assumption, but it allows direct comparison with the
original HOGWILD! code and implementation [15].
6
behavior is nearly identical, with very minor degradations in performance for the sparsest data. (We
plot the gaps f(xk) − f(x⋆) on a logarithmic axis.) Moreover, as the data becomes denser, the
more asynchronous methods—larger number of cores—achieve performance essentially identical
to the fully synchronous method in terms of convergence versus number of epochs. In Figure 2,
we plot the speedup achieved using different numbers of cores. We also include speedup achieved
using multiple cores with explicit synchronization (locking) of the updates, meaning that instead
of allowing asynchronous updates, each of the cores globally locks the decision vector when it
reads, unlocks and performs mini-batched gradient computations, and locks the vector again when
it updates the vector. We can see that the performance curve is much worse than than the without-
locking performance curve across all densities. That the locking strategy also gains some speedup
when the density is higher is likely due to longer computation of the gradients. However, the locking-
strategy performance is still not competitive with that of the without-locking strategy.
0 5 10 15 20
10-4
10-3
10-2
10-1
100
10 cores
8 cores
4 cores
1 core
f
(x
k
)
−
f
(x
⋆
)
Epochs
0 5 10 15 20
10-4
10-2
100
10 cores
8 cores
4 cores
1 core
Epochs
0 5 10 15 20
10-2
10-1
100
101
10 cores
8 cores
4 cores
1 core
Epochs
0 5 10 15 20
10-4
10-2
100
102
10 cores
8 cores
4 cores
1 core
Epochs
(a) ρ = .005 (b) ρ = .01 (c) ρ = .2 (d) ρ = 1
Figure 1. (Exponential backoff stepsizes) Optimality gaps for synthetic linear regression experiments
showing effects of data sparsity and asynchrony on f(xk)−f(x
⋆). A fraction ρ of each vector ai ∈ R
d
is non-zero.
2 4 6 8 10
0
2
4
6
8
10
linear speedup
without locking
with locking
Cores
2 4 6 8 10
0
2
4
6
8
10
linear speedup
without locking
with locking
Cores
2 4 6 8 10
0
2
4
6
8
10
linear speedup
without locking
with locking
Cores
2 4 6 8 10
0
2
4
6
8
10
linear speedup
without locking
with locking
Cores
(a) ρ = .005 (b) ρ = .01 (c) ρ = .2 (d) ρ = 1
Figure 2. (Exponential backoff stepsizes) Speedups for synthetic linear regression experiments show-
ing effects of data sparsity on speedup (10). A fraction ρ of each vector ai ∈ R
d is non-zero.
3.2 Hardware issues and cache locality
We detail a small set of experiments investigating hardware issues that arise even in implementation
of asynchronous gradient methods. The Intel x86 architecture (as with essentially every processor
architecture) organizes memory in a hierarchy, going from L1 to L3 (level 1 to level 3) caches of
increasing sizes. An important aspect of the speed of different optimization schemes is the relative
fraction of memory hits, meaning accesses to memory that is cached locally (in order of decreasing
speed, L1, L2, or L3 cache). In Table 1, we show the proportion of cache misses at each level of the
memory hierarchy for our synthetic regression experiment with fully dense data (ρ = 1) over the
execution of 20 epochs, averaged over 10 different experiments. We compare memory contention
when the batch size B used to compute the local asynchronous gradients (9) is 1 and 10. We see
that the proportion of misses for the fastest two levels—1 and 2—of the cache for B = 1 increase
significantly with the number of cores, while increasing the batch size to B = 10 substantially
mitigates cache incoherency. In particular, we maintain (near) linear increases in iteration speed
with little degradation in solution quality (the gap f(x̂) − f(x⋆) output by each of the procedures
with and without batching is identical to within 10−3; cf. Figure 1(d)).
7
No batching (B = 1)
Number of cores 1 4 8 10
fraction of L1 misses 0.0009 0.0017 0.0025 0.0026
fraction of L2 misses 0.5638 0.6594 0.7551 0.7762
fraction of L3 misses 0.6152 0.4528 0.3068 0.2841
epoch average time (s) 4.2101 1.6577 1.4052 1.3183
speedup 1.00 2.54 3.00 3.19
Batch size B = 10
Number of cores 1 4 8 10
fraction of L1 misses 0.0012 0.0011 0.0011 0.0011
fraction of L2 misses 0.5420 0.5467 0.5537 0.5621
fraction of L3 misses 0.5677 0.5895 0.5714 0.5578
epoch average time (s) 4.4286 1.1868 0.6971 0.6220
speedup 1.00 3.73 6.35 7.12
Table 1. Memory traffic for batched updates (9) versus non-batched updates (B = 1) for a dense
linear regression problem in d = 103 dimensions with a sample of size N = 106. Cache misses are
substantially higher with B = 1.
3.3 Real datasets
We perform experiments using three different real-world datasets: the Reuters RCV1 corpus [11],
the Higgs detection dataset [2], and the Forest Cover dataset [12]. Each represents a binary classifi-
cation problem which we formulate using logistic regression. We briefly detail statistics for each:
(1) Reuters RCV1 dataset consists of N ≈ 7.81 · 105 data vectors (documents) ai ∈ {0, 1}d with
d ≈ 5 · 104 dimensions; each vector has sparsity approximately ρ = 3 · 10−3. Our task is to
classify each document as being about corporate industrial topics (CCAT) or not.
(2) The Higgs detection dataset consists of N = 106 data vectors ãi ∈ Rd0 , with d0 = 28. We
quantize each coordinate into 5 bins containing equal fraction of the coordinate values and
encode each vector ãi as a vector ai ∈ {0, 1}5d0 whose non-zero entries correspond to quantiles
into which coordinates fall. The task is to detect (simulated) emissions from a linear accelerator.
(3) The Forest Cover dataset consists of N ≈ 5.7 · 105 data vectors ai ∈ {−1, 1}d with d = 54,
and the task is to predict forest growth types.
0 5 10 15 20
10-3
10-2
10-1
10 cores
8 cores
4 cores
1 core
f
(x
k
)
−
f
(x
⋆
)
Epochs
0 5 10 15 20
10-3
10-2
10-1
10 cores
8 cores
4 cores
1 core
Epochs
0 5 10 15 20
10-3
10-2
10-1
10 cores
8 cores
4 cores
1 core
Epochs
Figure 3. (Exponential
backoff stepsizes) Op-
timality gaps f(xk) −
f(x⋆) on the (a) RCV1,
(b) Higgs, and (c) Forest
Cover datasets.
(a) RCV1 (ρ = .003) (b) Higgs (ρ = 1) (c) Forest (ρ = 1)
2 4 6 8 10
1
2
3
4
5
6
7
8
9
10
linear speedup
without locking
Cores
2 4 6 8 10
1
2
3
4
5
6
7
8
9
10
linear speedup
without locking
Cores
2 4 6 8 10
1
2
3
4
5
6
7
8
9
10
linear speedup
without locking
Cores
Figure 4. (Exponen-
tial backoff stepsizes)
Logistic regression
experiments showing
speedup (10) on the
(a) RCV1, (b) Higgs,
and (c) Forest Cover
datasets.
(a) RCV1 (ρ = .003) (b) Higgs (ρ = 1) (c) Forest (ρ = 1)
In Figure 3, we plot the gap f(xk) − f(x⋆) as a function of epochs, giving standard error intervals
over 10 runs for each experiment. There is essentially no degradation in objective value for the
different numbers of processors, and in Figure 4, we plot speedup achieved using 1, 4, 8, and 10
cores with batch sizes B = 10. Asynchronous gradient methods achieve speedup of between 6×
and 8× on each of the datasets using 10 cores.
8
References
[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in
Neural Information Processing Systems 24, 2011.
[2] P. Baldi, P. Sadowski, and D. Whiteson. Searching for exotic particles in high-energy physics
with deep learning. Nature Communications, 5, July 2014.
[3] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Meth-
ods. Prentice-Hall, Inc., 1989.
[4] J. C. Duchi, M. I. Jordan, and H. B. McMahan. Estimation, optimization, and parallelism when
data is sparse. In Advances in Neural Information Processing Systems 26, 2013.
[5] J. C. Duchi, S. Chaturapruek, and C. Ré. Asynchronous stochastic convex optimization.
arXiv:1508.00882 [math.OC], 2015.
[6] J. C. Duchi, S. Chaturapruek, and C. Ré. Asynchronous stochastic convex optimization, 2015.
URL https://www.codalab.org/worksheets/. Code for reproducing experiments.
[7] Y. M. Ermoliev. On the stochastic quasi-gradient method and stochastic quasi-Feyer sequences.
Kibernetika, 2:72–83, 1969.
[8] A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with the stochastic
mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011.
[9] L. Le Cam and G. L. Yang. Asymptotics in Statistics: Some Basic Concepts. Springer, 2000.
[10] E. L. Lehmann and G. Casella. Theory of Point Estimation, Second Edition. Springer, 1998.
[11] D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new benchmark collection for text catego-
rization research. Journal of Machine Learning Research, 5:361–397, 2004.
[12] M. Lichman. UCI machine learning repository, 2013. URL
http://archive.ics.uci.edu/ml.
[13] J. Liu, S. J. Wright, C. Ré, V. Bittorf, and S. Sridhar. An asynchronous parallel stochastic
coordinate descent algorithm. In Proceedings of the 31st International Conference on Machine
Learning, 2014.
[14] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach
to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.
[15] F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: a lock-free approach to parallelizing stochas-
tic gradient descent. In Advances in Neural Information Processing Systems 24, 2011.
[16] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM
Journal on Control and Optimization, 30(4):838–855, 1992.
[17] B. Recht and C. Ré. Beneath the valley of the noncommutative arithmetic-geometric mean
inequality: conjectures, case-studies, and consequences. In Proceedings of the Twenty Fifth
Annual Conference on Computational Learning Theory, 2012.
[18] P. Richtárik and M. Takáč. Parallel coordinate descent methods for big data
optimization. Mathematical Programming, page Online first, 2015. URL
http://link.springer.com/article/10.1007/s10107-015-0901-6.
[19] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical
Statistics, 22:400–407, 1951.
[20] H. Robbins and D. Siegmund. A convergence theorem for non-negative almost supermartin-
gales and some applications. In Optimizing Methods in Statistics, pages 233–257. Academic
Press, New York, 1971.
[21] A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic
Mathematics. Cambridge University Press, 1998. ISBN 0-521-49603-9.
9
