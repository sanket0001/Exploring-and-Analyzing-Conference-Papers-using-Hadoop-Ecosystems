


Paper ID = 5936
Title = Less is More: NystroÃàm Computational Regularization
Alessandro Rudi‚Ä† Raffaello Camoriano‚Ä†‚Ä° Lorenzo Rosasco‚Ä†‚ó¶
‚Ä†UniversitaÃÄ degli Studi di Genova - DIBRIS, Via Dodecaneso 35, Genova, Italy
‚Ä°Istituto Italiano di Tecnologia - iCub Facility, Via Morego 30, Genova, Italy
‚ó¶Massachusetts Institute of Technology and Istituto Italiano di Tecnologia
Laboratory for Computational and Statistical Learning, Cambridge, MA 02139, USA
{ale rudi, lrosasco}@mit.edu raffaello.camoriano@iit.it
Abstract
We study NystroÃàm type subsampling approaches to large scale kernel methods,
and prove learning bounds in the statistical learning setting, where random sam-
pling and high probability estimates are considered. In particular, we prove that
these approaches can achieve optimal learning bounds, provided the subsampling
level is suitably chosen. These results suggest a simple incremental variant of
NystroÃàm Kernel Regularized Least Squares, where the subsampling level im-
plements a form of computational regularization, in the sense that it controls at
the same time regularization and computations. Extensive experimental analy-
sis shows that the considered approach achieves state of the art performances on
benchmark large scale datasets.
1 Introduction
Kernel methods provide an elegant and effective framework to develop nonparametric statistical
approaches to learning [1]. However, memory requirements make these methods unfeasible when
dealing with large datasets. Indeed, this observation has motivated a variety of computational strate-
gies to develop large scale kernel methods [2‚Äì8].
In this paper we study subsampling methods, that we broadly refer to as NystroÃàm approaches. These
methods replace the empirical kernel matrix, needed by standard kernel methods, with a smaller
matrix obtained by (column) subsampling [2, 3]. Such procedures are shown to often dramatically
reduce memory/time requirements while preserving good practical performances [9‚Äì12]. The goal
of our study is two-fold. First, and foremost, we aim at providing a theoretical characterization of the
generalization properties of such learning schemes in a statistical learning setting. Second, we wish
to understand the role played by the subsampling level both from a statistical and a computational
point of view. As discussed in the following, this latter question leads to a natural variant of Kernel
Regularized Least Squares (KRLS), where the subsampling level controls both regularization and
computations.
From a theoretical perspective, the effect of NystroÃàm approaches has been primarily character-
ized considering the discrepancy between a given empirical kernel matrix and its subsampled ver-
sion [13‚Äì19]. While interesting in their own right, these latter results do not directly yield infor-
mation on the generalization properties of the obtained algorithm. Results in this direction, albeit
suboptimal, were first derived in [20] (see also [21,22]), and more recently in [23,24]. In these latter
papers, sharp error analyses in expectation are derived in a fixed design regression setting for a form
of Kernel Regularized Least Squares. In particular, in [23] a basic uniform sampling approach is
studied, while in [24] a subsampling scheme based on the notion of leverage score is considered.
The main technical contribution of our study is an extension of these latter results to the statistical
learning setting, where the design is random and high probability estimates are considered. The
1
more general setting makes the analysis considerably more complex. Our main result gives opti-
mal finite sample bounds for both uniform and leverage score based subsampling strategies. These
methods are shown to achieve the same (optimal) learning error as kernel regularized least squares,
recovered as a special case, while allowing substantial computational gains. Our analysis highlights
the interplay between the regularization and subsampling parameters, suggesting that the latter can
be used to control simultaneously regularization and computations. This strategy implements a
form of computational regularization in the sense that the computational resources are tailored to
the generalization properties in the data. This idea is developed considering an incremental strat-
egy to efficiently compute learning solutions for different subsampling levels. The procedure thus
obtained, which is a simple variant of classical NystroÃàm Kernel Regularized Least Squares with uni-
form sampling, allows for efficient model selection and achieves state of the art results on a variety
of benchmark large scale datasets.
The rest of the paper is organized as follows. In Section 2, we introduce the setting and algorithms
we consider. In Section 3, we present our main theoretical contributions. In Section 4, we discuss
computational aspects and experimental results.
2 Supervised learning with KRLS and NystroÃàm approaches
LetX√óR be a probability space with distribution œÅ, where we viewX and R as the input and output
spaces, respectively. Let œÅX denote the marginal distribution of œÅ on X and œÅ(¬∑|x) the conditional
distribution on R given x ‚àà X . Given a hypothesis space H of measurable functions from X to R,
the goal is to minimize the expected risk,
min
f‚ààH
E(f), E(f) =
‚à´
X√óR
(f(x)‚àí y)2dœÅ(x, y), (1)
provided œÅ is known only through a training set of (xi, yi)ni=1 sampled identically and independently
according to œÅ. A basic example of the above setting is random design regression with the squared
loss, in which case
yi = f‚àó(xi) + i, i = 1, . . . , n, (2)
with f‚àó a fixed regression function, 1, . . . , n a sequence of random variables seen as noise, and
x1, . . . , xn random inputs. In the following, we consider kernel methods, based on choosing a
hypothesis space which is a separable reproducing kernel Hilbert space. The latter is a Hilbert space
H of functions, with inner product „Äà¬∑, ¬∑„ÄâH, such that there exists a function K : X √óX ‚Üí R with
the following two properties: 1) for all x ‚àà X , Kx(¬∑) = K(x, ¬∑) belongs to H, and 2) the so called
reproducing property holds: f(x) = „Äàf,Kx„ÄâH, for all f ‚àà H, x ‚àà X [25]. The function K, called
reproducing kernel, is easily shown to be symmetric and positive definite, that is the kernel matrix
(KN )i,j = K(xi, xj) is positive semidefinite for all x1, . . . , xN ‚àà X , N ‚àà N. A classical way to
derive an empirical solution to problem (1) is to consider a Tikhonov regularization approach, based
on the minimization of the penalized empirical functional,
min
f‚ààH
1
n
n‚àë
i=1
(f(xi)‚àí yi)2 + Œª‚Äñf‚Äñ2H, Œª > 0. (3)
The above approach is referred to as Kernel Regularized Least Squares (KRLS) or Kernel Ridge
Regression (KRR). It is easy to see that a solution fÃÇŒª to problem (3) exists, it is unique and the
representer theorem [1] shows that it can be written as
fÃÇŒª(x) =
n‚àë
i=1
Œ±ÃÇiK(xi, x) with Œ±ÃÇ = (Kn + ŒªnI)‚àí1y, (4)
where x1, . . . , xn are the training set points, y = (y1, . . . , yn) andKn is the empirical kernel matrix.
Note that this result implies that we can restrict the minimization in (3) to the space,
Hn = {f ‚àà H | f =
n‚àë
i=1
Œ±iK(xi, ¬∑), Œ±1, . . . , Œ±n ‚àà R}.
Storing the kernel matrix Kn, and solving the linear system in (4), can become computationally
unfeasible as n increases. In the following, we consider strategies to find more efficient solutions,
2
based on the idea of replacingHn with
Hm = {f | f =
m‚àë
i=1
Œ±iK(xÃÉi, ¬∑), Œ± ‚àà Rm},
where m ‚â§ n and {xÃÉ1, . . . , xÃÉm} is a subset of the input points in the training set. The solution fÃÇŒª,m
of the corresponding minimization problem can now be written as,
fÃÇŒª,m(x) =
m‚àë
i=1
Œ±ÃÉiK(xÃÉi, x) with Œ±ÃÉ = (K>nmKnm + ŒªnKmm)
‚Ä†K>nmy, (5)
where A‚Ä† denotes the Moore-Penrose pseudoinverse of a matrix A, and (Knm)ij = K(xi, xÃÉj),
(Kmm)kj = K(xÃÉk, xÃÉj) with i ‚àà {1, . . . , n} and j, k ‚àà {1, . . . ,m} [2]. The above approach is
related to NystroÃàm methods and different approximation strategies correspond to different ways to
select the inputs subset. While our framework applies to a broader class of strategies, see Sec-
tion C.1, in the following we primarily consider two techniques.
Plain NystroÃàm. The points {xÃÉ1, . . . , xÃÉm} are sampled uniformly at random without replacement
from the training set.
Approximate leverage scores (ALS) NystroÃàm. Recall that the leverage scores associated to the
training set points x1, . . . , xn are
(li(t))
n
i=1, li(t) = (Kn(Kn + tnI)
‚àí1)ii, i ‚àà {1, . . . , n} (6)
for any t > 0, where (Kn)ij = K(xi, xj). In practice, leverage scores are onerous to compute
and approximations (lÃÇi(t))ni=1 can be considered [16, 17, 24] . In particular, in the following we are
interested in suitable approximations defined as follows:
Definition 1 (T -approximate leverage scores). Let (li(t))ni=1 be the leverage scores associated to
the training set for a given t. Let Œ¥ > 0, t0 > 0 and T ‚â• 1. We say that (lÃÇi(t))ni=1 are T -approximate
leverage scores with confidence Œ¥, when with probability at least 1‚àí Œ¥,
1
T
li(t) ‚â§ lÃÇi(t) ‚â§ T li(t) ‚àÄi ‚àà {1, . . . , n}, t ‚â• t0.
Given T -approximate leverage scores for t > Œª0, {xÃÉ1, . . . , xÃÉm} are sampled from the training set in-
dependently with replacement, and with probability to be selected given by Pt(i) = lÃÇi(t)/
‚àë
j lÃÇj(t).
In the next section, we state and discuss our main result showing that the KRLS formulation based on
plain or approximate leverage scores NystroÃàm provides optimal empirical solutions to problem (1).
3 Theoretical analysis
In this section, we state and discuss our main results. We need several assumptions. The first basic
assumption is that problem (1) admits at least a solution.
Assumption 1. There exists an fH ‚àà H such that
E(fH) = min
f‚ààH
E(f).
Note that, while the minimizer might not be unique, our results apply to the case in which fH is the
unique minimizer with minimal norm. Also, note that the above condition is weaker than assuming
the regression function in (2) to belong to H. Finally, we note that the study of the paper can be
adapted to the case in which minimizers do not exist, but the analysis is considerably more involved
and left to a longer version of the paper.
The second assumption is a basic condition on the probability distribution.
Assumption 2. Let zx be the random variable zx = y ‚àí fH(x), with x ‚àà X , and y distributed
according to œÅ(y|x). Then, there exists M,œÉ > 0 such that E|zx|p ‚â§ 12p!M
p‚àí2œÉ2 for any p ‚â• 2,
almost everywhere on X .
The above assumption is needed to control random quantities and is related to a noise assumption in
the regression model (2). It is clearly weaker than the often considered bounded output assumption
3
[25], and trivially verified in classification.
The last two assumptions describe the capacity (roughly speaking the ‚Äúsize‚Äù) of the hypothesis space
induced by K with respect to œÅ and the regularity of fH with respect to K and œÅ. To discuss them,
we first need the following definition.
Definition 2 (Covariance operator and effective dimensions). We define the covariance operator as
C : H ‚Üí H, „Äàf, Cg„ÄâH =
‚à´
X
f(x)g(x)dœÅX(x) , ‚àÄ f, g ‚àà H.
Moreover, for Œª > 0, we define the random variable Nx(Œª) =
‚å©
Kx, (C + ŒªI)
‚àí1Kx
‚å™
H with x ‚àà X
distributed according to œÅX and let
N (Œª) = ENx(Œª), N‚àû(Œª) = sup
x‚ààX
Nx(Œª).
We add several comments. Note that C corresponds to the second moment operator, but we refer to
it as the covariance operator with an abuse of terminology. Moreover, note thatN (Œª) = Tr(C(C +
ŒªI)‚àí1) (see [26]). This latter quantity, called effective dimension or degrees of freedom, can be seen
as a measure of the capacity of the hypothesis space. The quantity N‚àû(Œª) can be seen to provide a
uniform bound on the leverage scores in Eq. (6). Clearly, N (Œª) ‚â§ N‚àû(Œª) for all Œª > 0.
Assumption 3. The kernel K is measurable, C is bounded. Moreover, for all Œª > 0 and a Q > 0,
N‚àû(Œª) <‚àû, (7)
N (Œª) ‚â§ QŒª‚àíŒ≥ , 0 < Œ≥ ‚â§ 1. (8)
Measurability of K and boundedness of C are minimal conditions to ensure that the covariance
operator is a well defined linear, continuous, self-adjoint, positive operator [25]. Condition (7) is
satisfied if the kernel is bounded supx‚ààX K(x, x) = Œ∫
2 < ‚àû, indeed in this case N‚àû(Œª) ‚â§ Œ∫2/Œª
for all Œª > 0. Conversely, it can be seen that condition (7) together with boundedness of C imply
that the kernel is bounded, indeed 1
Œ∫2 ‚â§ 2‚ÄñC‚ÄñN‚àû(‚ÄñC‚Äñ).
Boundedness of the kernel implies in particular that the operator C is trace class and allows to
use tools from spectral theory. Condition (8) quantifies the capacity assumption and is related to
covering/entropy number conditions (see [25] for further details). In particular, it is known that
condition (8) is ensured if the eigenvalues (œÉi)i of C satisfy a polynomial decaying condition œÉi ‚àº
i‚àí
1
Œ≥ . Note that, since the operator C is trace class, Condition (8) always holds for Œ≥ = 1. Here,
for space constraints and in the interest of clarity we restrict to such a polynomial condition, but the
analysis directly applies to other conditions including exponential decay or a finite rank conditions
[26]. Finally, we have the following regularity assumption.
Assumption 4. There exists s ‚â• 0, 1 ‚â§ R <‚àû, such that ‚ÄñC‚àísfH‚ÄñH < R.
The above condition is fairly standard, and can be equivalently formulated in terms of classical
concepts in approximation theory such as interpolation spaces [25]. Intuitively, it quantifies the
degree to which fH can be well approximated by functions in the RKHS H and allows to control
the bias/approximation error of a learning solution. For s = 0, it is always satisfied. For larger
s, we are assuming fH to belong to subspaces of H that are the images of the fractional compact
operators Cs. Such spaces contain functions which, expanded on a basis of eigenfunctions of C,
have larger coefficients in correspondence to large eigenvalues. Such an assumption is natural in
view of using techniques such as (4), which can be seen as a form of spectral filtering, that estimate
stable solutions by discarding the contribution of small eigenvalues [27]. In the next section, we
are going to quantify the quality of empirical solutions of Problem (1) obtained by schemes of the
form (5), in terms of the quantities in Assumptions 2, 3, 4.
1IfN‚àû(Œª) is finite, thenN‚àû(‚ÄñC‚Äñ) = supx‚ààX‚Äñ(C + ‚ÄñC‚ÄñI)
‚àí1Kx‚Äñ2 ‚â• 1/2‚ÄñC‚Äñ‚àí1supx‚ààX‚ÄñKx‚Äñ
2, there-
fore K(x, x) ‚â§ 2‚ÄñC‚ÄñN‚àû(‚ÄñC‚Äñ).
4
3.1 Main results
In this section, we state and discuss our main results, starting with optimal finite sample error bounds
for regularized least squares based on plain and approximate leverage score based NystroÃàm subsam-
pling.
Theorem 1. Under Assumptions 1, 2, 3, and 4, let Œ¥ > 0, v = min(s, 1/2), p = 1 + 1/(2v + Œ≥)
and assume
n ‚â• 1655Œ∫2 + 223Œ∫2 log 6Œ∫
2
Œ¥
+
(
38p
‚ÄñC‚Äñ
log
114Œ∫2p
‚ÄñC‚ÄñŒ¥
)p
.
Then, the following inequality holds with probability at least 1‚àí Œ¥,
E(fÃÇŒª,m)‚àí E(fH) ‚â§ q2 n‚àí
2v+1
2v+Œ≥+1 , with q = 6R
(
2‚ÄñC‚Äñ+ MŒ∫‚àö
‚ÄñC‚Äñ
+
‚àö
QœÉ2
‚ÄñC‚ÄñŒ≥
)
log
6
Œ¥
, (9)
with fÃÇŒª,m as in (5), Œª = ‚ÄñC‚Äñn‚àí
1
2v+Œ≥+1 and
1. for plain NystroÃàm
m ‚â• (67 ‚à® 5N‚àû(Œª)) log
12Œ∫2
ŒªŒ¥
;
2. for ALS NystroÃàm and T -approximate leverage scores with subsampling probabilities PŒª,
t0 ‚â• 19Œ∫
2
n log
12n
Œ¥ and
m ‚â• (334 ‚à® 78T 2N (Œª)) log 48n
Œ¥
.
We add several comments. First, the above results can be shown to be optimal in a minimax sense.
Indeed, minimax lower bounds proved in [26, 28] show that the learning rate in (9) is optimal un-
der the considered assumptions (see Thm. 2, 3 of [26], for a discussion on minimax lower bounds
see Sec. 2 of [26]). Second, the obtained bounds can be compared to those obtained for other reg-
ularized learning techniques. Techniques known to achieve optimal error rates include Tikhonov
regularization [26, 28, 29], iterative regularization by early stopping [30, 31], spectral cut-off regu-
larization (a.k.a. principal component regression or truncated SVD) [30, 31], as well as regularized
stochastic gradient methods [32]. All these techniques are essentially equivalent from a statistical
point of view and differ only in the required computations. For example, iterative methods allow
for a computation of solutions corresponding to different regularization levels which is more effi-
cient than Tikhonov or SVD based approaches. The key observation is that all these methods have
the same O(n2) memory requirement. In this view, our results show that randomized subsampling
methods can break such a memory barrier, and consequently achieve much better time complexity,
while preserving optimal learning guarantees. Finally, we can compare our results with previous
analysis of randomized kernel methods. As already mentioned, results close to those in Theorem 1
are given in [23, 24] in a fixed design setting. Our results extend and generalize the conclusions of
these papers to a general statistical learning setting. Relevant results are given in [8] for a different
approach, based on averaging KRLS solutions obtained splitting the data in m groups (divide and
conquer RLS). The analysis in [8] is only in expectation, but considers random design and shows
that the proposed method is indeed optimal provided the number of splits is chosen depending on
the effective dimension N (Œª). This is the only other work we are aware of establishing optimal
learning rates for randomized kernel approaches in a statistical learning setting. In comparison with
NystroÃàm computational regularization the main disadvantage of the divide and conquer approach is
computational and in the model selection phase where solutions corresponding to different regular-
ization parameters and number of splits usually need to be computed.
The proof of Theorem 1 is fairly technical and lengthy. It incorporates ideas from [26] and tech-
niques developed to study spectral filtering regularization [30, 33]. In the next section, we briefly
sketch some main ideas and discuss how they suggest an interesting perspective on regularization
techniques including subsampling.
3.2 Proof sketch and a computational regularization perspective
A key step in the proof of Theorem 1 is an error decomposition, and corresponding bound, for any
fixed Œª and m. Indeed, it is proved in Theorem 2 and Proposition 2 that, for Œ¥ > 0, with probability
5
m
200 400 600 800 1000
Œª
10
-6
10
-4
10
-2
10
0
RMSE
0.032 0.0325 0.033 0.0335 0.034 0.0345 0.035
m
50 100 150 200 250 300
Œª
10
-12
10
-10
10
-8
10
-6
10
-4
Classification Error
0.04 0.05 0.06 0.07 0.08 0.09 0.1
m
1000 2000 3000 4000 5000
Œª
10
-15
10
-14
10
-13
10
-12
RMSE
15 20 25
Figure 1: Validation errors associated to 20 √ó 20 grids of values for m (x axis) and Œª (y axis) on
pumadyn32nh (left), breast cancer (center) and cpuSmall (right).
at least 1‚àí Œ¥,‚à£‚à£‚à£E(fÃÇŒª,m)‚àí E(fH)‚à£‚à£‚à£1/2 . R(M‚àöN‚àû(Œª)
n
+
‚àö
œÉ2N (Œª)
n
)
log
6
Œ¥
+RC(m)1/2+v +RŒª1/2+v.
(10)
The first and last term in the right hand side of the above inequality can be seen as forms of sample
and approximation errors [25] and are studied in Lemma 4 and Theorem 2. The mid term can be
seen as a computational error and depends on the considered subsampling scheme. Indeed, it is
shown in Proposition 2 that C(m) can be taken as,
Cpl(m) = min
{
t > 0
‚à£‚à£‚à£‚à£ (67 ‚à® 5N‚àû(t)) log 12Œ∫2tŒ¥ ‚â§ m
}
,
for the plain NystroÃàm approach, and
CALS(m) = min
{
19Œ∫2
n
log
12n
Œ¥
‚â§ t ‚â§ ‚ÄñC‚Äñ
‚à£‚à£‚à£‚à£ 78T 2N (t) log 48nŒ¥ ‚â§ m
}
,
for the approximate leverage scores approach. The bounds in Theorem 1 follow by: 1) minimizing
in Œª the sum of the first and third term 2) choosing m so that the computational error is of the
same order of the other terms. Computational resources and regularization are then tailored to the
generalization properties of the data at hand. We add a few comments. First, note that the error bound
in (10) holds for a large class of subsampling schemes, as discussed in Section C.1 in the appendix.
Then specific error bounds can be derived developing computational error estimates. Second, the
error bounds in Theorem 2 and Proposition 2, and hence in Theorem 1, easily generalize to a larger
class of regularization schemes beyond Tikhonov approaches, namely spectral filtering [30]. For
space constraints, these extensions are deferred to a longer version of the paper. Third, we note that,
in practice, optimal data driven parameter choices, e.g. based on hold-out estimates [31], can be
used to adaptively achieve optimal learning bounds.
Finally, we observe that a different perspective is derived starting from inequality (10), and noting
that the role played by m and Œª can also be exchanged. Letting m play the role of a regularization
parameter, Œª can be set as a function of m and m tuned adaptively. For example, in the case of a
plain NystroÃàm approach, if we set
Œª =
logm
m
, and m = 3n
1
2v+Œ≥+1 log n,
then the obtained learning solution achieves the error bound in Eq. (9). As above, the subsampling
level can also be chosen by cross-validation. Interestingly, in this case by tuning m we naturally
control computational resources and regularization. An advantage of this latter parameterization
is that, as described in the following, the solution corresponding to different subsampling levels is
easy to update using Cholesky rank-one update formulas [34]. As discussed in the next section,
in practice, a joint tuning over m and Œª can be done starting from small m and appears to be
advantageous both for error and computational performances.
4 Incremental updates and experimental analysis
In this section, we first describe an incremental strategy to efficiently explore different subsampling
levels and then perform extensive empirical tests aimed in particular at: 1) investigating the sta-
tistical and computational benefits of considering varying subsampling levels, and 2) compare the
6
Input: Dataset (xi, yi)ni=1, Subsampling (xÃÉj)mj=1,
Regularization Parameter Œª.
Output: NystroÃàm KRLS estimators {Œ±ÃÉ1, . . . , Œ±ÃÉm}.
Compute Œ≥1; R1 ‚Üê
‚àö
Œ≥1;
for t ‚àà {2, . . . ,m} do
Compute At, ut, vt;
Rt ‚Üê
(
Rt‚àí1 0
0 0
)
;
Rt ‚Üê cholup(Rt, ut,‚Ä≤+‚Ä≤);
Rt ‚Üê cholup(Rt, vt,‚Ä≤‚àí‚Ä≤);
Œ±ÃÉt ‚Üê R‚àí1t (R‚àí>t (Aty));
end for
Algorithm 1: Incremental NystroÃàm KRLS.
m
1 201 401 600 800 1000
T
im
e
 (
s
)
0
20
40
60
80
100
120
Incremental Nystr√∂m
Batch Nystr√∂m
Figure 2: Model selection time on the
cpuSmall dataset. m ‚àà [1, 1000]
and T = 50, 10 repetitions.
performance of the algorithm with respect to state of the art solutions on several large scale bench-
mark datasets. Throughout this section, we only consider a plain NystroÃàm approach, deferring to
future work the analysis of leverage scores based sampling techniques. Interestingly, we will see
that such a basic approach can often provide state of the art performances.
4.1 Efficient incremental updates
Algorithm 1 efficiently compute solutions corresponding to different subsampling levels, by exploit-
ing rank-one Cholesky updates [34]. The proposed procedure allows to efficiently compute a whole
regularization path of solutions, and hence perform fast model selection2 (see Sect. A). In Algo-
rithm 1, the function cholup is the Cholesky rank-one update formula available in many linear
algebra libraries. The total cost of the algorithm is O(nm2 + m3) time to compute Œ±ÃÉ2, . . . , Œ±ÃÉm,
while a naive non-incremental algorithm would require O(nm2T +m3T ) with T is the number of
analyzed subsampling levels. The following are some quantities needed by the algorithm: A1 = a1
and At = (At‚àí1 at) ‚àà Rn√ót, for any 2 ‚â§ t ‚â§ m. Moreover, for any 1 ‚â§ t ‚â§ m, gt =
‚àö
1 + Œ≥t and
ut = (ct/(1 + gt), gt), at = (K(xÃÉt, x1), . . . ,K(xÃÉt, xn)), ct = A
>
t‚àí1at + Œªnbt,
vt = (ct/(1 + gt), ‚àí1), bt = (K(xÃÉt, xÃÉ1), . . . ,K(xÃÉt, xÃÉt‚àí1)), Œ≥t = a>t at + ŒªnK(xÃÉt, xÃÉt).
4.2 Experimental analysis
We empirically study the properties of Algorithm 1, considering a Gaussian kernel of width œÉ. The
selected datasets are already divided in a training and a test part3. We randomly split the training
part in a training set and a validation set (80% and 20% of the n training points, respectively) for
parameter tuning via cross-validation. The m subsampled points for NystroÃàm approximation are se-
lected uniformly at random from the training set. We report the performance of the selected model
on the fixed test set, repeating the process for several trials.
Interplay between Œª and m. We begin with a set of results showing that incrementally explor-
ing different subsampling levels can yield very good performance while substantially reducing the
computational requirements. We consider the pumadyn32nh (n = 8192, d = 32), the breast
cancer (n = 569, d = 30), and the cpuSmall (n = 8192, d = 12) datasets4. In Figure 1, we
report the validation errors associated to a 20 √ó 20 grid of values for Œª and m. The Œª values are
logarithmically spaced, while the m values are linearly spaced. The ranges and kernel bandwidths,
chosen according to preliminary tests on the data, are œÉ = 2.66, Œª ‚àà
[
10‚àí7, 1
]
, m ‚àà [10, 1000] for
pumadyn32nh, œÉ = 0.9, Œª ‚àà
[
10‚àí12, 10‚àí3
]
, m ‚àà [5, 300] for breast cancer, and œÉ = 0.1,
Œª ‚àà
[
10‚àí15, 10‚àí12
]
, m ‚àà [100, 5000] for cpuSmall. The main observation that can be derived
from this first series of tests is that a small m is sufficient to obtain the same results achieved with
the largest m. For example, for pumadyn32nh it is sufficient to choose m = 62 and Œª = 10‚àí7
to obtain an average test RMSE of 0.33 over 10 trials, which is the same as the one obtained using
m = 1000 and Œª = 10‚àí3, with a 3-fold speedup of the joint training and validation phase. Also,
it is interesting to observe that for given values of Œª, large values of m can decrease the perfor-
mance. This observation is consistent with the results in Section 3.1, showing that m can play the
2The code for Algorithm 1 is available at lcsl.github.io/NystromCoRe.
3In the following we denote by n the total number of points and by d the number of dimensions.
4www.cs.toronto.edu/Àúdelve and archive.ics.uci.edu/ml/datasets
7
Table 1: Test RMSE comparison for exact and approximated kernel methods. The results for KRLS,
Batch NystroÃàm, RF and Fastfood are the ones reported in [6]. ntr is the size of the training set.
Dataset ntr d Incremental KRLS Batch RF Fastfood Fastfood KRLS Fastfood
NystroÃàm RBF RBF NystroÃàm RBF RBF RBF FFT Matern Matern
Insurance Company 5822 85 0.23180¬± 4√ó 10‚àí5 0.231 0.232 0.266 0.264 0.266 0.234 0.235
CPU 6554 21 2.8466¬± 0.0497 7.271 6.758 7.103 7.366 4.544 4.345 4.211
CT slices (axial) 42800 384 7.1106¬± 0.0772 NA 60.683 49.491 43.858 58.425 NA 14.868
Year Prediction MSD 463715 90 0.10470¬± 5√ó 10‚àí5 NA 0.113 0.123 0.115 0.106 NA 0.116
Forest 522910 54 0.9638¬± 0.0186 NA 0.837 0.840 0.840 0.838 NA 0.976
role of a regularization parameter. Similar results are obtained for breast cancer, where for
Œª = 4.28 √ó 10‚àí6 and m = 300 we obtain a 1.24% average classification error on the test set over
20 trials, while for Œª = 10‚àí12 and m = 67 we obtain 1.86%. For cpuSmall, with m = 5000 and
Œª = 10‚àí12 the average test RMSE over 5 trials is 12.2, while for m = 2679 and Œª = 10‚àí15 it is
only slightly higher, 13.3, but computing its associated solution requires less than half of the time
and approximately half of the memory.
Regularization path computation. If the subsampling levelm is used as a regularization parameter,
the computation of a regularization path corresponding to different subsampling levels becomes cru-
cial during the model selection phase. A naive approach, that consists in recomputing the solutions
of Eq. 5 for each subsampling level, would require O(m2nT +m3LT ) computational time, where
T is the number of solutions with different subsampling levels to be evaluated and L is the number
of Tikhonov regularization parameters. On the other hand, by using the incremental NystroÃàm al-
gorithm the model selection time complexity is O(m2n + m3L) for the whole regularization path.
We experimentally verify this speedup on cpuSmall with 10 repetitions, setting m ‚àà [1, 5000]
and T = 50. The model selection times, measured on a server with 12 √ó 2.10GHz Intelr Xeonr
E5-2620 v2 CPUs and 132 GB of RAM, are reported in Figure 2. The result clearly confirms the
beneficial effects of incremental NystroÃàm model selection on the computational time.
Predictive performance comparison. Finally, we consider the performance of the algorithm on
several large scale benchmark datasets considered in [6], see Table 1. œÉ has been chosen on the
basis of preliminary data analysis. m and Œª have been chosen by cross-validation, starting from
small subsampling values up to mmax = 2048, and considering Œª ‚àà
[
10‚àí12, 1
]
. After model se-
lection, we retrain the best model on the entire training set and compute the RMSE on the test set.
We consider 10 trials, reporting the performance mean and standard deviation. The results in Table
1 compare NystroÃàm computational regularization with the following methods (as in [6]):
‚Ä¢ Kernel Regularized Least Squares (KRLS): Not compatible with large datasets.
‚Ä¢ Random Fourier features (RF): As in [4], with a number of random features D = 2048.
‚Ä¢ Fastfood RBF, FFT and Matern kernel: As in [6], with D = 2048 random features.
‚Ä¢ Batch NystroÃàm: NystroÃàm method [3] with uniform sampling and m = 2048.
The above results show that the proposed incremental NystroÃàm approach behaves really well, match-
ing state of the art predictive performances.
Acknowledgments
The work described in this paper is supported by the Center for Brains, Minds and Machines
(CBMM), funded by NSF STC award CCF-1231216; and by FIRB project RBFR12M3AC, funded
by the Italian Ministry of Education, University and Research.
References
[1] Bernhard SchoÃàlkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines, Regular-
ization, Optimization, and Beyond (Adaptive Computation and Machine Learning). MIT Press, 2002.
[2] Alex J. Smola and Bernhard SchoÃàlkopf. Sparse Greedy Matrix Approximation for Machine Learning. In
ICML, pages 911‚Äì918. Morgan Kaufmann, 2000.
[3] C. Williams and M. Seeger. Using the NystroÃàm Method to Speed Up Kernel Machines. In NIPS, pages
682‚Äì688. MIT Press, 2000.
[4] Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines. In NIPS, pages
1177‚Äì1184. Curran Associates, Inc., 2007.
8
[5] J. Yang, V. Sindhwani, H. Avron, and M. W. Mahoney. Quasi-Monte Carlo Feature Maps for Shift-
Invariant Kernels. In ICML, volume 32 of JMLR Proceedings, pages 485‚Äì493. JMLR.org, 2014.
[6] Quoc V. Le, TamaÃÅs SarloÃÅs, and Alexander J. Smola. Fastfood - Computing Hilbert Space Expansions in
loglinear time. In ICML, volume 28 of JMLR Proceedings, pages 244‚Äì252. JMLR.org, 2013.
[7] Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Memory Efficient Kernel Approximation. In ICML, vol-
ume 32 of JMLR Proceedings, pages 701‚Äì709. JMLR.org, 2014.
[8] Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Divide and Conquer Kernel Ridge Regression.
In COLT, volume 30 of JMLR Proceedings, pages 592‚Äì617. JMLR.org, 2013.
[9] S. Kumar, M. Mohri, and A. Talwalkar. Ensemble Nystrom Method. In NIPS, pages 1060‚Äì1068, 2009.
[10] Mu Li, James T. Kwok, and Bao-Liang Lu. Making Large-Scale NystroÃàm Approximation Possible. In
ICML, pages 631‚Äì638. Omnipress, 2010.
[11] Kai Zhang, Ivor W. Tsang, and James T. Kwok. Improved NystroÃàm Low-rank Approximation and Error
Analysis. ICML, pages 1232‚Äì1239. ACM, 2008.
[12] Bo Dai, Bo Xie 0002, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, and Le Song. Scalable
Kernel Methods via Doubly Stochastic Gradients. In NIPS, pages 3041‚Äì3049, 2014.
[13] Petros Drineas and Michael W. Mahoney. On the NystroÃàm Method for Approximating a Gram Matrix for
Improved Kernel-Based Learning. JMLR, 6:2153‚Äì2175, December 2005.
[14] A. Gittens and M. W. Mahoney. Revisiting the Nystrom method for improved large-scale machine learn-
ing. 28:567‚Äì575, 2013.
[15] Shusen Wang and Zhihua Zhang. Improving CUR Matrix Decomposition and the NystroÃàm Approxima-
tion via Adaptive Sampling. JMLR, 14(1):2729‚Äì2769, 2013.
[16] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approximation
of matrix coherence and statistical leverage. JMLR, 13:3475‚Äì3506, 2012.
[17] Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford.
Uniform Sampling for Matrix Approximation. In ITCS, pages 181‚Äì190. ACM, 2015.
[18] Shusen Wang and Zhihua Zhang. Efficient Algorithms and Error Analysis for the Modified Nystrom
Method. In AISTATS, volume 33 of JMLR Proceedings, pages 996‚Äì1004. JMLR.org, 2014.
[19] S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the NystroÃàm method. JMLR, 13(1):981‚Äì
1006, 2012.
[20] Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the Impact of Kernel Approximation on
Learning Accuracy. In AISTATS, volume 9 of JMLR Proceedings, pages 113‚Äì120. JMLR.org, 2010.
[21] R Jin, T. Yang, M. Mahdavi, Y. Li, and Z. Zhou. Improved Bounds for the NystroÃàm Method With
Application to Kernel Classification. Information Theory, IEEE Transactions on, 59(10), Oct 2013.
[22] Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. NystroÃàm Method vs Ran-
dom Fourier Features: A Theoretical and Empirical Comparison. In NIPS, pages 485‚Äì493, 2012.
[23] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT, volume 30, 2013.
[24] A. Alaoui and M. W. Mahoney. Fast randomized kernel methods with statistical guarantees. arXiv, 2014.
[25] I. Steinwart and A. Christmann. Support Vector Machines. Springer New York, 2008.
[26] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foun-
dations of Computational Mathematics, 7(3):331‚Äì368, 2007.
[27] L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri. Spectral Algo-
rithms for Supervised Learning. Neural Computation, 20(7):1873‚Äì1897, 2008.
[28] I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In COLT,
2009.
[29] S. Mendelson and J. Neeman. Regularization in kernel learning. The Annals of Statistics, 38(1), 2010.
[30] F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory. Journal of
complexity, 23(1):52‚Äì72, 2007.
[31] A. Caponnetto and Yuan Yao. Adaptive rates for regularization operators in learning theory. Analysis and
Applications, 08, 2010.
[32] Y. Ying and M. Pontil. Online gradient descent learning algorithms. Foundations of Computational
Mathematics, 8(5):561‚Äì596, 2008.
[33] Alessandro Rudi, Guillermo D. Canas, and Lorenzo Rosasco. On the Sample Complexity of Subspace
Learning. In NIPS, pages 2067‚Äì2075, 2013.
[34] Gene H. Golub and Charles F. Van Loan. Matrix computations, volume 3. JHU Press, 2012.
9
