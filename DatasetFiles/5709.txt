


Paper ID = 5709
Title = Monotone k-Submodular Function Maximization
with Size Constraints
Naoto Ohsaka
The University of Tokyo
ohsaka@is.s.u-tokyo.ac.jp
Yuichi Yoshida
National Institute of Informatics, and
Preferred Infrastructure, Inc.
yyoshida@nii.ac.jp
Abstract
A k-submodular function is a generalization of a submodular function, where the
input consists of k disjoint subsets, instead of a single subset, of the domain.
Many machine learning problems, including influence maximization with k kinds
of topics and sensor placement with k kinds of sensors, can be naturally modeled
as the problem of maximizing monotone k-submodular functions. In this paper,
we give constant-factor approximation algorithms for maximizing monotone k-
submodular functions subject to several size constraints. The running time of our
algorithms are almost linear in the domain size. We experimentally demonstrate
that our algorithms outperform baseline algorithms in terms of the solution quality.
1 Introduction
The task of selecting a set of items subject to constraints on the size or the cost of the set is versatile
in machine learning problems. The objective can be often modeled as maximizing a function with
the diminishing return property, where for a finite set V , a function f : 2V â†’ R satisfies the
diminishing return property if
f(S âˆª {e})âˆ’ f(S) â‰¥ f(T âˆª {e})âˆ’ f(T )
for any S âŠ† T and e âˆˆ V \ T . For example, sensor placement [13, 14], influence maximization
in social networks [11], document summarization [15], and feature selection [12] involve objectives
satisfying the diminishing return property. It is well known that the diminishing return property is
equivalent to submodularity, where a function f : 2V â†’ R is submodular if
f(S) + f(T ) â‰¥ f(S âˆ© T ) + f(S âˆª T )
holds for any S, T âŠ† V . When the objective function is submodular and hence satisfies the dimin-
ishing return property, we can find in polynomial time a solution with a provable guarantee on its
solution quality even with various constraints [2, 3, 18, 21].
In many practical applications, however, we want to select several disjoint sets of items instead of a
single set. To see this, let us describe two examples:
Influence maximization: Viral marketing is a cost-effective marketing strategy that promotes prod-
ucts by giving free (or discounted) items to a selected group of highly influential people in the hope
that, through the word-of-mouth effects, a large number of product adoptions will occur [4, 19].
Suppose that we have k kinds of items, each having a different topic and thus a different word-of-
mouth effect. Then, we want to distribute these items to B people selected from a group V of n
people so as to maximize the (expected) number of product adoptions. It is natural to impose a con-
straint that each person can receive at most one item since giving many free items to one particular
person would be unfair.
Sensor placement: There are k kinds of sensors for different measures such as temperature,
humidity, and illuminance. Suppose that we have Bi many sensors of the i-th kind for each
1
i âˆˆ {1, 2, . . . , k}, and there is a set V of n locations, each of which can be instrumented with
exactly one sensor. Then, we want to allocate those sensors so as to maximize the information gain.
When k = 1, these problems can be modeled as maximizing monotone submodular functions [11,
14] and admit polynomial-time (1 âˆ’ 1/e)-approximation [18]. Unfortunately, however, the case of
general k cannot be modeled as maximizing submodular functions, and we cannot apply the methods
in the literature on maximizing submodular functions [2, 3, 18, 21]. We note that the problem of
selecting k disjoint sets can be sometimes modeled as maximizing monotone submodular functions
over the extended domain k Ã— V subject to a partition matroid. Although (1âˆ’ 1/e)-approximation
algorithms are known [3, 5], the running time is around O(k8n8) and is prohibitively slow.
Our contributions: To address the problem of selecting k disjoint sets, we use the fact that the
objectives can be often modeled as k-submodular functions. Let (k + 1)V := {(X1, . . . , Xk) |
Xi âŠ† V âˆ€i âˆˆ {1, 2, . . . , k}, Xi âˆ©Xj = âˆ… âˆ€i 6= j} be the family of k disjoint sets. Then, a function
f : (k + 1)V â†’ R is called k-submodular [9] if, for any x = (X1, . . . , Xk) and y = (Y1, . . . , Yk)
in (k + 1)V , we have
f(x) + f(y) â‰¥ f(x t y) + f(x u y)
where
x u y := (X1 âˆ© Y1, . . . , Xk âˆ© Yk),
x t y :=
(
X1 âˆª Y1 \
(â‹ƒ
i6=1
Xi âˆª Yi
)
, . . . , Xk âˆª Yk \
(â‹ƒ
i 6=k
Xi âˆª Yi
))
.
Roughly speaking, k-submodularity captures the property that, if we choose exactly one set Xe âˆˆ
{X1, . . . , Xk} that an element e can belong to for each e âˆˆ V , then the resulting function is sub-
modular (see Section 2 for details). When k = 1, k-submodularity coincides with submodularity.
In this paper, we give approximation algorithms for maximizing non-negative monotone k-
submodular functions with several constraints on the sizes of the k sets. Here, we say that f is
monotone if f(x) â‰¤ f(y) for any x = (X1, . . . , Xk) and y = (Y1, . . . , Yk) with Xi âŠ† Yi for each
i âˆˆ {1, . . . , k}. Let n = |V | be the size of the domain. For the total size constraint, under which
the total size of the k sets is bounded by B âˆˆ Z+, we show that a simple greedy algorithm outputs
1/2-approximation in O(knB) time. The approximation ratio of 1/2 is asymptotically tight since
the lower bound of k+12k +  for any  > 0 is known even when B = n [10]. Combining the random
sampling technique [17], we also give a randomized algorithm that outputs 1/2-approximation with
probability at least 1 âˆ’ Î´ in O(kn logB log(B/Î´)) time. Hence, even when B is as large as n, the
running time is almost linear in n. For the individual size constraint, under which the size of the i-th
set is bounded by Bi âˆˆ Z+ for each i âˆˆ {1, . . . , k}, we give a 1/3-approximation algorithm with
running time O(knB), where B =
âˆ‘k
i=1Bi. We then give a randomized algorithm that outputs
1/3-approximation with probability at least 1âˆ’ Î´ in O(k2n log(B/k) log(B/Î´)) time.
To show the practicality of our algorithms, we apply them to the influence maximization problem
and the sensor placement problem, and we demonstrate that they outperform previous methods based
on submodular function maximization and several baseline methods in terms of the solution quality.
Related work: When k = 2, k-submodularity is called bisubmodularity, and [20] applied bisub-
modular functions to machine learning problems. However, their algorithms do not have any ap-
proximation guarantee. Huber and Kolmogorov introduced k-submodularity as a generalization of
submodularity and bisubmodularity [9], and minimizing k-submodular functions was successfully
used in a computer vision application [8]. Iwata et al. [10] gave a 1/2-approximation algorithm
and a k2kâˆ’1 -approximation algorithm for maximizing non-monotone and monotone k-submodular
functions, respectively, when there is no constraint.
Organization: The rest of this paper is organized as follows. In Section 2, we review properties
of k-submodular functions. Sections 3 and 4 are devoted to show 1/2-approximation algorithms
for the total size constraint, and 1/3-approximation algorithms for the individual size constraint,
respectively. We show our experimental results in Section 5. We conclude our paper in Section 6.
2
Algorithm 1 k-Greedy-TS
Input: a monotone k-submodular function f : (k + 1)V â†’ R+ and an integer B âˆˆ Z+.
Output: a vector s with |supp(s)| = B.
1: sâ† 0.
2: for j = 1 to B do
3: (e, i)â† arg maxeâˆˆV \supp(s),iâˆˆ[k] âˆ†e,if(s).
4: s(e)â† i.
5: return s.
2 Preliminaries
For an integer k âˆˆ N, [k] denotes the set {1, 2, . . . , k}. We define a partial order  on (k + 1)V so
that, for x = (X1, . . . , Xk) and y = (Y1, . . . , Yk) in (k + 1)V , x  y if Xi âŠ† Yi for every i with
i âˆˆ [k]. We also define
âˆ†e,if(x) = f(X1, . . . , Xiâˆ’1, Xi âˆª {e}, Xi+1, . . . , Xk)âˆ’ f(X1, . . . , , Xk)
for x âˆˆ (k + 1)V , e /âˆˆ
â‹ƒ
`âˆˆ[k]X`, and i âˆˆ [k], which is the marginal gain when adding e to the
i-th set of x. Then, it is easy to see the monotonicity of f is equivalent to âˆ†e,if(x) â‰¥ 0 for any
x = (X1, . . . , Xk) and e 6âˆˆ
â‹ƒ
`âˆˆ[k]X` and i âˆˆ [k]. Also it is not hard to show (see [22] for details)
that the k-submodularity of f implies the orthant submodularity, i.e.,
âˆ†e,if(x) â‰¥ âˆ†e,if(y)
for any x,y âˆˆ (k+ 1)V with x  y, e /âˆˆ
â‹ƒ
`âˆˆ[k] Y`, and i âˆˆ [k], and the pairwise monotonicity, i.e.,
âˆ†e,if(x) + âˆ†e,jf(x) â‰¥ 0
for any x âˆˆ (k + 1)V , e /âˆˆ
â‹ƒ
`âˆˆ[k]X`, and i, j âˆˆ [k] with i 6= j. Actually, the converse holds:
Theorem 2.1 (Ward and ZÌŒivnyÌ [22]). A function f : (k + 1)V â†’ R is k-submodular if and only if
f is orthant submodular and pairwise monotone.
It is often convenient to identify (k + 1)V with {0, 1 . . . , k}V to analyze k-submodular functions,
Namely, we associate (X1, . . . , Xk) âˆˆ (k + 1)V with x âˆˆ {0, 1, . . . , k}V by Xi = {e âˆˆ V |
x(e) = i} for i âˆˆ [k]. Hence we sometimes abuse notation, and simply write x = (X1, . . . , Xk)
by regarding a vector x as disjoint k sets of V . We define the support of x âˆˆ {0, 1, . . . , k}V as
supp(x) = {e âˆˆ V | x(e) 6= 0}. Analogously, for x âˆˆ {0, 1, . . . , k}V and i âˆˆ [k], we define
suppi(x) = {e âˆˆ V | x(e) = i}. Let 0 be the zero vector in {0, 1, . . . , k}V .
3 Maximizing k-submodular Functions with the Total Size Constraint
In this section, we give a 1/2-approximation algorithm to the problem of maximizing monotone
k-submodular functions subject to the total size constraint. Namely, we consider
max f(x) subject to |supp(x)| â‰¤ B and x âˆˆ (k + 1)V ,
where f : (k + 1)V â†’ R+ is monotone k-submodular and B âˆˆ Z+ is a non-negative integer.
3.1 A greedy algorithm
The first algorithm we propose is a simple greedy algorithm (Algorithm 1). We show the following:
Theorem 3.1. Algorithm 1 outputs a 1/2-approximate solution by evaluating f O(knB) times,
where n = |V |.
The number of evaluations of f is clearly O(knB). Hence in what follows, we focus on analyzing
the approximation ratio of Algorithm 1. Our analysis is based on the framework of [10].
Consider the j-th iteration of the for loop from Line 2. Let (e(j), i(j)) âˆˆ V Ã— [k] be the pair greedily
chosen in this iteration, and let s(j) be the solution after this iteration. We define s(0) = 0. Let o be
3
Algorithm 2 k-Stochastic-Greedy-TS
Input: a monotone k-submodular function f : (k + 1)V â†’ R+, an integer B âˆˆ Z+, and a failure
probability Î´ > 0.
Output: a vector s with |supp(s)| = B.
1: sâ† 0.
2: for j = 1 to B do
3: Râ† a random subset of size min{ nâˆ’j+1Bâˆ’j+1 log
B
Î´ , n} uniformly sampled from V \ supp(s).
4: (e, i)â† arg maxeâˆˆR,iâˆˆ[k] âˆ†e,if(s).
5: s(e)â† i.
6: return s.
the optimal solution. We iteratively define o(0) = o,o(1), . . . ,o(B) as follows. For each j âˆˆ [B], let
S(j) = supp(o(jâˆ’1)) \ supp(s(jâˆ’1)). Then, we set o(j) = e(j) if e(j) âˆˆ S(j), and set o(j) to be an
arbitrary element in S(j) otherwise. Then, we define o(jâˆ’1/2) as the resulting vector obtained from
o(jâˆ’1) by assigning 0 to the o(j)-th element, and then define o(j) as the resulting vector obtained
from o(jâˆ’1/2) by assigning i(j) to the e(j)-th element. Note that supp(o(j)) = B holds for every
j âˆˆ {0, 1, . . . , B} and o(B) = s(B) = s. Moreover, we have s(jâˆ’1)  o(jâˆ’1/2) for every j âˆˆ [B].
Proof of Theorem 3.1. We first show that, for each j âˆˆ [B],
f(s(j))âˆ’ f(s(jâˆ’1)) â‰¥ f(o(jâˆ’1))âˆ’ f(o(j)). (1)
For each j âˆˆ [B], let y(j) = âˆ†e(j),i(j)f(s(jâˆ’1)), a(jâˆ’1/2) = âˆ†o(j),o(jâˆ’1)(o(j))f(o(jâˆ’1/2)), and
a(j) = âˆ†e(j),i(j)f(o
(jâˆ’1/2)). Then, note that f(s(j))âˆ’f(s(jâˆ’1)) = y(j), and f(o(jâˆ’1))âˆ’f(o(j)) =
a(jâˆ’1/2)âˆ’a(j). From the monotonicity of f , it suffices to show that y(j) â‰¥ a(jâˆ’1/2). Since e(j) and
i(j) are chosen greedily, we have y(j) â‰¥ âˆ†o(j),o(jâˆ’1)(o(j))f(s(jâˆ’1)). Since s(jâˆ’1)  o(jâˆ’1/2), we
have âˆ†o(j),o(jâˆ’1)(o(j))f(s(jâˆ’1)) â‰¥ a(jâˆ’1/2) from the orthant submodularity. Combining these two
inequalities, we establish (1).
Then, we have
f(o)âˆ’ f(s) =
Bâˆ‘
j=1
(f(o(jâˆ’1))âˆ’ f(o(j))) â‰¤
Bâˆ‘
j=1
(f(s(j))âˆ’ f(s(jâˆ’1))) = f(s)âˆ’ f(0) â‰¤ f(s),
which implies f(s) â‰¥ f(o)/2.
3.2 An almost linear-time algorithm by random sampling
In this section, we improve the number of evaluations of f from O(knB) to O(kn logB log BÎ´ ),
where Î´ > 0 is a failure probability.
Our algorithm is shown in Algorithm 2. The main difference from Algorithm 1 is that we sample a
sufficiently large subset R of V , and then greedily assign a value only looking at elements in R.
We reuse notations e(j), i(j), S(j) and s(j) from Section 3.1, and let R(j) be R in the j-th iteration.
We iteratively define o(0) = o,o(1), . . . ,o(B) as follows. IfR(j)âˆ©S(j) is empty, then we regard that
the algorithm failed. SupposeR(j)âˆ©S(j) is non-empty. Then, we set o(j) = e(j) if e(j) âˆˆ R(j)âˆ©S(j),
and set o(j) to be an arbitrary element in R(j) âˆ©S(j) otherwise. Finally, we define o(jâˆ’1/2) and o(j)
as in Section 3.1 using o(jâˆ’1), o(j), and e(j).
If the algorithm does not fail and o(1), . . . ,o(B) are well defined, or in other words, if R(j) âˆ©S(j) is
not empty for every j âˆˆ [B], then the rest of the analysis is completely the same as in Section 3.1,
and we achieve an approximation ratio of 1/2. Hence, it suffices to show that o(1), . . . ,o(B) are
well defined with a high probability.
Lemma 3.2. With probability at least 1âˆ’ Î´, we have R(j) âˆ© S(j) 6= âˆ… for every j âˆˆ [B].
4
Algorithm 3 k-Greedy-IS
Input: a monotone k-submodular function f : (k + 1)V â†’ R+ and integers B1, . . . , Bk âˆˆ Z+.
Output: a vector s with |suppi(s)| = Bi for each i âˆˆ [k].
1: sâ† 0 and B â†
âˆ‘
iâˆˆ[k]Bi.
2: for j = 1 to B do
3: I â† {i âˆˆ [k] | suppi(s) < Bi}.
4: (e, i)â† arg maxeâˆˆV \supp(s),iâˆˆI âˆ†e,if(s).
5: s(e)â† i.
6: return s.
Proof. Fix j âˆˆ [B]. If |R(j)| = n, then we clealy have Pr[R(j)âˆ©S(j) = âˆ…] = 0. Otherwise we have
Pr[R(j) âˆ© S(j) = âˆ…] =
(
1âˆ’ |S
(j)|
|V \ supp(s(jâˆ’1))|
)|R(j)|
â‰¤ eâˆ’
Bâˆ’j+1
nâˆ’j+1
nâˆ’j+1
Bâˆ’j+1 log
B
Î´ =
Î´
B
.
By the union bound over j âˆˆ [B], the lemma follows.
Theorem 3.3. Algorithm 2 outputs a 1/2-approximate solution with probability at least 1 âˆ’ Î´ by
evaluating f at most O(k(nâˆ’B) logB log BÎ´ ) times.
Proof. By Lemma 3.2 and the analysis in Section 3.1, Algorithm 2 outputs a 1/2-approximate
solution with probability at least 1âˆ’ Î´.
The number of evaluations of f is at most
k
âˆ‘
jâˆˆ[B]
nâˆ’ j + 1
B âˆ’ j + 1
log
B
Î´
= k
âˆ‘
jâˆˆ[B]
nâˆ’B + j
j
log
B
Î´
= O
(
kn logB log
B
Î´
)
.
4 Maximizing k-submodular Functions with the Individual Size Constraint
In this section, we consider the problem of maximizing monotone k-submodular functions subject
to the individual size constraint. Namely, we consider
max f(x) subject to |suppi(x)| â‰¤ Bi âˆ€i âˆˆ [k] and x âˆˆ (k + 1)V ,
where f : (k + 1)V â†’ R+ is monotone k-submodular, and B1, . . . , Bk âˆˆ Z+ are non-negative
integers.
4.1 A greedy algorithm
We first consider a simple greedy algorithm described in Algorithm 3. We show the following:
Theorem 4.1. Algorithm 3 outputs a 1/3-approximate solution by evaluating f at most O(knB)
times.
It is clear that the number of evaluations of f is O(knB). The analysis of the approximation ratio is
given in Appendix A.
4.2 An almost linear-time algorithm by random sampling
We next improve the number of evaluations of f from O(knB) to O
(
k2n log Bk log
B
Î´
)
. Our algo-
rithm is given in Algorithm 4. In Appendix A, we show the following.
Theorem 4.2. Algorithm 4 outputs a 1/3-approximate solution with probability at least 1 âˆ’ Î´ by
evaluating f at most O
(
k2n log Bk log
B
Î´
)
times.
5
Algorithm 4 k-Stochastic-Greedy-IS
Input: a monotone k-submodular function f : (k + 1)V â†’ R+, integers B1, . . . , Bk âˆˆ Z+, and a
failure probability Î´ > 0.
Output: a vector s with |suppi(s)| = Bi for each i âˆˆ [k].
1: sâ† 0 and B â†
âˆ‘
iâˆˆ[k]Bi.
2: for j = 1 to B do
3: I â† {i âˆˆ [k] | suppi(s) < Bi} and Râ† âˆ….
4: loop
5: Add a random element in V \ (supp(s) âˆªR) to R.
6: (e, i)â† arg maxeâˆˆR,iâˆˆI âˆ†e,if(s).
7: if |R| â‰¥ min{ nâˆ’|suppi(s)|Biâˆ’|suppi(s)| log
B
Î´ , n} then
8: s(e)â† i.
9: break the loop.
10: return s
5 Experiments
In this section, we experimentally demonstrate that our algorithms outperform baseline algorithms
and our almost linear-time algorithms significantly improve efficiency in practice. We conducted
experiments on a Linux server with Intel Xeon E5-2690 (2.90 GHz) and 264GB of main memory.
We implemented all algorithms in C++. We measured the computational cost in terms of the number
of function evaluations so that we can compare the efficiency of different methods independently
from concrete implementations.
5.1 Influence maximization with k topics under the total size constraint
We first apply our algorithms to the problem of maximizing the spread of influence on several topics.
First we describe our information diffusion model, called the k-topic independent cascade (k-IC)
model, which generalizes the independent cascade model [6, 7]. In the k-IC model, there are k
kinds of items, each having a different topic, and thus k kinds of rumors independently spread
through a social network. Let G = (V,E) be a social network with an edge probability piu,v for
each edge (u, v) âˆˆ E, representing the strength of influence from u to v on the i-th topic. Given
a seed s âˆˆ (k + 1)V , for each i âˆˆ [k], the diffusion process of the rumor about the i-th topic
starts by activating vertices in suppi(s), independently from other topics. Then the process unfolds
in discrete steps according to the following randomizes rule: When a vertex u becomes active in
the step t for the first time, it is given a single chance to activate each current inactive vertex v. It
succeeds with probability piu,v . If u succeeds, then v becomes active in the step t + 1. Whether or
not u succeeds, it cannot make any further attempt to activate v in subsequent steps. The process
runs until no more activation is possible.
The influence spread Ïƒ : (k+ 1)V â†’ R+ in the k-IC model is defined as the expected total number
of vertices who eventually become active in one of the k diffusion processes given a seed s, namely,
Ïƒ(s) = E
[
|
â‹ƒ
iâˆˆ[k]Ai(suppi(s))|
]
, whereAi(suppi(s)) is a random variable representing the set of
activated vertices in the diffusion process of the i-th topic. Given a directed graph G = (V,E), edge
probabilities piu,v ((u, v) âˆˆ E, i âˆˆ [k]), and a budgetB, the problem is to select a seed s âˆˆ (k+1)V
that maximizes Ïƒ(s) subject to |supp(s)| â‰¤ B. It is easy to see that the influence spread function Ïƒ
is monotone k-submodular (see Appendix B for the proof).
Experimental settings: We use a publicly available real-world dataset of a social news website
Digg.1 This dataset consists of a directed graph where each vertex represents a user and each edge
represents the friendship between a pair of users, and a log of user votes for stories. We set the
number of topics k to be 10, and estimated edge probabilities on each topic from the log using the
method of [1]. We set the value of B to 5, 10, . . . , 100 and compared the following algorithms:
1http://www.isi.edu/Ëœlerman/downloads/digg2009.html
6
k-Greedy-TS
k-Stochastic-Greedy-TS
Single(3)
Degree
Random
0
50
100
150
200
250
300
350
0 20 40 60 80 100
In
fl
u
e
n
c
e
 S
p
re
a
d
Budget
Figure 1: Comparison of influence spreads.
0
10000
20000
30000
40000
50000
60000
70000
0 20 40 60 80 100
#
 o
f 
E
v
a
lu
a
ti
o
n
s
Budget
Figure 2: The number of influence estimations.
â€¢ k-Greedy-TS (Algorithm 1).
â€¢ k-Stochastic-Greedy-TS (Algorithm 2). We chose Î´ = 0.1.
â€¢ Single(i): Greedily chooseB vertices only considering the i-th topic and assign them items
of the i-th topic.
â€¢ Degree: Choose B vertices in decreasing order of degrees and assign them items of ran-
dom topics.
â€¢ Random: Randomly choose B vertices and assign them items of random topics.
For the first three algorithms, we implemented the lazy evaluation technique [16] for efficiency. For
k-Greedy-TS, we maintain an upper bound on the gain of inserting each pair (e, i) to apply the lazy
evaluation technique directly. For k-Stochastic-Greedy-TS, we maintain an upper bound on the
gain for each pair (e, i), and we pick up a pair in R with the largest gain for each iteration. During
the process of the algorithms, the influence spread was approximated by simulating the diffusion
process 100 times. When the algorithms terminate, we simulated the diffusion process 10,000 times
to obtain sufficiently accurate estimates of the influence spread.
Results: Figure 1 shows the influence spread achieved by each algorithm. We only show Sin-
gle(3) among Single(i) strategies since its influence spread is the largest. k-Greedy-TS and k-
Stochastic-Greedy-TS clearly outperform the other methods owing to their theoretical guarantee
on the solution quality. Note that our two methods simulated the diffusion process 100 times to
choose a seed set, which is relatively small, because of the high computation cost. Consequently,
the approximate value of the influence spread has a relatively high variance, and this might have
caused the greedy method to choose seeds with small influence spreads. Remark that Single(3)
works worse than Degree for B larger than 35, which means that focusing on a single topic may
significantly degrade the influence spread. Random shows a poor performance as expected.
Figure 2 reports the number of influence estimations of greedy algorithms. We note that k-
Stochastic-Greedy-TS outperforms k-Greedy-TS, which implies that the random sampling tech-
nique is effective even when combined with the lazy evaluation technique. The number of evalu-
ations of k-Greedy-TS drastically increases when B is around 40 since we run out of influential
vertices and we need to reevaluate the remaining vertices. Indeed, the slope of k-Greedy-TS after
B = 40 is almost constant in Figure 1, which indicates that the remaining vertices have a similar
influence. Single(3) is faster than our algorithms since it only considers a single topic.
5.2 Sensor placement with k kinds of measures under the individual size constraint
Next we apply our algorithms for maximizing k-submodular functions with the individual size
constraint to the sensor placement problem that allows multiple kinds of sensors. In this prob-
lem, we want to determine the placement of multiple kinds of sensors that most effectively re-
duces the expected uncertainty. We need several notions to describe our model. Let â„¦ =
{X1, X2, . . . , Xn} be a set of discrete random variables. The entropy of a subset S of â„¦ is de-
fined as H(S) = âˆ’
âˆ‘
sâˆˆdomS Pr[s] log Pr[s]. The conditional entropy of â„¦ having observed S is
H(â„¦ | S) := H(â„¦) âˆ’H(S). Hence, in order to reduce the uncertainty of â„¦, we want to find a set
S of as a large entropy as possible.
Now we formalize the sensor placement problem. There are k kinds of sensors for different mea-
sures. Suppose that we want to allocate Bi many sensors of the i-th kind for each i âˆˆ [k], and there
7
k-Greedy-IS
k-Stochastic-Greedy-IS
Single(1)
Single(2)
Single(3)
 4
 5
 6
 7
 8
 9
 10
 11
 0  2  4  6  8  10  12  14  16  18
E
n
tr
o
p
y
Value of b
Figure 3: Comparison of entropy.
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 1800
 0  2  4  6  8  10  12  14  16  18
#
 o
f 
E
v
a
lu
a
ti
o
n
s
Value of b
Figure 4: The number of entropy evaluations.
are set V of n locations, each of which can be instrumented with exactly one sensor. Let Xie be the
random variable representing the observation collected from a sensor of the i-th kind if it is installed
at the e-th location, and let â„¦ = {Xie}iâˆˆ[k],eâˆˆV . Then, the problem is to select x âˆˆ (k + 1)V that
maximizes f(x) = H
(â‹ƒ
eâˆˆsupp(x){X
x(e)
e }
)
subject to |suppi(x)| â‰¤ Bi for each i âˆˆ [k]. It is easy
to see that f is monotone k-submodular (see Appendix B for details).
Experimental settings: We use the publicly available Intel Lab dataset.2 This dataset contains a
log of approximately 2.3 million readings collected from 54 sensors deployed in the Intel Berkeley
research lab between February 28th and April 5th, 2004. We extracted temperature, humidity, and
light values from each reading and discretized these values into several bins of 2 degrees Celsius
each, 5 points each, and 100 luxes each, respectively. Hence there are k = 3 kinds of sensors to be
allocated to n = 54 locations. Budgets for sensors measuring temperature, humidity, and light are
denoted by B1, B2, and B3. We set B1 = B2 = B3 = b, where b is a parameter varying from 1 to
18. We compare the following algorithms:
â€¢ k-Greedy-IS (Algorithm 3).
â€¢ k-Stochastic-Greedy-IS (Algorithm 4). We chose Î´ = 0.1.
â€¢ Single(i): Allocate sensors of the i-th kind to greedily chosen
âˆ‘
j Bj places.
We again implemented these algorithms with the lazy evaluation technique in a similar way to the
previous experiment. Also note that Single(i) strategies do not satisfy the individual size constraint.
Results: Figure 3 shows the entropy achieved by each algorithm. k-Greedy-IS and k-Stochastic-
Greedy-IS clearly outperform Single(i) strategies. The maximum gap of entropies achieved by
k-Greedy-IS and k-Stochastic-Greedy-IS is only 0.18%.
Figure 4 shows the number of entropy evaluations of each algorithm. We observe that k-Stochastic-
Greedy-IS outperforms k-Greedy-IS. Especially when b = 18, the number of entropy evaluations
is reduced by 31%. Single(i) strategies are faster because they only consider sensors of a fixed kind.
6 Conclusions
Motivated by real-world applications, we proposed approximation algorithms for maximizing mono-
tone k-submodular functions. Our algorithms run in almost linear time and achieve the approxima-
tion ratio of 1/2 for the total size constraint and 1/3 for the individual size constraint. We empir-
ically demonstrated that our algorithms outperform baseline methods for maximizing submodular
functions in terms of the solution quality. Improving the approximation ratio of 1/3 for the individ-
ual size constraint or showing it tight is an interesting open problem.
Acknowledgments
Y. Y. is supported by JSPS Grant-in-Aid for Young Scientists (B) (No. 26730009), MEXT Grant-
in-Aid for Scientific Research on Innovative Areas (24106003), and JST, ERATO, Kawarabayashi
Large Graph Project. N. O. is supported by JST, ERATO, Kawarabayashi Large Graph Project.
2http://db.csail.mit.edu/labdata/labdata.html
8
References
[1] N. Barbieri, F. Bonchi, and G. Manco. Topic-aware social influence propagation models. In
ICDM, pages 81â€“90, 2012.
[2] N. Buchbinder, M. Feldman, J. S. Naor, and R. Schwartz. A tight linear time (1/2)-
approximation for unconstrained submodular maximization. In FOCS, pages 649â€“658, 2012.
[3] G. Calinescu, C. Chekuri, M. PaÌl, and J. VondraÌk. Maximizing a monotone submodular func-
tion subject to a matroid constraint. SIAM Journal on Computing, 40(6):1740â€“1766, 2011.
[4] P. Domingos and M. Richardson. Mining the network value of customers. In KDD, pages
57â€“66, 2001.
[5] Y. Filmus and J. Ward. Monotone submodular maximization over a matroid via non-oblivious
local search. SIAM Journal on Computing, 43(2):514â€“542, 2014.
[6] J. Goldenberg, B. Libai, and E. Muller. Talk of the network: A complex systems look at the
underlying process of word-of-mouth. Marketing Letters, 12(3):211â€“223, 2001.
[7] J. Goldenberg, B. Libai, and E. Muller. Using complex systems analysis to advance marketing
theory development: Modeling heterogeneity effects on new product growth through stochastic
cellular automata. Academy of Marketing Science Review, 9(3):1â€“18, 2001.
[8] I. Gridchyn and V. Kolmogorov. Potts model, parametric maxflow and k-submodular functions.
In ICCV, pages 2320â€“2327, 2013.
[9] A. Huber and V. Kolmogorov. Towards minimizing k-submodular functions. In Combinatorial
Optimization, pages 451â€“462. Springer Berlin Heidelberg, 2012.
[10] S. Iwata, S. Tanigawa, and Y. Yoshida. Improved approximation algorithms for k-submodular
function maximization. In SODA, 2016. to appear.
[11] D. Kempe, J. Kleinberg, and EÌ. Tardos. Maximizing the spread of influence through a social
network. In KDD, pages 137â€“146, 2003.
[12] C.-W. Ko, J. Lee, and M. Queyranne. An exact algorithm for maximum entropy sampling.
Operations Research, 43(4):684â€“691, 1995.
[13] A. Krause, H. B. McMahon, C. Guestrin, and A. Gupta. Robust submodular observation
selection. The Journal of Machine Learning Research, 9:2761â€“2801, 2008.
[14] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in gaussian processes:
Theory, efficient algorithms and empirical studies. The Journal of Machine Learning Research,
9:235â€“284, 2008.
[15] H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submod-
ular functions. In NAACL/HLT, pages 912â€“920, 2010.
[16] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Opti-
mization Techniques, 7:234â€“243, 1978.
[17] B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. VondraÌk, and A. Krause. Lazier than lazy
greedy. In AAAI, pages 1812â€“1818, 2015.
[18] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maxi-
mizing submodular set functionsâ€”I. Mathematical Programming, 14(1):265â€“294, 1978.
[19] M. Richardson and P. Domingos. Mining knowledge-sharing sites for viral marketing. In
KDD, pages 61â€“70, 2002.
[20] A. P. Singh, A. Guillory, and J. A. Bilmes. On bisubmodular maximization. In AISTATS, pages
1055â€“1063, 2012.
[21] M. Sviridenko. A note on maximizing a submodular set function subject to a knapsack con-
straint. Operations Research Letters, 32(1):41â€“43, 2004.
[22] J. Ward and S. ZÌŒivnyÌ. Maximizing k-submodular functions and beyond. arXiv:1409.1399v1,
2014, A preliminary version appeared in SODA, pages 1468â€“1481, 2014.
9
